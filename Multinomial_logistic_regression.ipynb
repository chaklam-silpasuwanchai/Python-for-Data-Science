{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Lab 04: Multinomial Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Linear Models\n",
    "\n",
    "From lecture, we know that members of the exponential family distributions can be written in the form\n",
    "$$p(y;\\eta) = b(y)e^{(\\eta^\\top T(y)-a(\\eta))},$$\n",
    "where\n",
    "- $\\eta$ is the natural parameter or canonical paramter of the distribution,\n",
    "- $T(y)$ is the sufficient statistic (we normally use $T(y) = y$),\n",
    "- $b(y)$ is an arbitrary scalar function of y, and\n",
    "- $a(\\eta)$ is the log partition function. We use $e^{a(\\eta)}$ just to normalize the distribution to have a sum or integral of 1.\n",
    "\n",
    "Each choice of $T$, $a$, and $b$ defines a family (set) of distributions parameterized by $\\eta$.\n",
    "\n",
    "If we can write $p(y \\mid \\mathbf{x} ; \\theta)$ as a member of the exponential family of distributions with parameters $\\mathbf{\\eta}$ with\n",
    "$\\eta_i = \\theta^\\top_i \\mathbf{x}$, we obtain a *generalized linear model* that can be optimized using the maximum likelihood principle.\n",
    "\n",
    "The GLM for the Gaussian distribution with natural parameter $\\eta$ being the mean of the Gaussian gives us ordinary linear regression.\n",
    "\n",
    "The Bernoulli distribution with parameter $\\phi$ can be written as an exponential distribution\n",
    "with natural parmeter $\\eta = \\log \\frac{\\phi}{1-\\phi}$. The GLM for this distribution is logistic regression.\n",
    "\n",
    "When we write the multinomial distribution with paremeters $\\phi_i > 0$ for classes $i \\in 1..K$ with the constraint that\n",
    "$$\\sum_{i=1}^{K} \\phi_i = 1$$ as a member of the exponential family,\n",
    "the resulting GLM is called *multinomial logistic regression*. The parameters $\\phi_1, \\ldots, \\phi_K$ are written\n",
    "in terms of $\\theta$ as\n",
    "$$\\phi_i = p(y = i \\mid \\mathbf{x}; \\theta) = \\frac{e^{\\theta^\\top_i \\mathbf{x}}}{\\sum_{j=1}^{K}e^{\\theta^\\top_j \\mathbf{x}}}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing a Multinomial Regression Model\n",
    "\n",
    "In multinomial regression, we have\n",
    "\n",
    "1. Data are pairs $\\mathbf{x}^{(i)}, y^{(i)}$ with $\\mathbf{x}^{(i)} \\in \\mathbb{R}^n$ and\n",
    "   $y \\in 1..K$.\n",
    "   \n",
    "2. The hypothesis is a vector-valued function\n",
    "$$\\mathbf{h}_\\theta(\\mathbf{x}) = \\begin{bmatrix} p(y = 1 \\mid \\mathbf{x} ; \\theta) \\\\\n",
    "                                         p(y = 2 \\mid \\mathbf{x} ; \\theta) \\\\\n",
    "                                         \\vdots \\\\\n",
    "                                         p(y = K \\mid \\mathbf{x} ; \\theta) \\end{bmatrix},$$\n",
    "    where $$p(y = i \\mid \\mathbf{x}) = \\phi_i = p(y = i \\mid \\mathbf{x}; \\theta) = \\frac{e^{\\theta^\\top_i \\mathbf{x}}}{\\sum_{j=1}^{K}e^{\\theta^\\top_j \\mathbf{x}}}. $$\n",
    "\n",
    "We need a cost function and a way to minimize that cost function. As usual, we try to find the parameters maximizing the likelihood or log likelihood function, or equivalently, minimizing the negative log likelihood function:\n",
    "\n",
    "$$\\theta^* = \\text{argmax}_\\theta {\\cal L}(\\theta) =\n",
    "             \\text{argmax}_\\theta \\ell(\\theta) =\n",
    "             \\text{argmin}_\\theta J(\\theta),$$\n",
    "where\n",
    "$$\\begin{eqnarray}\n",
    "   J(\\theta) & = & - \\ell(\\theta) \\\\\n",
    "   & = & - \\sum_{i=1}^m \\log p(y^{(i)} \\mid \\textbf{x}^{(i)} ; \\theta).\n",
    "   \\end{eqnarray}$$\n",
    "\n",
    "Now that we know what is $J(\\theta)$, let's try to find its minimimum by taking the derivatives with respect to an arbitrary parameter $\\theta_{kl}$, the $l$-th element of the parameter vector $\\theta_k$ for class $k$. Before we start, let's define a variable $a_k$ as the linear activation for class $k$ in the softmax function:\n",
    "$$ a_k = \\theta_k^\\top \\mathbf{x}^{(i)}, $$\n",
    "and rewrite the softmax more conveniently as\n",
    "$$ \\phi_k = \\frac{e^{a_k}}{\\sum_{j=1}^K e^{a_j}}. $$\n",
    "That makes it a little easier to compute the gradient:\n",
    "$$\\begin{eqnarray}\n",
    "   \\frac{\\partial J}{\\partial \\theta_{kl}} & = & - \\sum_{i=1}^m \\frac{1}{\\phi_{y^{(i)}}} \\frac{\\partial \\phi_{y^{(i)}}}{\\partial \\theta_{kl}}. \\\\\n",
    "   \\end{eqnarray}$$\n",
    "Using the chain rule, we have\n",
    "$$\\frac{\\partial \\phi_{y^{(i)}}}{\\partial \\theta_{kl}} = \\sum_{j=1}^K \\frac{\\partial \\phi_{y^{(i)}}}{\\partial a_j} \\frac{\\partial a_j}{\\partial \\theta_{kl}}$$\n",
    "The second factor is easy:\n",
    "$$ \\frac{\\partial a_j}{\\partial \\theta_{kl}} = \\delta(k=j)x^{(i)}_l. $$\n",
    "For the first factor, we have\n",
    "$$\\begin{eqnarray}\n",
    "\\frac{\\partial \\phi_{y^{(i)}}}{\\partial a_j}\n",
    "& = & \\frac{ \\left[ \\delta(y^{(i)}=j)e^{a_j} \\sum_{c=1}^K e^{a_c} \\right] - e^{a_j} e^{a_j} }{\\left[ \\sum_{c=1}^K e^{a_c} \\right]^2} \\\\\n",
    "& = & \\delta(y^{(i)}=j) \\phi_j - \\phi_j^2\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "Substituting what we've derived into the definition above, we obtain\n",
    "$$ \\frac{\\partial J}{\\theta_{kl}} = - \\sum_{i=1}^m \\sum_{j=1}^K (\\delta(y^{(i)}=j) - \\phi_j) \\frac{\\partial a_j}{\\partial \\theta_{kl}}. $$\n",
    "\n",
    "There are two ways to do the calculation.\n",
    "In deep neural networks with multinomial outputs, we want to first calculate the $\\frac{\\partial J}{\\partial a_j}$ terms then use them to calculate $\\frac{\\partial J}{\\partial \\theta_{kl}}$.\n",
    "\n",
    "However, if we only have the \"single layer\" model described up till now, we note\n",
    "that\n",
    "$$\\frac{\\partial a_j}{\\partial \\theta_{kl}} = \\delta(j=k) x^{(i)}_l,$$\n",
    "so we can simplify as follows:\n",
    "$$\\begin{eqnarray}\n",
    "  \\frac{\\partial J}{\\theta_{kl}} & = & - \\sum_{i=1}^m \\sum_{j=1}^K (\\delta(y^{(i)}=j) - \\phi_j) \\frac{\\partial a_j}{\\partial \\theta_{kl}} \\\\\n",
    "  & = & - \\sum_{i=1}^m \\sum_{j=1}^K (\\delta(y^{(i)}=j) - \\phi_j) \\delta(j=k) x^{(i)}_l \\\\\n",
    "  & = & - \\sum_{i=1}^m (\\delta(y^{(i)}=k) - \\phi_k) x^{(i)}_l \\\\\n",
    "  \\end{eqnarray}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put It Together\n",
    "\n",
    "OK! Now we have all 4 criteria for our multinomial regression model:\n",
    "\n",
    "1. Data are pairs $\\mathbf{x}^{(i)}, y^{(i)}$ with $\\mathbf{x}^{(i)} \\in \\mathbb{R}^n$ and\n",
    "   $y \\in 1..K$.\n",
    "   \n",
    "2. The hypothesis is a vector-valued function\n",
    "$$\\mathbf{h}_\\theta(\\mathbf{x}) = \\begin{bmatrix} p(y = 1 \\mid \\mathbf{x} ; \\theta) \\\\\n",
    "                                         p(y = 2 \\mid \\mathbf{x} ; \\theta) \\\\\n",
    "                                         \\vdots \\\\\n",
    "                                         p(y = K \\mid \\mathbf{x} ; \\theta) \\end{bmatrix},$$\n",
    "    where $$p(y = i \\mid \\mathbf{x}) = \\phi_i = p(y = i \\mid \\mathbf{x}; \\theta) = \\frac{e^{\\theta^\\top_i \\mathbf{x}}}{\\sum_{j=1}^{K}e^{\\theta^\\top_j \\mathbf{x}}}. $$\n",
    "    \n",
    "3. The cost function is\n",
    "   $$J(\\theta) = - \\sum_{i=1}^m \\log p(y^{(i)} \\mid \\textbf{x}^{(i)})$$\n",
    "   \n",
    "4. The optimization algorithm is gradient descent on $J(\\theta)$ with the update rule\n",
    "   $$\\theta_{kl}^{(n+1)} \\leftarrow \\theta_{kl}^{(n)} - \\alpha \\sum_{i=1}^m (\\delta(y^{(i)}=k) - \\phi_k) x^{(i)}_l.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Regression Example\n",
    "\n",
    "The following example of multinomial logistic regression is from [Kaggle](https://www.kaggle.com/saksham219/softmax-regression-for-iris-classification).\n",
    "\n",
    "The data set is the famous [Iris dataset from the UCI machine learning repository](https://archive.ics.uci.edu/ml/datasets/iris).\n",
    "\n",
    "The data contain 50 samples from each of three classes. Each class refers to a particular species of the iris plant. \n",
    "The data include four independent variables:\n",
    "1. Sepal length in cm\n",
    "2. Sepal width in cm\n",
    "3. Petal length in cm\n",
    "4. Petal width in cm\n",
    "\n",
    "The target takes on one of three classes:\n",
    "1. Iris Setosa\n",
    "2. Iris Versicolour\n",
    "3. Iris Virginica\n",
    "   \n",
    "To predict the target value, we use multinomial logistic regression for $k=3$ classes i.e. $y \\in \\{ 1, 2, 3 \\}$. \n",
    "\n",
    "Given $\\mathbf{x}$, we would like to predict a probability distribution over the\n",
    "three outcomes for $y$, i.e., $\\phi_1 = p(y=1 \\mid \\mathbf{x})$, $\\phi_2 = p(y=2 \\mid \\mathbf{x})$, and $\\phi_3 = p(y=3 \\mid \\mathbf{x})$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `phi` function returns $\\phi_i$ for input patterns $\\mathtt{X}$ and parameters $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(i, theta, X):\n",
    "    mat_theta = np.matrix(theta[i])\n",
    "    mat_x = np.matrix(X)\n",
    "    num = math.exp(np.dot(mat_theta,mat_x.T))\n",
    "    den = 0\n",
    "    for j in range(0,k):\n",
    "        mat_theta_j = np.matrix(theta[j])\n",
    "        den = den + math.exp(np.dot(mat_theta_j,mat_x.T))\n",
    "    phi_i = num/den\n",
    "    return phi_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `grad_cost` function gives the gradient of the cost for data $\\mathtt{X}, \\mathbf{y}$ for class $j\\in 1..k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indicator(i, j):\n",
    "    if i == j: return 1\n",
    "    else: return 0\n",
    "\n",
    "\n",
    "def grad_cost(X, y, j, theta):\n",
    "    sum = np.array([0 for i in range(0,n)])\n",
    "    for i in range(0, m):\n",
    "        p = indicator(y[i], j) - phi(j, theta,X.loc[i])\n",
    "        sum = sum + (X.loc[i] * p)\n",
    "    grad = -sum/m\n",
    "    return grad\n",
    "\n",
    "def gradient_descent(X, y, theta, alpha, iters):\n",
    "    for iter in range(iters):\n",
    "        for j in range(0, k):\n",
    "            theta[j] = theta[j] - alpha * grad_cost(X, y, j, theta)\n",
    "    return theta\n",
    "\n",
    "def h(X, theta):\n",
    "    X = np.matrix(X)\n",
    "    h_matrix = np.empty((k,1))\n",
    "    den = 0\n",
    "    for j in range(0,k):\n",
    "        den = den + math.exp(np.dot(theta[j], X.T))\n",
    "    for i in range(0,k):\n",
    "        h_matrix[i] = math.exp(np.dot(theta[i],X.T))\n",
    "    h_matrix = h_matrix/den\n",
    "    return h_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
      "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
      "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
      "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
      "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
      "4   5            5.0           3.6            1.4           0.2  Iris-setosa\n",
      "   SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
      "0            5.1           3.5            1.4           0.2  Iris-setosa\n",
      "1            4.9           3.0            1.4           0.2  Iris-setosa\n",
      "2            4.7           3.2            1.3           0.2  Iris-setosa\n",
      "3            4.6           3.1            1.5           0.2  Iris-setosa\n",
      "4            5.0           3.6            1.4           0.2  Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('Iris.csv')\n",
    "print(data.head())\n",
    "\n",
    "data = data.drop(['Id'],axis=1)\n",
    "print(data.head())\n",
    "\n",
    "# Extract y from data\n",
    "y_label = 'Species';\n",
    "\n",
    "y = data[y_label];\n",
    "\n",
    "y_index = data.columns.get_loc(y_label)\n",
    "# Extract features from data\n",
    "X = data.iloc[:,:y_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = X.shape\n",
    "idx = np.arange(0,m)\n",
    "\n",
    "# Partion data into training and testing dataset\n",
    "random.shuffle(idx)\n",
    "data = data.iloc[idx,:]\n",
    "data = data.reset_index()\n",
    "data = data.drop(['index'],axis=1)\n",
    "percent_train = 0.7\n",
    "m_train = int(m*percent_train)\n",
    "\n",
    "X_train = data.iloc[0:m_train,0:y_index];\n",
    "X_test = data.iloc[m_train:,0:y_index];\n",
    "\n",
    "y_train = data.iloc[:m_train,y_index];\n",
    "y_test = data.iloc[m_train:,y_index];\n",
    "labels = pd.unique(data[y_label])\n",
    "\n",
    "# Encode target labels as integers 0..k-1\n",
    "\n",
    "i = 0\n",
    "for label in y.unique():\n",
    "    y_train[y_train.str.match(label)] = str(i)\n",
    "    y_test[y_test.str.match(label)] = str(i)\n",
    "    i = i + 1      \n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# k is the number of unique labels\n",
    "\n",
    "k = len(y.unique())\n",
    "\n",
    "if (X_train.shape[1] == X.shape[1]): \n",
    "    X_train.insert(0, \"intercept\", 1)\n",
    "\n",
    "# Reset m and n for training data\n",
    "\n",
    "m, n = X_train.shape\n",
    "\n",
    "# Initialize theta for each class  \n",
    "\n",
    "theta_initial = np.ones((k, n))\n",
    "\n",
    "alpha = .05\n",
    "iterations = 200\n",
    "\n",
    "# Logistic regression\n",
    "\n",
    "theta = gradient_descent(X_train, y_train, theta_initial, alpha, iterations)\n",
    "\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting classes on test data \n",
    "\n",
    "if (X_test.shape[1] == X.shape[1]): \n",
    "    X_test.insert(0, \"intercept\", 1)\n",
    "\n",
    "# Reset m and n for test data\n",
    "\n",
    "m,n = X_test.shape\n",
    "\n",
    "y_pred = []\n",
    "for index,row in X_test.iterrows():\n",
    "    h_matrix = h(row, theta)\n",
    "    prediction = int(np.where(h_matrix == h_matrix.max())[0])\n",
    "    y_pred.append(prediction)\n",
    "        \n",
    "# Estimate accuracy of model on test data        \n",
    "\n",
    "correct = (y_pred == y_test).value_counts()[True]\n",
    "accuracy = correct/m\n",
    "print('Accuracy: %.4f' % accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On your own in lab\n",
    "\n",
    "Do the following in lab:\n",
    "1. Write a function to obtain the cost for particular $\\mathtt{X}$, $\\mathbf{y}$, and \\theta$.\n",
    "2. Plot the training set and test cost as training goes on and find the best value for the number of iterations and learning rate.\n",
    "3. Make 2D scatter plots showing the predicted and actual class of each item in the training set, plotting two features at a time.\n",
    "   Comment on the cause of the errors you observe. If you obtain perfect test set accuracy, re-run the train/test split\n",
    "   and rerun the optimization until you observe some mistaken predictions on the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On your own to take home\n",
    "\n",
    "We see that the Iris dataset is pretty easy. Depending on the train/test split, we get 97-100% accuracy.\n",
    "\n",
    "Find a more interesting multi-class classification problem on Kaggle, clean the dataset to obtain numerical input features without missing values,\n",
    "split the data into test and train, and experiment with multinomial logistic regression.\n",
    "\n",
    "Write a brief report on your experiments and results. As always, turn in a Jupyter notebook by email\n",
    "to the instructor and TA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
