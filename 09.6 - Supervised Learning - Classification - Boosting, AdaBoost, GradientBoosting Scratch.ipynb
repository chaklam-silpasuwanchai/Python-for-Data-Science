{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science and Artificial Intelligence\n",
    "\n",
    "## 9.6 - Classification - Boosting, AdaBoost, GradientBoosting Scratch\n",
    "\n",
    "### Readings:\n",
    "- [GERON] Ch7\n",
    "- [VANDER] Ch5\n",
    "- [HASTIE] Ch16\n",
    "- https://scikit-learn.org/stable/modules/ensemble.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Boosting is a general strategy for learning classifiers by combining simpler ones. The idea of boosting is to take a \"weak classifier\" — that is, any classifier that will do at least slightly better than chance — and use it to build a much better classifier, thereby boosting the performance of the weak classification algorithm. This boosting is done by averaging the outputs of a collection of weak classifiers.  The common form of hypothesis function for boosting is as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H(x) & =  \\alpha_1h_1(x) + \\alpha_2h_2(x) + \\cdots + \\alpha_sh_s(x) ) \\\\\n",
    "& = \\Sigma_{s=1}^{S}\\alpha_sh_s(x)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $S =$ number of classifiers and $\\alpha$ is the weight associated with each classifier \n",
    "\n",
    "The among the first, and therefore popular boosting algorithm is **AdaBoost**, so-called because it is *adaptive.*\n",
    "\n",
    "AdaBoost is extremely simple to use and implement (far simpler than SVMs), and often gives very effective results. There is tremendous flexibility in the choice of weak classifier as well. Anyhow, Decision Tree with max_depth=1 and max_leaf_nodes=2 are often used (also known as **stump**)\n",
    "\n",
    "Suppose we are given training data ${(\\mathbf{x_i}, y_i)}$, where $\\mathbf{x_i} \\in \\mathbb{R}^n$ and $y_i \\in \\{-1, 1\\}$.  And suppose we are given a (potentially large) number (denoted $S$) of weak classifiers, denoted $h_s(x) \\in \\{-1, 1\\}$ where $s = 1, 2, \\cdots, S$, and for each classifier, we define $\\alpha_s$ as the *voting power* of the classifier $h_s(x)$. Then, the hypothesis function is based on a linear combination of the weak classifier and is written as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H(x) & = \\text{sign}\\big(\\alpha_1h_1(x) + \\alpha_2h_2(x) + \\cdots + \\alpha_sh_s(x) )\\big) \\\\\n",
    "& = \\text{sign}\\big(\\Sigma_{s=1}^{S}\\alpha_sh_s(x)\\big)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Our job is to find the optimal $\\alpha_s$, so we can know which classifier we should give more weightage (i.e., believe more) in our hypothesis function since their accuracy is relatively better compared to other classifiers.  To get this alpha, we should define what is \"good\" classifier.  This is simple, since good classifier should simply has the maximum number of accurate classified samples as:\n",
    "\n",
    "$$ max \\big( \\Sigma_{i=1}^m I(h_s(x_i) = y_i) \\big)$$\n",
    "\n",
    "or can be written as minimization function as\n",
    "\n",
    "$$ min \\big( \\Sigma_{i=1}^m I(h_s(x_i) \\neq y_i) \\big)$$\n",
    "\n",
    "Aside from \"weighted\" wisdom of crowd, AdaBoost has one more capability, and that is that each subsequent classifier will try to correct the errors made by previous predictor.  In other words,  whatever samples the previous classifier misclassified, it should be prioritized in the subsequent classifier.  To realize this mechanism, the concept is to increase the penalty if those previously misclassified sample are wrong.  To do so, we first initialize the weight for each sample, which shall be applied to the first predictor $h_1(x)$ to be \n",
    "\n",
    "$$ w_i^{(s)} = \\frac{1}{m} ; s = 1;  i = 1, 2, \\cdots, m$$\n",
    "\n",
    "Then, after the first classifier was fitted, we readjust this weight by increasing weight for those misclassified sample, and decreasing weight for those correctly classified sample.  To make sure classifier will be chosen on the basis of these weighted errors, we shall revise our definition of \"good\" classifiers as follows:\n",
    "\n",
    "$$ min \\big( \\frac{\\Sigma_{i=1}^m w_i^{s}I(h_s(x_i) \\neq y_i)}{\\Sigma_{i=1}^m w_i^{s}} \\big ) $$\n",
    "\n",
    "Note that the lower term is simply so that all weights sum to 1.\n",
    "\n",
    "Thus, the subsequent classifier will be chosen based on the one that can create the least weighted errors. \n",
    "\n",
    "Let's put everything into the AdaBoost algorithm as follows:\n",
    "\n",
    "define $S$\n",
    "\n",
    "**for** $i$ from 1 to m {\n",
    "\n",
    "$$w_i^{(1)} = \\frac{1}{m}$$ \n",
    "\n",
    "make sure $y \\in \\{-1, 1\\}$\n",
    "\n",
    "}\n",
    "\n",
    "**for** $s$ = 1 to $S$ {\n",
    "\n",
    "Looping through all features and threshold, identify the best stump, whose value has the minimum of this objective function:\n",
    "    \n",
    "$$\\epsilon_s = \\Sigma_{i=1}^m w_i^{s}I(h_s(x_i) \\neq y_i) $$\n",
    "\n",
    "where $I$ is indicator function $I(h_s(x_i) \\neq y_i) = 1$ if $h_s(x_i) \\neq y_i$ and 0 otherwise\n",
    "\n",
    "Then calculate the voting power of the weak classifier, denoted $\\alpha_s$ and can be calculated as:\n",
    "\n",
    "$$\\alpha_s = \\frac{1}{2}ln\\frac{1-\\epsilon_s}{\\epsilon_s}$$\n",
    "\n",
    "Before fitting the next stump, we need to make sure to exaggerate the weights of *incorrectly* classified samples so our next stump will be chosen based on the new weighted objective function:\n",
    "\n",
    "Then **for** all $i$ { $$w_i^{(s+1)} = \\frac{w_i^{(s)}e^{ \\alpha_sI(h_s(\\mathbf{x_i}\\neq y_i)}}{{\\Sigma_{i=1}^m w_i^{s}}} $$}\n",
    "\n",
    "}\n",
    "\n",
    "To predict, we simply take the weighted sum of all predictors and take the sign of them.  Recall that $S$ is number of stumps/predictors you have\n",
    "\n",
    "$$ \n",
    "  H(x) = \\text{sign}\\big(\\Sigma_{s=1}^{S}\\alpha_sh_s(x)\\big)\n",
    "$$\n",
    "\n",
    "Stopping criteria of AdaBoost is important to impose or else we can get some overfitting with AdaBoost by adding too many classifiers.  We can either specify the number of iterations, or when we reach a certain level of accuracy, or perform early stopping by using a validation set to detect the iteration when overfit starts to happen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=500, random_state=1)\n",
    "y = np.where(y==0,-1,1)  #change our y to be -1 if it is 0, otherwise 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.96      0.97      0.97        79\n",
      "           1       0.97      0.96      0.96        71\n",
      "\n",
      "    accuracy                           0.97       150\n",
      "   macro avg       0.97      0.97      0.97       150\n",
      "weighted avg       0.97      0.97      0.97       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "m = X_train.shape[0]\n",
    "S = 20\n",
    "stump_params = {'max_depth': 1, 'max_leaf_nodes': 2}\n",
    "models = [DecisionTreeClassifier(**stump_params) for _ in range(S)]\n",
    "\n",
    "#initially, we set our weight to 1/m\n",
    "W = np.full(m, 1/m)\n",
    "\n",
    "#keep collection of a_j\n",
    "a_js = np.zeros(S)\n",
    "\n",
    "for j, model in enumerate(models):\n",
    "    \n",
    "    #train weak learner\n",
    "    model.fit(X_train, y_train, sample_weight = W)\n",
    "    \n",
    "    #compute the errors\n",
    "    yhat = model.predict(X_train) \n",
    "    err = W[(yhat != y_train)].sum()\n",
    "        \n",
    "    #compute the predictor weight a_j\n",
    "    #if predictor is doing well, a_j will be big\n",
    "    a_j = np.log ((1 - err) / err) / 2\n",
    "    a_js[j] = a_j\n",
    "    \n",
    "    #update sample weight; divide sum of W to normalize\n",
    "    W = (W * np.exp(-a_j * y_train * yhat)) \n",
    "    W = W / sum (W)\n",
    "    \n",
    "        \n",
    "#make weighted predictions\n",
    "Hx = 0\n",
    "for i, model in enumerate(models):\n",
    "    yhat = model.predict(X_test)\n",
    "    Hx += a_js[i] * yhat\n",
    "    \n",
    "yhat = np.sign(Hx)\n",
    "\n",
    "print(classification_report(y_test, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn implements AdaBoost using SAMME which stands for Stagewise Additive Modeling using a Multiclass Exponential Loss Function.\n",
    "\n",
    "The following code trains an AdaBoost classifier based on 200 Decision stumps.  A Decision stump is basically a Decision Tree with max_depth=1.  This is the default base estimator of AdaBoostClassifier class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ada score:  0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "#SAMME.R - a variant of SAMME which relies on class probabilities \n",
    "#rather than predictions and generally performs better\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    learning_rate=0.5, random_state=42)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "y_pred = ada_clf.predict(X_test)\n",
    "print(\"Ada score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===Task===\n",
    "\n",
    "<strong>Your work: Let's modify the above scratch code:</strong>\n",
    "    <ol>\n",
    "    <li>Notice that if <code>err</code> = 0, then $\\alpha$ will be undefined, thus attempt to fix this by adding some very small value to the lower term</li>\n",
    "    <li>Notice that sklearn version of AdaBoost has a parameter <code>learning_rate</code>.  This is in fact the $\\frac{1}{2}$ in front of the $\\alpha$ calculation.  Attempt to change this $\\frac{1}{2}$ into a parameter called <code>eta</code>, and try different values of it and see whether accuracy is improved.  Note that sklearn default this value to 1.\n",
    "        <li>Observe that we are actually using sklearn DecisionTreeClassifier.  If we take a look at it closely, it is actually using weighted gini index, instead of weighted errors that we learn above.   Attempt to write your own class of <code>class Stump</code> that actually uses weighted errors, instead of weighted gini index</li>\n",
    "    <li>Put everything into a class</li>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "Another popular one is Gradient Boosting.  Similar to AdaBoost, Gradient Boosting works by adding sequential predictors.  However, instead of adding **weights**, this method tries to fit the new predictor to the **residual errors** made by the previous predictor.    The hypothesis function of gradient boosting is as follows:\n",
    "\n",
    "$$\n",
    "H(x) = h_0(x) + \\alpha_1h_1(x) + \\cdots + \\alpha_sh_s(x)\n",
    "$$\n",
    "\n",
    "Although they look similar, notice that no alpha is applied to the first predictor.  In addition, each alpha is the same, as opposed to voting power in AdaBoost.  Typically, similar to AdaBoost, decision trees are used for each $h_i(x)$ but are not limited to stump.  In practice, min_leaves are set to around 8 to 32.\n",
    "\n",
    "Since gradient boosting actually originate from additive linear regression, we shall first talk about **gradient boosting for regression**.  Also assume that we are using **regression trees** for our regressors.\n",
    "\n",
    "### Gradient Boosting for Regression\n",
    "\n",
    "Firstly, let's look at the following equation where $h_0(x)$ is our first predictor and we would like to minimize the residual as follows:\n",
    "\n",
    "$$h_0(x) + residual_0 = y $$\n",
    "$$ residual_0 =  y - h_0(x) $$\n",
    "\n",
    "That is, we would $y$ to be as close as $h_0(x)$ such that residual is 0\n",
    "\n",
    "$$ y = h_0(x) $$\n",
    "\n",
    "The question is that is it possible to add the second predictor $h_1(x)$ such that the residual is further reduced\n",
    "\n",
    "$$ y = h_0(x) + h_1(x) $$\n",
    "\n",
    "This equation can be written as:\n",
    "\n",
    "$$h_1(x) = y - h_0(x) $$\n",
    "\n",
    "This equation informs us that if we can find a subsequent predictor that can best fit the \"residual\" (i.e. $y - h_0(x)$), then we can improve the accuracy.\n",
    "\n",
    "**How is this related to gradient descent?**\n",
    "\n",
    "Well, firstly, here is our loss function for regression:\n",
    "\n",
    "$$J = \\frac{1}{2}(y - h(x))^2$$\n",
    "\n",
    "And here, we want to minimize $J$ by gradient of the loss function in respect to by adjusting $h_x$.  We can thus treat $h_x$ as parameters and take derivatives:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial h_(x)} = y - h(x)$$\n",
    "\n",
    "Thus, we can interpret residuals as negative gradients:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "y & = h_0(x) + h_1(x)\\\\\n",
    "& = h_0(x) + (y - h_0(x)) \\\\\n",
    "& = h_0(x) - (h_0(x) - y) \\\\\n",
    "& = h_0(x) - \\frac{\\partial J}{\\partial h_0(x)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So in fact, we are using \"gradient\" descent in \"gradient\" boosting to find the new model, written as:\n",
    "\n",
    "$$h_1(x) = - \\frac{\\partial J}{\\partial h_0(x)} = y - h_0(x)$$\n",
    "\n",
    "or more generally\n",
    "\n",
    "$$h_s(x) = - \\frac{\\partial J}{\\partial h_{s-1}(x)} = y - h_{s-1}(x)$$\n",
    "\n",
    "where $s$ is the index of predictor\n",
    "\n",
    "**So residuals or gradients?**\n",
    "\n",
    "Although they are equivalent in the mse loss function, it is more useful to use negative gradients as it is more general, and can apply to other loss functions as well, e.g., cross-entropy in the case of classification.\n",
    "\n",
    "In cross entropy, the loss function is\n",
    "\n",
    " $$J= y log h(x) + (1 - y) lg (1-h(x))$$\n",
    " \n",
    "If you look at our previous lecture on logistic regression, the derivative of this **in respect to h(x)** will be:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial h_(x)} = y - h(x)$$\n",
    "\n",
    "This may look the same as mse, but here, h(x) is\n",
    "\n",
    "$$h(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "\n",
    "**Adding learning rate**\n",
    "\n",
    "To make sure adding the subsequent predictor would not overfit our model, we shall add an learning rate $\\alpha$ in front of this, which shall be the same across all predictors (different from AdaBoost where alpha is different across all predictors)\n",
    "\n",
    "$$h_s(x) = - \\alpha \\frac{\\partial J}{\\partial h_{s-1}(x)}$$\n",
    "\n",
    "**What about next predictor**\n",
    "\n",
    "We can stop if we are happy, either using some predefined iterations, or whether the residual does not decrease further using some validation set.  \n",
    "\n",
    "In this case, it is obvious that 2 predictors are simply not enough.  Thus, we first need to calculate the residuals which are\n",
    "\n",
    "$$ residual_1 =  y - (h_0(x) + \\alpha h_1(x))$$\n",
    "\n",
    "then we define $h_2(x)$ as \n",
    "\n",
    "$$h_2(x) = \\alpha(y - (h_0(x) + \\alpha h_1(x)))$$\n",
    "\n",
    "And then repeat\n",
    "\n",
    "The final prediction shall use the following hypothesis function $H(x)$:\n",
    "\n",
    "$$\n",
    "H(x) = h_0(x) + \\alpha_1h_1(x) + \\cdots + \\alpha_sh_s(x)\n",
    "$$\n",
    "\n",
    "**Summary of steps**\n",
    "\n",
    "1. Initialize the model as simply mean or some constant\n",
    "2. Predict and calculate the residual\n",
    "3. Let the next model fit the residual\n",
    "4. Predict using the combined models and calculate the residual\n",
    "5. Let the next model fit this residual\n",
    "6. Simply repeat 4-5 until stopping criteria is reached\n",
    "\n",
    "### Gradient Boosting for Classification\n",
    "\n",
    "What we have discussed is gradient boosting for regression.  However, there are not much difference for classification.  \n",
    "\n",
    "Recall the cost function of classification is **cross entropy** where its derivative is simply:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial h_(x)} = y - h(x)$$\n",
    "\n",
    "Although this may look similar, $h(x)$ carries a function that convert real value to probabilities.\n",
    "\n",
    "For binary classification, $h(x)$ is defined as the sigmoid function:\n",
    "\n",
    "$$h(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "For multiclass/binary classification, $h(x)$ is defined as the softmax function:\n",
    "\n",
    "$$\\frac{e^x_c}{\\Sigma_{i=1}^{k} e^x_k}$$\n",
    "\n",
    "Also remind that to use softmax function, we need to first one-hot encode our y.  And during prediction, we need to perform <code>np.argmax</code> along the axis=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "def grad(y, f):\n",
    "    return y - f\n",
    "\n",
    "def fit(X, y, models):\n",
    "    \n",
    "    models_trained = []\n",
    "    \n",
    "    #using DummyRegressor is a good technique for starting model\n",
    "    first_model = DummyRegressor(strategy='mean')\n",
    "    first_model.fit(X, y)\n",
    "    models_trained.append(first_model)\n",
    "    \n",
    "    #fit the estimators\n",
    "    for i, model in enumerate(models):\n",
    "        #predict using all the weak learners we trained up to\n",
    "        #this point\n",
    "        y_pred = predict(X, models_trained)\n",
    "        \n",
    "        #errors will be the total errors maded by models_trained\n",
    "        residual = grad(y, y_pred)\n",
    "        \n",
    "        #fit the next model with residual\n",
    "        model.fit(X, residual)\n",
    "        \n",
    "        models_trained.append(model)\n",
    "        \n",
    "    return models_trained\n",
    "        \n",
    "def predict(X, models):\n",
    "    learning_rate = 0.1  ##hard code for now\n",
    "    f0 = models[0].predict(X)  #first use the dummy model\n",
    "    boosting = sum(learning_rate * model.predict(X) for model in models[1:])\n",
    "    return f0 + boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our MSE:  12.945557601580582\n"
     ]
    }
   ],
   "source": [
    "# Regression\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3, random_state=42)\n",
    "\n",
    "n_estimators = 200\n",
    "tree_params = {'max_depth': 1}\n",
    "models = [DecisionTreeRegressor(**tree_params) for _ in range(n_estimators)]\n",
    "\n",
    "#fit the models\n",
    "models = fit(X_train, y_train, models)\n",
    "\n",
    "#predict\n",
    "y_pred = predict(X_test, models)\n",
    "\n",
    "#print metrics\n",
    "print(\"Our MSE: \", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn \n",
    "\n",
    "sklearn has implemented GradientBoosting under the API of <code>GradientBoostingClassifier</code> for classification and <code>GradientBoostingRegressor</code> for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn MSE:  12.945557601580584\n"
     ]
    }
   ],
   "source": [
    "#Compare to sklearn: ls is the same as our mse\n",
    "sklearn_model = GradientBoostingRegressor(\n",
    "    n_estimators=n_estimators,\n",
    "    learning_rate = 0.1,\n",
    "    max_depth=1,\n",
    "    loss='ls'\n",
    ")\n",
    "\n",
    "y_pred_sk = sklearn_model.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#print metrics\n",
    "print(\"Sklearn MSE: \", mean_squared_error(y_test, y_pred_sk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost\n",
    "\n",
    "XGBoost is an optimized distributed gradient boosting, designed to be more efficient, flexible, and portable (Chen and Guestrin 2016).  In fact, XGBoost is often an important component of the winning entries in ML competitions (e.g., Kaggle).  XGBoost also offers several nice features, such as automatically taking care of early stopping: XGBoost’s API is quite similar to Scikit-Learn’s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:16.15458\n",
      "Will train until validation_0-rmse hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-rmse:11.84377\n",
      "[2]\tvalidation_0-rmse:8.79602\n",
      "[3]\tvalidation_0-rmse:6.72584\n",
      "[4]\tvalidation_0-rmse:5.46526\n",
      "[5]\tvalidation_0-rmse:4.65454\n",
      "[6]\tvalidation_0-rmse:4.08462\n",
      "[7]\tvalidation_0-rmse:3.76129\n",
      "[8]\tvalidation_0-rmse:3.54313\n",
      "[9]\tvalidation_0-rmse:3.37742\n",
      "[10]\tvalidation_0-rmse:3.24836\n",
      "[11]\tvalidation_0-rmse:3.18872\n",
      "[12]\tvalidation_0-rmse:3.10860\n",
      "[13]\tvalidation_0-rmse:3.09993\n",
      "[14]\tvalidation_0-rmse:3.08393\n",
      "[15]\tvalidation_0-rmse:3.08760\n",
      "[16]\tvalidation_0-rmse:3.06310\n",
      "[17]\tvalidation_0-rmse:3.05292\n",
      "[18]\tvalidation_0-rmse:3.05715\n",
      "[19]\tvalidation_0-rmse:3.05827\n",
      "Stopping. Best iteration:\n",
      "[17]\tvalidation_0-rmse:3.05292\n",
      "\n",
      "MSE: 9.320308418219375\n"
     ]
    }
   ],
   "source": [
    "#make sure to pip install xgboost\n",
    "#for mac guys, do \"brew install libomp\" which installs openMP library\n",
    "#required for XGBoost\n",
    "\n",
    "import xgboost\n",
    "\n",
    "xgb_reg = xgboost.XGBRegressor() \n",
    "\n",
    "#not improved after 2 iterations\n",
    "xgb_reg.fit(X_train, y_train,\n",
    "                eval_set=[(X_test, y_test)], early_stopping_rounds=2)\n",
    "y_pred = xgb_reg.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"MSE:\", mse)  #notice we are using mse while xgb uses root mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.1 ms ± 886 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "79.6 ms ± 329 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit xgboost.XGBRegressor().fit(X_train, y_train)\n",
    "%timeit GradientBoostingRegressor().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===Task===\n",
    "\n",
    "Modify the above scratch code such that:\n",
    "- Notice that we are still using max_depth = 1.  Attempt to tweak min_samples_split, max_depth for the regression and see whether we can achieve better mse on our boston data\n",
    "- Notice that we only write scratch code for gradient boosting for regression, add some code so that it also works for binary classification.  Load the breast cancer data from sklearn and see that it works.\n",
    "- Further change the code so that it works for multiclass classification.  Load the digits data from sklearn and see that it works\n",
    "- Put everything into class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use Boosting\n",
    "\n",
    "Let's summarize some useful info about Gradient Boosting:\n",
    "\n",
    "Advantages:\n",
    "1. Extremely powerful - especially useful for heterogeneous data (e.g., house price, number of bedrooms). \n",
    "\n",
    "Disadvantages:\n",
    "1. They cannot be parallelized.  Obvious since they are sequential predictors.\n",
    "2. They can easily overfit, thus require careful choice of estimators or the use of regularization such as max_depth.\n",
    "3. When we talk about homogeneous data such as images, videos, audio, text, or huge amount of data, deep learning works better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
