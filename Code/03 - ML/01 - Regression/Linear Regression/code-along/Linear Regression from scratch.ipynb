{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression from Scratch\n",
    "\n",
    "| | Egg price  | Gold price    | Oil price   | GDP   |\n",
    "|---:|:-------------|:-----------|:------|:------|\n",
    "| 1 | 3  | 100       | 4   | 21   |\n",
    "| 2 | 4  | 500    | 7   | 43     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notations and Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#sample 1  $x^1$\n",
    "x1 = np.array([3, 100, 4])\n",
    "y1 = np.array([21])\n",
    "\n",
    "#what's the idea of prediction?  What is machine learning?\n",
    "#- find the weights that can bring you from x1 to y1\n",
    "\n",
    "#first sample\n",
    "#3 * w1 + 100 * w2 + 4 * w3 = 21\n",
    "#3 * 1  + 100 * 1  + 4 * 1  = 107\n",
    "#3 * 7  + 100 * 1  + 4 * -25  = 21\n",
    "\n",
    "#machine learning is trying to find the `best` weights\n",
    "\n",
    "#2nd sample\n",
    "#4 * w1 + 500 * w2 + 7 * w3   = 43\n",
    "#4 * 7  + 500 * 1  + 7 * -25  = 353 \n",
    "\n",
    "#machine learning is trying to find the `best` weights ACROSS all samples....\n",
    "\n",
    "#all deep learning is based on these weight systems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition of terms and notations\n",
    "\n",
    "#2 samples\n",
    "#3 features - egg price, gold price, oil price\n",
    "    #features are the variables used for predicting the label\n",
    "    #factors, independent variables, predictors, X\n",
    "\n",
    "#egg price - x_1 --> always a vector,  e.g., [3, 4]\n",
    "#gold price - x_2 --> always a vector, e.g., [100, 500]\n",
    "#oil price - x_3 --> always a vector, e.g., [4, 7]\n",
    "#we call egg price + gold price + oil price - whole `feature matrix` --> \\mathbf{X}\n",
    "    \n",
    "#1 label - gdp\n",
    "    #label is the variable that we want to predict....\n",
    "    #target, outcome, y\n",
    "    #y_1 = y = a vector of labels, e.g., [21, 43]\n",
    "    \n",
    "#Tips: small and big\n",
    "# small mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Math notations:\n",
    "\n",
    "- normal a -> scalar (one number)\n",
    "- bold  $\\mathbf{a}$  --> vector (a 1D numpy array)\n",
    "- bold  $\\mathbf{A}$  --> matrix (a 2D numpy array....)\n",
    "\n",
    "- $\\mathbf{x}_1^2$  --> feature 1, second sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How dot product works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([  [3, 100, 4] , [4, 500, 7]  ])\n",
    "X.shape  #(2, 3) means 2 samples = m, 3 features = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#weights = theta = params\n",
    "theta = np.array([7, 1, -25])\n",
    "theta.shape  #weights must be the sample shape as X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 21, 353])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X.dot(theta)\n",
    "#to be able to dot, the number should be same in the close pair\n",
    "#(2, 3)  @ (3, ) = (2, )\n",
    "#(4, 6)  @ (6, 1) = (4, 1)\n",
    "#(4, 6, 1) @ (1, 2) = (4, 6, 1, 2)\n",
    "X @ theta\n",
    "\n",
    "#the common error: matmul error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][0] * theta[0] + X[0][1] * theta[1] + X[0][2] * theta[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps for linear regression / gradient descent\n",
    "\n",
    "### Gradient descent is basically backpropagation in deep learning....\n",
    "\n",
    "Step 1: Randomize your weight\n",
    "  - weight.shape (n, )\n",
    "\n",
    "Step 2: Use this inital weight to predict\n",
    "  - you will get errors\n",
    "\n",
    "Step 3: Find the derivative\n",
    "\n",
    "$\\mathbf{X}^\\top (\\mathbf{\\hat{y}} - \\mathbf{y})$\n",
    "\n",
    "Step 4: Change the weight\n",
    "\n",
    "$\\mathbf{w} = \\mathbf{w} - \\alpha * \\mathbf{X}^\\top (\\mathbf{\\hat{y}} - \\mathbf{y})$\n",
    "\n",
    "Step 5:  Repeat Step 2, 3, 4, until you either (1) reach the max iteration, or (2) your validation loss does not decrease anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Load some toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "#print the shape of X and y\n",
    "X.shape, y.shape\n",
    "assert X.ndim == 2\n",
    "assert y.ndim == 1\n",
    "\n",
    "#print one row of X, and maybe try to see what it is...\n",
    "#print one row of y, and maybe try to see what it is....\n",
    "# X[0]\n",
    "# y[0]\n",
    "# diabetes.feature_names\n",
    "# label is blood glucose level.....\n",
    "\n",
    "#please help me set m and \n",
    "m = X.shape[0]  #number of samples\n",
    "n = X.shape[1]  #number of features\n",
    "\n",
    "#write an assert function to check that X and y has same amount of samples...\n",
    "assert m == y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We skip EDA and cleaning, because we are lazy; but actually this dataset is already clean..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split here\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.3, random_state = 9999\n",
    ")\n",
    "\n",
    "#assert that X_train and y_train have the same amount of samples\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "\n",
    "#assert that X_test and y_test have the same amount of samples\n",
    "assert X_test.shape[0] == y_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "#standardize the training set\n",
    "X_train = sc.fit_transform(X_train)\n",
    "\n",
    "#standardize the test set\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Add intercept to your X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: if your X is        [  [3, 2, 4],    [2, 6, 8]  ]\n",
    "# I want you to make it into   [  [1, 3, 2, 4], [1, 2, 6, 8]  ]\n",
    "# Why 1?  because imagine you have another weight, which let's call w0\n",
    "# this w0 is actually the intercept; so multiply with 1, will do nothing\n",
    "# so we can still use X @ theta....\n",
    "\n",
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "intercept.shape\n",
    "\n",
    "#hint: use np.concatenate with X_train on axis=1, to add these ones to X_train\n",
    "X_train = np.concatenate((intercept, X_train), axis=1)\n",
    "\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "intercept.shape\n",
    "\n",
    "#hint: use np.concatenate with X_test on axis=1, to add these ones to X_test\n",
    "X_test = np.concatenate((intercept, X_test), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Fitting!!! Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put everything fit()\n",
    "\n",
    "#1. randomize our theta\n",
    "#please help me create a random theta of size (X_train.shape[1], )\n",
    "theta = np.ones(X_train.shape[1])\n",
    "#why X_train.shape[1]\n",
    "\n",
    "#5. repeat 2, 3, 4\n",
    "#please put a for loop for 2, 3, 4, for 1000 times\n",
    "#set 1000 call it max_iter\n",
    "#for _ in range(max_iter):\n",
    "max_iter = 1000\n",
    "alpha = 0.0001\n",
    "\n",
    "def predict(X, theta):\n",
    "    return X @ theta\n",
    "\n",
    "def mean_squared_error(ytrue, ypred):\n",
    "    return ((ypred - ytrue) ** 2).sum() / ytrue.shape[0]\n",
    "\n",
    "def _grad(X, error):  #it's only for internal purpose\n",
    "    #not for external purpose\n",
    "    #what do you mean by external:  i mean the main() program of python\n",
    "    return X.T @ error\n",
    "\n",
    "def fit(X_train, y_train, theta, max_iter, alpha):\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        #2. predict\n",
    "        yhat = predict(X_train, theta)  #put this into a function called predict(X_train, theta)\n",
    "\n",
    "        #2.1 can you guys compute the squared error\n",
    "        # squared_error = ((yhat - y_train) ** 2).sum()\n",
    "        #print the mean squared error, we can see whether MSE goes down eventually...\n",
    "        mse =  mean_squared_error(y_train, yhat)\n",
    "        if(i % 50 == 0):\n",
    "            print(f\"MSE: {mse}\")  \n",
    "\n",
    "        #3. get derivatives\n",
    "        deriv = _grad(X_train, yhat - y_train)\n",
    "\n",
    "        #4. update weight\n",
    "        theta = theta - alpha * deriv\n",
    "        \n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 28562.951824703876\n",
      "MSE: 3897.3289014795173\n",
      "MSE: 2877.3645633448773\n",
      "MSE: 2831.24639141041\n",
      "MSE: 2827.9023866215557\n",
      "MSE: 2826.5434217394395\n",
      "MSE: 2825.3251211991583\n",
      "MSE: 2824.1524285941555\n",
      "MSE: 2823.016722214682\n",
      "MSE: 2821.91531119463\n",
      "MSE: 2820.8463667746114\n",
      "MSE: 2819.8083642041256\n",
      "MSE: 2818.79996461671\n",
      "MSE: 2817.819969874501\n",
      "MSE: 2816.867295693387\n",
      "MSE: 2815.9409519375267\n",
      "MSE: 2815.040027131533\n",
      "MSE: 2814.163676031542\n",
      "MSE: 2813.311109582284\n",
      "MSE: 2812.481586776238\n"
     ]
    }
   ],
   "source": [
    "theta = fit(X_train, y_train, theta, max_iter, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3079.246779652193"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = predict(X_test, theta)\n",
    "\n",
    "mean_squared_error(y_test, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many things I wanna do today\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives:\n",
    "    \n",
    "1. Understand better about cross validation\n",
    "2. Remind how to write a class\n",
    "3. Learn about stochastic and mini-batch gradient descent, which are very important concepts later on in deep learning....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ex1:  Please write this whole thing into a class called LinearRegression()\n",
    "    #it should have two functions at least\n",
    "    #fit() to fit the model\n",
    "    #predict() to inference\n",
    "    #trivial: try to do like what sklearn do....\n",
    "    #15:10 - 15:45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "minibatch-gradient descent\n",
    "- use only a subset of data for finding the gradient\n",
    "- why?  \n",
    "- because using the whole set of data to find the gradient takes time\n",
    "- we assume that the subset of data, should give a general good slope anyway for the whole population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stochastic-gradient descent\n",
    "- use only one sample to get the gradient\n",
    "- very fast (but NOT good! usually.....)\n",
    "- Exercise: add a parameter at init called `method`\n",
    "  - if method=mini_batch, then do mini_batch\n",
    "  - if method=sto, do sto ===>please do without replacement \n",
    "  - otherwise, do the normal gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class LinearRegression(object):\n",
    "    \n",
    "    kfold = KFold(n_splits=5)\n",
    "            \n",
    "    def __init__(self, alpha=0.001, num_epochs=5, batch_size=50, method='batch',\n",
    "                 cv=kfold):\n",
    "        self.alpha      = alpha\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.method     = method\n",
    "        self.cv         = cv\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \n",
    "        #using training......\n",
    "        \n",
    "        #please change it to cross-validation.....\n",
    "        \n",
    "        #create a list of kfold scores\n",
    "        self.kfold = list()\n",
    "\n",
    "        #Kfold.split in the sklearn.....\n",
    "        #5 splits\n",
    "        for fold, (train_idx, val_idx) in enumerate(self.cv.split(X_train)):\n",
    "            \n",
    "            X_cross_train = X_train[train_idx]\n",
    "            y_cross_train = y_train[train_idx]\n",
    "            X_cross_val   = X_train[val_idx]\n",
    "            y_cross_val   = y_train[val_idx]\n",
    "            \n",
    "            #create self.theta here\n",
    "            self.theta = np.zeros(X_cross_train.shape[1])\n",
    "            \n",
    "            #define X_cross_train as only a subset of the data\n",
    "            #how big is this subset?  => mini-batch size ==> 50\n",
    "            \n",
    "            #one epoch will exhaust the WHOLE training set\n",
    "            for epoch in range(self.num_epochs):\n",
    "            \n",
    "                #with replacement or no replacement\n",
    "                #with replacement means just randomize\n",
    "                #with no replacement means 0:50, 51:100, 101:150, ......300:323\n",
    "                #shuffle your index\n",
    "                #===> please shuffle your index\n",
    "                perm = np.random.permutation(X_cross_train.shape[0])\n",
    "                        \n",
    "                X_cross_train = X_cross_train[perm]\n",
    "                y_cross_train = y_cross_train[perm]\n",
    "                \n",
    "                if   self.method == 'sto':\n",
    "                    for batch_idx in range(X_cross_train.shape[0]):\n",
    "                        X_method_train = X_cross_train[batch_idx].reshape(1, -1) #(11,) ==> (1, 11) ==> (m, n)\n",
    "                        y_method_train = y_cross_train[batch_idx]                    \n",
    "                        self._train(X_method_train, y_method_train)\n",
    "                elif self.method == 'mini':\n",
    "                    for batch_idx in range(0, X_cross_train.shape[0], self.batch_size):\n",
    "                        #batch_idx = 0, 50, 100, 150\n",
    "                        X_method_train = X_cross_train[batch_idx:batch_idx+self.batch_size, :]\n",
    "                        y_method_train = y_cross_train[batch_idx:batch_idx+self.batch_size]\n",
    "                        self._train(X_method_train, y_method_train)\n",
    "                else:\n",
    "                    X_method_train = X_cross_train\n",
    "                    y_method_train = y_cross_train\n",
    "                    self._train(X_method_train, y_method_train)\n",
    "                    \n",
    "            yhat_val = self.predict(X_cross_val)\n",
    "            self.kfold.append(mean_squared_error(y_cross_val, yhat_val))\n",
    "            print(f\"Fold {fold}: {mean_squared_error(y_cross_val, yhat_val)}\")\n",
    "                    \n",
    "    def _train(self, X, y):\n",
    "        yhat = self.predict(X)\n",
    "        grad = X.T @(yhat - y)\n",
    "        self.theta = self.theta - self.alpha * grad\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X @ self.theta  #===>(m, n) @ (n, )\n",
    "    \n",
    "    def _coef(self):\n",
    "        return self.theta[1:]  #remind that theta is (w0, w1, w2, w3, w4.....wn)\n",
    "                               #w0 is the bias or the intercep\n",
    "                               #w1....wn are the weights / coefficients / theta\n",
    "        \n",
    "    def _bias(self):\n",
    "        return self.theta[0]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(method='sto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: 5620.27956941824\n",
      "Fold 1: 5768.666315412345\n",
      "Fold 2: 4629.721164928177\n",
      "Fold 3: 5450.907534123112\n",
      "Fold 4: 5116.584108221089\n"
     ]
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5620.27956941824,\n",
       " 5768.666315412345,\n",
       " 4629.721164928177,\n",
       " 5450.907534123112,\n",
       " 5116.584108221089]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5593.9083192895405"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.33106786,  -2.715477  ,  22.68812407,  15.15014952,\n",
       "        -4.25690344,  -6.56984262, -10.44804581,   4.7615443 ,\n",
       "        16.47253632,   9.5670414 ])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr._coef()  #the weight associated with the ten features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age has 1.33\n",
    "sex has -2.7\n",
    "bmi has 22.688\n",
    "bp  has 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108.85318639690865"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr._bias() #the bias or the intercept "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 ('teaching_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "becc4c8e5ad229b2591d820334d85e3db0111492344629bf57f272470dce75a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
