{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science and Artificial Intelligence\n",
    "\n",
    "## Classification - AdaBoost\n",
    "\n",
    "### Readings:\n",
    "- [GERON] Ch7\n",
    "- [VANDER] Ch5\n",
    "- [HASTIE] Ch16\n",
    "- https://scikit-learn.org/stable/modules/ensemble.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "\n",
    "**AdaBoost** is a boosting algorithm that try to take a weak classifier on top of one another, **boosting** the overall performance. AdaBoost is extremely simple to use and implement, and often gives very effective results. There is tremendous flexibility in the choice of weak classifier as well. Anyhow, Decision Tree with <code>max_depth=1</code> and <code>max_leaf_nodes=2</code> are often used (also known as **stump**)\n",
    "\n",
    "<img src = \"../../Figures/ada.png\" />\n",
    "\n",
    "Suppose we are given training data ${(\\mathbf{x_i}, y_i)}$, where $\\mathbf{x_i} \\in \\mathbb{R}^n$ and $y_i \\in \\{-1, 1\\}$.  And we have $S$ number of weak classifiers, denoted $h_s(x)$.  For each classifier, we define $\\alpha_s$ as the *voting power* of the classifier $h_s(x)$. Then, the hypothesis function is based on a linear combination of the weak classifier and is written as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h(x) & = \\text{sign}\\big(\\alpha_1h_1(x) + \\alpha_2h_2(x) + \\cdots + \\alpha_sh_s(x) )\\big) \\\\\n",
    "& = \\text{sign}\\big(\\sum_{s=1}^{S}\\alpha_sh_s(x)\\big)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Our job is to find the optimal $\\alpha_s$, so we can know which classifier we should give more weightage (i.e., believe more) in our hypothesis function since their accuracy is relatively better compared to other classifiers.  To get this alpha, we should define what is \"good\" classifier.  This is simple, since good classifier should simply has the minimum weighted errors as:\n",
    "\n",
    "$$\\epsilon_s = \\sum_{i=1}^m w_i^{s}I(h_s(x_i) \\neq y_i) $$\n",
    "\n",
    "in which the weights are initialized in the beginning as\n",
    "\n",
    "$$w_i^{(1)} = \\frac{1}{m}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, given $h(x)$ as <code>yhat</code> and <code>y</code> as the real y, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = np.array([-1,  1, -1, 1, 1]) #(h_s(x))\n",
    "y    = np.array([ 1,  1,  1, 1, 1])\n",
    "(yhat != y).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate its weighted errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2 0.2 0.2 0.2 0.2]\n"
     ]
    }
   ],
   "source": [
    "m = 5 #since we have five samples\n",
    "\n",
    "#initially, we set our weight to 1/m\n",
    "W = np.full(m, 1/m)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    }
   ],
   "source": [
    "err = W[(yhat != y)].sum()\n",
    "print(err.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to change our weight bigger for the first one, you will see that the final error is enlarged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "W = np.array([0.7, 0.2, 0.2, 0.2, 0.2])\n",
    "err = W[(yhat != y)].sum()\n",
    "print(err.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we perform the first classifier, we update the weights of the samples using this formula:\n",
    "\n",
    "$$w_i^{(s+1)} = \\frac{w_i^{(s)}e^{ -\\alpha_sh_s(\\mathbf{x_i}) y_i}}{{\\displaystyle\\sum_{i=1}^m w_i^{s}}} $$\n",
    "\n",
    "where $\\alpha_s$ is:\n",
    "\n",
    "$$\\alpha_s = \\frac{1}{2}\\ln\\frac{1-\\epsilon_s}{\\epsilon_s}$$\n",
    "\n",
    "First, to see why this formula works, let's plot alpha against errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nHigher the error, lower is alpha, which means we don't trust that classifier.  And vice versa.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "err = np.linspace(0, 0.99, 10000) #select only 0.99 not 1 because np.log(1-1=0) = inf\n",
    "err = err + 0.00001 #prevent divide by zero\n",
    "a_j = np.log ((1 - err) / err) * 0.5\n",
    "\n",
    "plt.plot(err, a_j)\n",
    "plt.xlabel(\"Error\")\n",
    "plt.ylabel(\"Alpha\")\n",
    "\n",
    "'''\n",
    "Higher the error, lower is alpha, which means we don't trust that classifier.  And vice versa.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, let's look at the update rule on why it works:\n",
    "\n",
    "$$w_i^{(s+1)} = \\frac{w_i^{(s)}e^{ -\\alpha_sh_s(\\mathbf{x_i}) y_i}}{{\\displaystyle\\sum_{i=1}^m w_i^{s}}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1] [-1  1 -1  1  1]\n"
     ]
    }
   ],
   "source": [
    "#recall\n",
    "print(y, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    }
   ],
   "source": [
    "#recall the err\n",
    "W = np.full(m, 1/m)\n",
    "err = W[(yhat != y)].sum()\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2027325540540821\n"
     ]
    }
   ],
   "source": [
    "#calculate the a_j\n",
    "a_j = np.log ((1 - err) / err) * 0.5\n",
    "print(a_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1, -1,  1,  1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.20273255, -0.20273255,  0.20273255, -0.20273255, -0.20273255])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-a_j * yhat * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.22474487, 0.81649658, 1.22474487, 0.81649658, 0.81649658])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(-a_j * yhat * y)  #notice the scalar is bigger for incorrectly classified sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24494897 0.16329932 0.24494897 0.16329932 0.16329932]\n"
     ]
    }
   ],
   "source": [
    "W = (W * np.exp(-a_j * yhat * y))/sum(W)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything together:\n",
    "\n",
    "1. Loop through all features, threshold, and polarity, identify the best stump which has lowest weighted errors.\n",
    "\n",
    "2. Calculate alpha of the first classifier\n",
    "\n",
    "$$\\alpha_s = \\frac{1}{2}ln\\frac{1-\\epsilon_s}{\\epsilon_s}$$\n",
    "\n",
    "3. Exaggerate the incorrect samples using\n",
    "\n",
    "$$w_i^{(s+1)} = \\frac{w_i^{(s)}e^{ -\\alpha_sh_s(\\mathbf{x_i}) y_i}}{{\\displaystyle\\sum_{i=1}^m w_i^{s}}} $$\n",
    "\n",
    "4. Repeat 1.\n",
    "\n",
    "5. We stop 1-4 using max_iter, early stopping, or number of classifiers.\n",
    "\n",
    "6. To predict, we use the hypothesis function:\n",
    "\n",
    "$$ \n",
    "  H(x) = \\text{sign}\\big(\\sum_{s=1}^{S}\\alpha_sh_s(x)\\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=500, random_state=1)\n",
    "y = np.where(y==0,-1,1)  #change our y to be -1 if it is 0, otherwise 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.96      0.97      0.97        79\n",
      "           1       0.97      0.96      0.96        71\n",
      "\n",
      "    accuracy                           0.97       150\n",
      "   macro avg       0.97      0.97      0.97       150\n",
      "weighted avg       0.97      0.97      0.97       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "m = X_train.shape[0]\n",
    "S = 20\n",
    "stump_params = {'max_depth': 1, 'max_leaf_nodes': 2}\n",
    "models = [DecisionTreeClassifier(**stump_params) for _ in range(S)]\n",
    "\n",
    "#initially, we set our weight to 1/m\n",
    "W = np.full(m, 1/m)\n",
    "\n",
    "#keep collection of a_j\n",
    "a_js = np.zeros(S)\n",
    "\n",
    "for j, model in enumerate(models):\n",
    "    \n",
    "    #train weak learner\n",
    "    model.fit(X_train, y_train, sample_weight = W)\n",
    "    \n",
    "    #compute the errors\n",
    "    yhat = model.predict(X_train) \n",
    "    err = W[(yhat != y_train)].sum()\n",
    "        \n",
    "    #compute the predictor weight a_j\n",
    "    #if predictor is doing well, a_j will be big\n",
    "    a_j = np.log ((1 - err) / err) / 2\n",
    "    a_js[j] = a_j\n",
    "    \n",
    "    #update sample weight; divide sum of W to normalize\n",
    "    W = (W * np.exp(-a_j * y_train * yhat)) \n",
    "    W = W / sum (W)\n",
    "    \n",
    "        \n",
    "#make weighted predictions\n",
    "Hx = 0\n",
    "for i, model in enumerate(models):\n",
    "    yhat = model.predict(X_test)\n",
    "    Hx += a_js[i] * yhat\n",
    "    \n",
    "yhat = np.sign(Hx)\n",
    "\n",
    "print(classification_report(y_test, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn implements AdaBoost using SAMME which stands for Stagewise Additive Modeling using a Multiclass Exponential Loss Function.\n",
    "\n",
    "The following code trains an AdaBoost classifier based on 200 Decision stumps.  A Decision stump is basically a Decision Tree with max_depth=1.  This is the default base estimator of AdaBoostClassifier class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ada score:  0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "#SAMME.R - a variant of SAMME which relies on class probabilities \n",
    "#rather than predictions and generally performs better\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    learning_rate=0.5, random_state=42)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "y_pred = ada_clf.predict(X_test)\n",
    "print(\"Ada score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===Task===\n",
    "\n",
    "Your work: Let's modify the above scratch code:\n",
    "- Notice that if <code>err</code> = 0, then $\\alpha$ will be undefined, thus attempt to fix this by adding some very small value to the lower term\n",
    "- Notice that sklearn version of AdaBoost has a parameter <code>learning_rate</code>.  This is in fact the $\\frac{1}{2}$ in front of the $\\alpha$ calculation.  Attempt to change this $\\frac{1}{2}$ into a parameter called <code>eta</code>, and try different values of it and see whether accuracy is improved.  Note that sklearn default this value to 1.\n",
    "- Observe that we are actually using sklearn DecisionTreeClassifier.  If we take a look at it closely, it is actually using weighted gini index, instead of weighted errors that we learn above.   Attempt to write your own class of <code>class Stump</code> that actually uses weighted errors, instead of weighted gini index\n",
    "- Put everything into a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
