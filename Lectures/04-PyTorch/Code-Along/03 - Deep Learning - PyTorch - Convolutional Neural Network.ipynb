{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science and Artificial Intelligence\n",
    "\n",
    "## Deep Learning -  PyTorch III - Convolutional Neural Network\n",
    "\n",
    "- [WEIDMAN] Ch7\n",
    "- https://pytorch.org/tutorials/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last class we use linear network for classifying MNIST data.  Today we shall propose CNN (Convolutional Neural Network) as a better way for dealing with image classification.\n",
    "\n",
    "There are mainly three layers that can help dealing with images:\n",
    "\n",
    "1. Convolutional layer\n",
    "2. Max/Average pooling layer\n",
    "3. BatchNorm layer\n",
    "4. Dropout layer\n",
    "\n",
    "### 1. Convolutional Layer\n",
    "Let's say given a image of 14 x 14 pixels = 196 features like this.  Each data point is an array of numbers describing how dark each pixel is, where value range from 0 to 255.  These values can be normalized ranging from 0 to 1. For example, for the following digit (the digit 1), we could have:\n",
    "\n",
    "<img src =\"../../figures/one.png\" width=\"250\">\n",
    "\n",
    "It is first important to define the input shape of an image, which will be <code>(input channels, image height, image width)</code>.  If we have lots of images, the input shall be <code>(batch size, input channels, image height, image width)</code>.  For our case, if it is a grayscale image, the shape is <code>(1, 14, 14)</code>.  If it is a RGB image, it shall be <code>(3, 14, 14)</code>.  If it is a CMYK, it shall be <code>(4, 14, 14)</code>.  If I define batch size as 500 (out of many more images I have), my input is <code>(500, 4, 14, 14)</code>.  (Commonly, batch size is around few hundreds).\n",
    "\n",
    "Convolutional network works on the central concept of a convolution operation like this:\n",
    "\n",
    "<img src =\"../../figures/no_padding_no_strides.gif\" width=\"150\">\n",
    "\n",
    "Mathematically, it looks like this:\n",
    "\n",
    "Let's say we have a 5 x 5 input image $I$ of channel 0 of batch 0:\n",
    "\n",
    "$$ I = \\begin{bmatrix}\n",
    "i_{11} & i_{12} & i_{13} & i_{14} & i_{15}\n",
    "\\\\\n",
    "i_{21} & i_{22} & i_{23} & i_{24} & i_{25}\n",
    "\\\\\n",
    "i_{31} & i_{32} & i_{33} & i_{34} & i_{35}\n",
    "\\\\\n",
    "i_{41} & i_{42} & i_{43} & i_{44} & i_{45}\n",
    "\\\\\n",
    "i_{51} & i_{52} & i_{53} & i_{54} & i_{55}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Each of this pixel may represent the brightness ranging from 0 to 255.  Or if normalized, shall be 0 to 1.\n",
    "\n",
    "If we define a 3 x 3 patch which we commonly called **weights (W)** or in computer vision, we called **filters/kernels** like this (*we shall called filters in this lecture note for simplicity*) :\n",
    "\n",
    "$$ W = \\begin{bmatrix}\n",
    "w_{11} & w_{12} & w_{13}\n",
    "\\\\\n",
    "w_{21} & w_{22} & w_{23}\n",
    "\\\\\n",
    "w_{31} & w_{32} & w_{33}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let's say we are scanning the middle of the image, then the output feature would be (we'll denote this as $o_{33}$):\n",
    "\n",
    "$$o_{33} = w_{11} * i_{22} + w_{12} * i_{23} + w_{13} * i_{24} + \\\n",
    "           w_{21} * i_{32} + w_{22} * i_{32} + w_{23} * i_{34} + \\\n",
    "           w_{32} * i_{43} + w_{33} * i_{44}$$\n",
    "           \n",
    "This will result in one output feature called **feature map**.  Of course, we may add bias to it and then will be fed through an activation function.\n",
    "\n",
    "Actual feature maps look like this.  Each feature map is a output of a single training example and convolve each kernel over the sample.    In simple words, if we have $k$ filters, then we have $k$ feature maps.  They represent the activation part corresponding to the kernels.\n",
    "\n",
    "<img src =\"../../figures/feature-map2.png\" width=\"450\">\n",
    "\n",
    "In a convolution operation, there are 3 main hyperparameters to fine tune - (1) filter size, (2) padding, and (3) stride.\n",
    "\n",
    "#### A. Filters\n",
    "\n",
    "1. **How the filters look like?**.  It turns out that each filter actually detect the presence of certain visual pattern.  For example, this filter below detects whether there is an edge at that location of the image.  There are also other similar filters detecting corners, lines, etc.  Check out https://setosa.io/ev/image-kernels/  and try changing the values\n",
    "\n",
    "$$ w = \\begin{bmatrix}\n",
    "0 & 1 & 0\n",
    "\\\\\n",
    "1 & -4 & 1\n",
    "\\\\\n",
    "0 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Real filters can look like this.  They may look somewhat random at first glance, but we can see that clear structure being learned in most kernels. For example, filters 3 and 4 seem to be learning diagonal edges in opposite directions, and other capture round edges or enclosed spaces:\n",
    "\n",
    "<img src =\"../../figures/kernels.png\" width=\"200\">\n",
    "\n",
    "However, **it is important to note that we DON'T need to decide the filters** to use.  We can simply feed a random generated filter, and it is the job of CNN to learn these filters.   These learned filters will learn what features are most efficient for the classification process.\n",
    "\n",
    "**What is the shape of filters?**.  For each image, we can apply multiple filters, depending on how many output channels we want.  Let's say the input channel is 3, and we want the output channel to 64, then we apply a filter of size <code>(3, 64, filter width, filter height)</code>.  How do we know how many output channel to use? The answer is we don't know...we just try and see what works.  More filter allows the network to look at more patterns.\n",
    "\n",
    "**What should be the filter size?**  If we use a 3 x 3 filter, each pixel got 8 neighboring information.  On the other hand, if we we use big filter like 9 x 9, then we got 80 neighboring information.  Typical size is 3 and 5.\n",
    "\n",
    "\n",
    "#### B. Padding\n",
    "\n",
    "2. **How should we convolve the edges?**. Recall this image:\n",
    "\n",
    "<img src =\"../../figures/no_padding_no_strides.gif\" width=\"150\">\n",
    "\n",
    "It has 4 x 4 pixels = 16 features.  But after convolution, we only got 2 x 2 pixels = 4 features left.  Is that good?  There are no correct answers here but we are quite sure that we lose some information.  One way is address this is **padding**, where we can enlarge the input image by padding the surroundings with zeros.  How much?  Padding until we get the original size or larger size, for example, like this.  The below put zero padding around which result the output features to be the same size as input features.\n",
    "\n",
    "<img src =\"../../figures/same_padding_no_strides.gif\" width=\"150\">\n",
    "\n",
    "The below put even more padding which pad to make sure each single pixel is convoluted (full padding), which result the output features to be even large\n",
    "\n",
    "<img src =\"../../figures/full_padding_no_strides.gif\" width=\"150\">\n",
    "\n",
    "#### C. Strides\n",
    "\n",
    "3. **How many step we should take to slide our filter? Skip 2?** Should we shift 1 step per convolution, or 2 steps, or how many steps.  **In fact, it really depends on how detail you want it to be.  But defining bigger steps reduce the feature size and thus reduce the computation time.**  Bigger step is like human scanning picture more roughly but can reduce the computation time....whether to use it is something to be experimented though. \n",
    "\n",
    "In computer vision, we called this step as **stride**.  Example is like this:\n",
    "\n",
    "**No padding with stride of 2**\n",
    "\n",
    "<img src =\"../../figures/no_padding_strides.gif\" width=\"150\">\n",
    "\n",
    "**Padding with stride of 2**\n",
    "\n",
    "<img src =\"../../figures/padding_strides.gif\" width=\"150\">\n",
    "\n",
    "Actual image convolution can look like this (with stride 1 and no padding):\n",
    "\n",
    "<img src =\"../../figures/conv.gif\" width=\"400\">\n",
    "\n",
    "The convoluted image may look like this (nothing relate with the above matrix though):\n",
    "\n",
    "<img src =\"../../figures/convimages.png\" width=\"400\">\n",
    "\n",
    "**The formula to be used to measure the padding value to get the spatial size of the input and output volume to be the same with stride 1** is\n",
    "\n",
    "$$ \\frac{K-1}{2} $$\n",
    "\n",
    "where $K$ is the filter size.\n",
    "\n",
    "This means that if our image is size $24 * 24$, and the filter size is $3 x 3$, then our $K$ has size 3 so the padding should be $(3-1)/2 = 1$, then we need to add **a border of one pixel valued 0 around the outside of the image**, which would result in the input image of size $26 * 26$\n",
    "\n",
    "#### D. Shape\n",
    "\n",
    "4. **What would be the shape of the output matrix?**\n",
    "\n",
    "The output shape (denote as $O$) depend on the stride (denote as $S$), padding (denote as $P$), filter size (denote as $F$) as well as the input width and height (denote as $I$). $O$ can be calculated with the formula as follows:\n",
    "\n",
    "$$O = \\frac{W-F+2P}{S} + 1$$\n",
    "\n",
    "In this case (code below), if our W is 28, F is 5, P is 2, and S is 1 then the width/height is 28\n",
    "\n",
    "In conclusion, \n",
    "\n",
    "- The input will have a 4D shape of <code>(batch size, input channels, input height, input width)</code>\n",
    "\n",
    "- The output will have a 4D shape of <code>(batch size, output channels, output height, output width)</code>\n",
    "\n",
    "- The convolutional filters will have 4D shape of <code>(input channels, output channels, filter height, filter width)</code>\n",
    "\n",
    "**Note: The order does not matter and it depends on the python library you use but these four dimensions always exist in CNN.**\n",
    "\n",
    "### 2. Max/Average Pooling Layer\n",
    "\n",
    "Talking about **reducing computation time**, a common way is to perform a **pooling layer** which simply downsample the image by average a set of pixels, or by taking the maximum value.  If we define a pooling size of 2, this involves mapping each 2 x 2 pixels to one output, like this:\n",
    "\n",
    "<img src =\"../../figures/pooling.png\" width=\"150\">\n",
    "\n",
    "Nevertheless, pooling has a really big downsides, i.e., it basically lose a lot of information.  Compared to strides, strides simply scan less but maintain the same resolution but pooling simply reduce the resolution of the images....As Geoffrey Hinton said on Reddit AMA in 2014 - **The pooling operation used in CNN is a big mistake and the fact that it works so well is a disaster**.  In fact, in most recent CNN architectures like ResNets, it uses pooling very minimially or not at all.\n",
    "\n",
    "### 3. BatchNorm Layer\n",
    "\n",
    "Batch norm is nothing other than normalize samples within the batch.  That is, minus the mean of features within the batch.  This helps with unstable gradients in SGD. Note that the output size does not change from input size after BatchNorm.\n",
    "\n",
    "<img src =\"../../figures/batchnorm.png\" width=\"300\">\n",
    "\n",
    "<img src =\"../../figures/landscape.png\" width=\"500\">\n",
    "\n",
    "\n",
    "### 4. Dropout Layer\n",
    "\n",
    "This is a layer of arbitrarily removing some values in your data.  By randomly removing data in each iteration, you make the neural network more robust against overfitting, because it needs to learn to fight with incomplete data.\n",
    "\n",
    "For example, say we have a vector of $x = {1, 2, 3, 4, 5}$.  Let's set $p=0.2$ which means 20\\% of data will be turn to 0.  In training mode, $x_\\text{train} = {1, 0, 3, 4, 5}$ ; do not confuse why I turn off 2 and not others, I just turn 20\\% off randomly.  In evaluation mode, we turn off dropout, but to make sure the distribution remains similar, we multiply the values by $1 - 0.2 = 0.8$, which becomes $x_\\text{inference} = {0.8, 1.6, 2.4, 3.2, 4.0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.cs.ryerson.ca/~aharley/vis/conv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "\n",
    "Here we will be exploring how to use pyTorch for CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice\n",
    "\n",
    "- Since layer1 and layer2 shares very similar code, replace them with a function <code>def _block</code>\n",
    "\n",
    "- Try to load the CIFAR dataset and test your network\n",
    "\n",
    "<code>train_data = datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
    "</code>\n",
    "\n",
    "- Instead of max pooling which result in loss of information, it may be wise to replace that with <code>stride=2</code> instead.\n",
    "\n",
    "- There is a huge debate whether to put BatchNorm before ReLU or after.  The original paper put BatchNorm before ReLU.  I invite you to search for the debate as it's quite fun to read through these debates.  Another debate is whether to put Dropout before or after BatchNorm and ReLU.  Some also debated that BatchNorm is already a regularizer thus dropout is not required.   I invite you to read through these debates.  This paper is also nice to read:  https://arxiv.org/abs/1905.05928"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
