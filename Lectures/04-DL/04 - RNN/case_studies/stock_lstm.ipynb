{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this notebook we will be building and training LSTM to predict IBM stock. We will use PyTorch.","metadata":{"_uuid":"91e741e3-0b5a-4d10-a22e-fab5924337e5","_cell_guid":"fc7bf9a0-e26b-4de5-8db7-7a410a9c31d1","trusted":true}},{"cell_type":"markdown","source":"## 1. Libraries and settings","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport random\nimport pandas as pd \nfrom pylab import mpl, plt\nplt.style.use('seaborn')\nmpl.rcParams['font.family'] = 'serif'\n%matplotlib inline\n\nfrom pandas import datetime\nimport math, time\nimport itertools\nimport datetime\nfrom operator import itemgetter\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom math import sqrt\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for i, filename in enumerate(filenames):\n        if i<5:\n            print(os.path.join(dirname,filename))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Load data","metadata":{}},{"cell_type":"code","source":"def stocks_data(symbols, dates):\n    df = pd.DataFrame(index=dates)\n    for symbol in symbols:\n        df_temp = pd.read_csv(\"../input/Data/Stocks/{}.us.txt\".format(symbol), index_col='Date',\n                parse_dates=True, usecols=['Date', 'Close'], na_values=['nan'])\n        df_temp = df_temp.rename(columns={'Close': symbol})\n        df = df.join(df_temp)\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dates = pd.date_range('2015-01-02','2016-12-31',freq='B')\nsymbols = ['goog','ibm','aapl']\ndf = stocks_data(symbols, dates)\ndf.fillna(method='pad')\ndf.plot(figsize=(10, 6), subplots=True);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dates = pd.date_range('2010-01-02','2017-10-11',freq='B')\ndf1=pd.DataFrame(index=dates)\ndf_ibm=pd.read_csv(\"../input/Data/Stocks/ibm.us.txt\", parse_dates=True, index_col=0)\ndf_ibm=df1.join(df_ibm)\ndf_ibm[['Close']].plot(figsize=(15, 6))\nplt.ylabel(\"stock_price\")\nplt.title(\"IBM Stock\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_ibm=df_ibm[['Close']]\ndf_ibm.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_ibm=df_ibm.fillna(method='ffill')\n\nscaler = MinMaxScaler(feature_range=(-1, 1))\ndf_ibm['Close'] = scaler.fit_transform(df_ibm['Close'].values.reshape(-1,1))\n#df_ibm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to create train, test data given stock data and sequence length\ndef load_data(stock, look_back):\n    data_raw = stock.values # convert to numpy array\n    data = []\n    \n    # create all possible sequences of length look_back\n    for index in range(len(data_raw) - look_back): \n        data.append(data_raw[index: index + look_back])\n    \n    data = np.array(data);\n    test_set_size = int(np.round(0.2*data.shape[0]));\n    train_set_size = data.shape[0] - (test_set_size);\n    \n    x_train = data[:train_set_size,:-1,:]\n    y_train = data[:train_set_size,-1,:]\n    \n    x_test = data[train_set_size:,:-1]\n    y_test = data[train_set_size:,-1,:]\n    \n    return [x_train, y_train, x_test, y_test]\n\nlook_back = 60 # choose sequence length\nx_train, y_train, x_test, y_test = load_data(df_ibm, look_back)\nprint('x_train.shape = ',x_train.shape)\nprint('y_train.shape = ',y_train.shape)\nprint('x_test.shape = ',x_test.shape)\nprint('y_test.shape = ',y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make training and test sets in torch\nx_train = torch.from_numpy(x_train).type(torch.Tensor)\nx_test = torch.from_numpy(x_test).type(torch.Tensor)\ny_train = torch.from_numpy(y_train).type(torch.Tensor)\ny_test = torch.from_numpy(y_test).type(torch.Tensor)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.size(),x_train.size()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Build the structure of model","metadata":{}},{"cell_type":"code","source":"# Build model\n#####################\ninput_dim = 1\nhidden_dim = 32\nnum_layers = 2 \noutput_dim = 1\n\n\n# Here we define our model as a class\nclass LSTM(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n        super(LSTM, self).__init__()\n        # Hidden dimensions\n        self.hidden_dim = hidden_dim\n\n        # Number of hidden layers\n        self.num_layers = num_layers\n\n        # batch_first=True causes input/output tensors to be of shape\n        # (batch_dim, seq_dim, feature_dim)\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n\n        # Readout layer\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        # Initialize hidden state with zeros\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n\n        # Initialize cell state\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n\n        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n        # If we don't, we'll backprop all the way to the start even after going through another batch\n        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n\n        # Index hidden state of last time step\n        # out.size() --> 100, 32, 100\n        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n        out = self.fc(out[:, -1, :]) \n        # out.size() --> 100, 10\n        return out\n    \nmodel = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)\n\nloss_fn = torch.nn.MSELoss()\n\noptimiser = torch.optim.Adam(model.parameters(), lr=0.01)\nprint(model)\nprint(len(list(model.parameters())))\nfor i in range(len(list(model.parameters()))):\n    print(list(model.parameters())[i].size())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train model\n#####################\nnum_epochs = 100\nhist = np.zeros(num_epochs)\n\n# Number of steps to unroll\nseq_dim =look_back-1  \n\nfor t in range(num_epochs):\n    # Initialise hidden state\n    # Don't do this if you want your LSTM to be stateful\n    #model.hidden = model.init_hidden()\n    \n    # Forward pass\n    y_train_pred = model(x_train)\n\n    loss = loss_fn(y_train_pred, y_train)\n    if t % 10 == 0 and t !=0:\n        print(\"Epoch \", t, \"MSE: \", loss.item())\n    hist[t] = loss.item()\n\n    # Zero out gradient, else they will accumulate between epochs\n    optimiser.zero_grad()\n\n    # Backward pass\n    loss.backward()\n\n    # Update parameters\n    optimiser.step()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(hist, label=\"Training loss\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.shape(y_train_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make predictions\ny_test_pred = model(x_test)\n\n# invert predictions\ny_train_pred = scaler.inverse_transform(y_train_pred.detach().numpy())\ny_train = scaler.inverse_transform(y_train.detach().numpy())\ny_test_pred = scaler.inverse_transform(y_test_pred.detach().numpy())\ny_test = scaler.inverse_transform(y_test.detach().numpy())\n\n# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(y_train[:,0], y_train_pred[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(y_test[:,0], y_test_pred[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualising the results\nfigure, axes = plt.subplots(figsize=(15, 6))\naxes.xaxis_date()\n\naxes.plot(df_ibm[len(df_ibm)-len(y_test):].index, y_test, color = 'red', label = 'Real IBM Stock Price')\naxes.plot(df_ibm[len(df_ibm)-len(y_test):].index, y_test_pred, color = 'blue', label = 'Predicted IBM Stock Price')\n#axes.xticks(np.arange(0,394,50))\nplt.title('IBM Stock Price Prediction')\nplt.xlabel('Time')\nplt.ylabel('IBM Stock Price')\nplt.legend()\nplt.savefig('ibm_pred.png')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}