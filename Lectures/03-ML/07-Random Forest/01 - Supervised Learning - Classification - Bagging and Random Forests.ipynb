{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science and Artificial Intelligence\n",
    "\n",
    "## Supervised Learning - Classification - Bagging and Random Forests\n",
    "\n",
    "### Readings: \n",
    "- [GERON] Ch7\n",
    "- [VANDER] Ch5\n",
    "- [HASTIE] Ch15\n",
    "- https://scikit-learn.org/stable/modules/ensemble.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "A single decision tree does not perform well as it tends to overfit.  A possible solution is the construct multiple trees to reduce variances.  To make sure each tree is not exactly learning the same thing since it will then be all same trees, we need to inject some differences to these trees (i.e., make them as diverse as possible but at the same time they also see some overlappinp samples).  One simple idea is that each of the tree is trained on a subset of **bootstrapping sample** and then perform some sort of aggregation of the decision.\n",
    "\n",
    "The process has the following steps:\n",
    "\n",
    "1. Sample $m$ times **with replacement** from the original training data\n",
    "2. Repeat $B$ times to generate $B$ \"boostrapped\" training datasets $D_1, D_2, \\cdots, D_B$\n",
    "3. Train $B$ trees using the training datasets $D_1, D_2, \\cdots, D_B$ \n",
    "\n",
    "Boostrapping the data plus performing some sort of aggregation (averaging or majority votes) is called **boostrap aggregation** or **bagging**.\n",
    "\n",
    "*Example*:\n",
    "\n",
    "Assume that we have a training set where $m=4$, and $n=2$:\n",
    "\n",
    "$$D = {(x_1, y_1), (x_2, y_2), (x_3, y_3), (x_4, y_4)}$$\n",
    "\n",
    "We generate, say, $B = 3$ datasets by boostrapping:\n",
    "\n",
    "$$D_1 = {(x_1, y_1), (x_2, y_2), (x_3, y_3), (x_3, y_3)}$$\n",
    "$$D_2 = {(x_1, y_1), (x_4, y_4), (x_4, y_4), (x_3, y_3)}$$\n",
    "$$D_3 = {(x_1, y_1), (x_1, y_1), (x_2, y_2), (x_2, y_2)}$$\n",
    "\n",
    "We can then train 3 trees.\n",
    "\n",
    "Note: When sampling is performed **without** replacement, it is called **pasting**.  In other words, both bagging and pasting allow training instances to be sampled several times across multiple predictors, but only bagging allows training instances to be sampled several times for the same predictor.\n",
    "\n",
    "Let's try to code from scratch.  To make our life easier, we shall use DecisionTree from the sklearn library (since we already code it from scratch in the previous class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                test_size=0.3, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  [[1. 0. 2. 1. 1. 0. 1. 2. 1. 1. 2. 0. 0. 0. 0. 1. 2. 1. 1. 2. 0. 2. 0. 2.\n",
      "  2. 2. 2. 2. 0. 0. 0. 0. 1. 0. 0. 2. 1. 0. 0. 0. 2. 1. 1. 0. 0.]\n",
      " [1. 0. 2. 1. 1. 0. 1. 2. 1. 1. 2. 0. 0. 0. 0. 1. 2. 1. 1. 2. 0. 1. 0. 2.\n",
      "  2. 2. 2. 2. 0. 0. 0. 0. 1. 0. 0. 2. 1. 0. 0. 0. 2. 1. 1. 0. 0.]\n",
      " [1. 0. 2. 1. 1. 0. 1. 2. 1. 1. 2. 0. 0. 0. 0. 1. 2. 1. 1. 2. 0. 2. 0. 2.\n",
      "  2. 2. 2. 2. 0. 0. 0. 0. 1. 0. 0. 2. 1. 0. 0. 0. 2. 1. 1. 0. 0.]\n",
      " [1. 0. 2. 1. 1. 0. 1. 2. 1. 1. 2. 0. 0. 0. 0. 1. 2. 1. 1. 2. 0. 2. 0. 2.\n",
      "  2. 2. 2. 2. 0. 0. 0. 0. 1. 0. 0. 2. 1. 0. 0. 0. 2. 1. 1. 0. 0.]\n",
      " [1. 0. 2. 1. 1. 0. 1. 2. 1. 1. 2. 0. 0. 0. 0. 1. 2. 1. 1. 2. 0. 2. 0. 2.\n",
      "  2. 2. 2. 2. 0. 0. 0. 0. 1. 0. 0. 2. 1. 0. 0. 0. 2. 1. 1. 0. 0.]]\n",
      "Mode:  [1. 0. 2. 1. 1. 0. 1. 2. 1. 1. 2. 0. 0. 0. 0. 1. 2. 1. 1. 2. 0. 2. 0. 2.\n",
      " 2. 2. 2. 2. 0. 0. 0. 0. 1. 0. 0. 2. 1. 0. 0. 0. 2. 1. 1. 0. 0.]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import random\n",
    "from scipy import stats\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "B = 5\n",
    "m, n = X_train.shape\n",
    "boostrap_ratio = 1\n",
    "tree_params = {'max_depth': 2, 'criterion':'gini', 'min_samples_split': 5}\n",
    "models = [DecisionTreeClassifier(**tree_params) for _ in range(B)]\n",
    "\n",
    "#sample size for each tree\n",
    "sample_size = int(boostrap_ratio * len(X_train))\n",
    "\n",
    "xsamples = np.zeros((B, sample_size, n))\n",
    "ysamples = np.zeros((B, sample_size))\n",
    "\n",
    "#subsamples for each model\n",
    "for i in range(B):\n",
    "    ##sampling with replacement; i.e., sample can occur more than once\n",
    "    #for the same predictor\n",
    "    for j in range(sample_size):\n",
    "        idx = random.randrange(m)   #<----with replacement #change so no repetition\n",
    "        xsamples[i, j, :] = X_train[idx]\n",
    "        ysamples[i, j] = y_train[idx]\n",
    "        #keep track of idx that i did not use for ith tree\n",
    "\n",
    "#fitting each estimator\n",
    "for i, model in enumerate(models):\n",
    "    _X = xsamples[i, :]\n",
    "    _y = ysamples[i, :]\n",
    "    model.fit(_X, _y)\n",
    "    \n",
    "#make prediction and return the probabilities\n",
    "predictions = np.zeros((B, X_test.shape[0]))\n",
    "for i, model in enumerate(models):\n",
    "    yhat = model.predict(X_test)\n",
    "    predictions[i, :] = yhat\n",
    "    \n",
    "#predictions.shape = (B, m_test)\n",
    "    \n",
    "yhat = stats.mode(predictions)[0][0] #the first zero gives us the mode array, the second zero convert from 2D ([[]]) to 1D ([])\n",
    "\n",
    "print(classification_report(y_test, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "'''\n",
    "To perform in sklearn, we can use the BaggingClassifier API.  \n",
    "Pasting can be done using BaggingClassifier< setting boostrap=False\n",
    "'''\n",
    "\n",
    "bag = BaggingClassifier(tree, n_estimators=5, max_samples=0.99)\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "yhat = bag.predict(X_test)\n",
    "print(classification_report(y_test, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===Task===\n",
    "\n",
    "#### Out of Bag Evaluation\n",
    "\n",
    "Well, it seems like our bagging technique is quite good.  Anyhow, one interesting observation is that each tree only see a subset of the dataset. Any data that a particular tree did not see is called **out of bag** (oob).  Note that oob is not the same for all predictors.\n",
    "\n",
    "One interesting thing is that since oob is something that each tree never see, thus oob is somewhat a validation set.  Thus what we can do is after we fit each tree. We can ask each tree to test their accuracy with their own oob, and then we can average the accuracy from all trees.  \n",
    "\n",
    "Let's modify the above scratch code to:\n",
    "- Calculate for oob evaluation for each bootstrapped dataset, and also the average score\n",
    "- Change the code to \"without replacement\"\n",
    "- Put everything into a class <code>Bagging</code>.  It should have at least two methods, <code>fit(X_train, y_train)</code>, and <code>predict(X_test)</code>\n",
    "- Modify the code from above to randomize features.  Set the number of features to be used in each tree to be <code>sqrt(n)</code>, and then select a subset of features for each tree.  This can be easily done by setting our DecisionTreeClassifier <code>max_features</code> to 'sqrt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "So far, it seems bagging works well.  However, the $B$ bootstrapped dataset are correlated, thus the power of variance reduction is diminished.  How do we further de-correlate these $B$ trees?\n",
    "\n",
    "A **random forest** is constructed by bagging, but for each split in each tree, only a random subset of $q \\leq n$ features are considered as splitting variables.\n",
    "\n",
    "Rule of thumb: $q = \\sqrt{n}$ for classification trees and $q = \\frac{n}{3}$ for regression trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch\n",
    "\n",
    "Note: This is actually your code after you finish the 4th task above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'max_depth': 4, 'n_estimators': 100}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#this is the same as RandomForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\"n_estimators\": [10, 50, 100], \n",
    "              \"criterion\": [\"gini\", \"entropy\"],\n",
    "              \"max_depth\": np.arange(1, 10)}\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "grid = GridSearchCV(model, param_grid)\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(grid.best_params_)\n",
    "\n",
    "model = grid.best_estimator_\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use Random Forests\n",
    "\n",
    "Advantages of Random Forest:\n",
    "\n",
    "- Voting helps overcome overfitting\n",
    "- Because bagging and pasting support parallel computing (e.g., using <code>n_jobs</code>), they are very popular methods.\n",
    "- Random forest can solve both type of problems that is classification and regression and does a decent estimation at both fronts.\n",
    "- The power to handle large data sets with higher dimensionality. It can handle thousands of input variables and identity most significant variables so it is considered as one of the dimensionality reduction method. Further, the model outputs importance of variable, which can be a very handy feature.  Sklearn implements <code>feature_importances_</code> in <code>RandomForestClassifier</code> which helps you understand which feature is useful for classification in Random Forest\n",
    "- It has an effective method for estimating missing data and maintains accuracy when large proportion of the data are missing (I did not really touch this, but I recommend you to check it out)\n",
    "- It has methods for balancing errors in data sets where classes are imbalanced.\n",
    "- The capability of the above can be extended to unlabeled data, leading to unsupervised clustering,data views and outlier detection.\n",
    "- Just like other ensemble, it works well with structured/tabular data.  Indeed, XGBoost (another ensemble method) is among the best classifier for structured/tabular data and often used for Kaggle competition.  But if we are working with image, sound, brain signal, deep learning remains the way to go.\n",
    "- Unlike Decision Trees, multiple trees can give out probability\n",
    "- Out of bag evaluation is handy\n",
    "\n",
    "Disadvantages of Random Forest:\n",
    "\n",
    "- It surely does a good job at classification but not as for regression problem as it does not gives precise continuous nature prediction. In case of regression, it doesn't predict beyond the range in the training data, and that they may over fit data sets that are particularly noisy.\n",
    "- Random forest can feel like a black box approach for a statistical modelers we have very little control on what the model does. You can at best try different parameters and random seeds.\n",
    "- At one point, more samples will not improve the accuracy, unlike deep neural network\n",
    "- It fails when there are rare outcomes or rare predictors, as the algorithm is based on bootstrap sampling. This makes it non-ideal if you're working with rare personality traits, high segmented customer behavior, or rare variants in genomics research.\n",
    "\n",
    "In conclusion, if you are working with structured/tabular data, and would like high accuracy but does not care much about interpretability (just like most Kaggle competition does), you may want to use ensemble methods (including Random Forests and the like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
