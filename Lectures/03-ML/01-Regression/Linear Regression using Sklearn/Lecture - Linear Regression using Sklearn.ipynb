{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science and Artificial Intelligence\n",
    "\n",
    "## Linear Regression using Sklearn\n",
    "\n",
    "### Readings: \n",
    "- https://scikit-learn.org/stable/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Scikit-Learn](http://scikit-learn.org) provides quick access to a huge pool of machine learning algorithms.\n",
    "\n",
    "Before using sklearn, there is **one thing you need to know**, i.e., the **data shape that sklearn wants**.\n",
    "\n",
    "To apply majority of the algorithms, sklearn requires two inputs, i.e., $\\mathbf{X}$ and $\\mathbf{y}$.\n",
    "\n",
    "-  $\\mathbf{X}$, or the **feature matrix** *typically* has the shape of ``[n_samples, n_features]``\n",
    "-  $\\mathbf{y}$, or the **target/label vector** *typically* has the shape of ``[n_samples, ]`` or ``[n_samples, n_targets]`` depending whether that algorithm supports multiple labels\n",
    "\n",
    "<img src = \"figures/shape.png\">\n",
    "\n",
    "Note 1:  if you $\\mathbf{X}$ has only 1 feature, the shape must be ``[n_samples, 1]`` NOT ``[n_samples, ]``\n",
    "\n",
    "Note 2:  sklearn supports both numpy and pandas, as long as the shape is right.  For example, if you use pandas, $\\mathbf{X}$ would be a dataframe, and $\\mathbf{y}$ could be a series or dataframe.\n",
    "\n",
    "Tips:  it's always better to look at sklearn documentation before applying any algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics of the sklearn API\n",
    "\n",
    "Most commonly, the steps in using the Scikit-Learn API are as follows:\n",
    "\n",
    "1. Import a class of model\n",
    "2. Choose model hyperparameters by instantiating this class with desired values.\n",
    "3. Arrange data into a features matrix and target vector following the discussion above.\n",
    "4. Fit the model to your data by calling the ``fit()`` method of the model instance.\n",
    "5. Perform inference using the ``predict()`` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try!\n",
    "\n",
    "Before anything, let's load a toy regression dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import a class of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Choose model hyperparameters\n",
    "\n",
    "For our linear regression example, we can instantiate the ``LinearRegression`` class and specify that we would like to fit the intercept using the ``fit_intercept`` hyperparameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression(fit_intercept=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Arrange data into a features matrix and target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert len(X_train.shape) == 2 and len(X_test.shape) == 2  #correct shape!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(y_train.shape) == 1 and len(y_test.shape) == 1  #correct shape!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Fit the model to your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)  #when you train your model, use your training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ``fit()`` command causes a number of model-dependent internal computations to take place, and the results of these computations are stored in model-specific attributes that the user can explore.\n",
    "In Scikit-Learn, by convention all model parameters that were learned during the ``fit()`` process have trailing underscores; for example in this linear model, we have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -0.87547741,  -9.26843436,  23.24283857,  18.52697371,\n",
       "       -30.72400693,  17.48415588,   2.06334652,   7.45377418,\n",
       "        33.45802239,   2.61129274])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151.29126213592232"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two parameters represent the slope and intercept of the simple linear fit to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Predict labels for unknown data\n",
    "\n",
    "Once the model is trained, we can now evaluate our model which is called **inference** or **testing**.  Usually we do this with test set (but here we are just lazy for simplicity).  \n",
    "\n",
    "In Scikit-Learn, this can be done using the ``predict()`` method.\n",
    "For the sake of this example, our \"new data\" will be a grid of *x* values, and we will ask what *y* values the model predicts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(X_test)  #inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3180.1621027594665"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute mean squared error\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# mean_squared_error(y_true, y_pred)\n",
    "mean_squared_error(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it's very close to what we got before, using our code from scratch, but with 10x fewer lines :-)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "Having only two sets, i.e., training and test set is NOT recommended because:\n",
    "\n",
    "1. What if I want to check which hyperparameter is good?  How to check when I should NEVER touch test set?\n",
    "\n",
    "2. What if somehow I got lucky with my split and my training set is very good, and my test set is also very good, just **by chance*?\n",
    " \n",
    "The recommended way is to do **cross-validation**\n",
    "\n",
    "- **Idea**:  further **split the training set into actual training set and validation set**.  To make sure we don't get lucky with our validation set, we do this split either randomly or walkforward like in this picture:\n",
    "\n",
    "<img width=\"400\" src = \"figures/cv.png\" >\n",
    "\n",
    "Here we split the data into five groups, and use each of them in turn to evaluate the model fit on the other 4/5 of the data.  This would be rather tedious to do by hand, and so we can use Scikit-Learn's ``cross_val_score`` convenience routine to do it succinctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.21229232, 0.45349073, 0.51633375, 0.58399592, 0.64328844])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(model, X_train, y_train, cv=5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, if we specify cv with integer, the <code>cross_val_score</code> uses KFold strategies by default (KFold is basically the picture above).  We can also manually specify the CV strategies we want.\n",
    "\n",
    "<img width=\"400\" src = \"figures/kfold.png\">\n",
    "\n",
    "For example, **ShuffleSplit**:\n",
    "\n",
    "ShuffleSplit is a good alternative to KFold cross validation that allows a finer control on the number of iterations and the proportion of samples on each side of the train / test split.\n",
    "\n",
    "<img width=\"400\" src = \"figures/shuffle.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.55653483, 0.61272515, 0.49418558, 0.2213325 , 0.40555013])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "shuffle_cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)  #splitting in a randomized way\n",
    "cross_val_score(model, X_train, y_train, cv=shuffle_cv) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common strategy is **Stratified KFold**\n",
    "\n",
    "StratifiedKFold is a variation of k-fold which returns stratified folds: **each set contains approximately the same percentage of samples of each target class**.\n",
    "\n",
    "<img width=\"400\" src = \"figures/skfold.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaklam/DSAI/Environments/teaching_env/lib/python3.8/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.30962551, 0.5635614 , 0.59665989])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold #mostly used for classification\n",
    "\n",
    "sk_cv = StratifiedKFold(n_splits=3)  #there's also stratified shuffle kfold!\n",
    "cross_val_score(model, X_train, y_train, cv=sk_cv) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another strategy is **Group KFold**:\n",
    "\n",
    "Very useful if you don't want the same group in both training and validation set.\n",
    "\n",
    "<img width=\"400\" src = \"figures/groupkfold.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(309,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.53971909, 0.44107877, 0.52276067, 0.47754004, 0.4587158 ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)  #there's also group shuffle kfold!\n",
    "\n",
    "#we have to specify the group\n",
    "#let's specify, just for the sake\n",
    "groups = np.random.randint(0, 5, size=y_train.shape[0])\n",
    "print(groups.shape)\n",
    "#print(groups)\n",
    "\n",
    "cross_val_score(model, X_train, y_train, cv=gkf, groups=groups) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details, read https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation\n",
    "\n",
    "Note: **one big reminder is that cross-validation is for finding optimal hyperparameters/models, you still need to evaluate with final test set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double/Nested Cross-validation\n",
    "\n",
    "Recall the two problems we have:\n",
    "\n",
    "1. What if I want to check which hyperparameter is good?  How to check when I should NEVER touch test set?\n",
    "\n",
    "2. What if somehow I got lucky with my split and my training set is very good, and my test set is also very good, just **by chance*?\n",
    "\n",
    "Actually, we solved number 1 only!\n",
    "\n",
    "How about number 2?  Because we may be lucky with our testing set!\n",
    "\n",
    "**Idea: put another loop when we initally split training and test set, i.e., we now have two loops**\n",
    "\n",
    "<img width=\"400\" src = \"figures/nestedcv.png\">\n",
    "\n",
    "**Then the final performance is simply the average of all outerloop performance, instead of testing the model with final test set, because here, we don't have final test set**\n",
    "\n",
    "To do nested cross-validation, let's first learn <code>GridSearch</code> which will be needed for doing nested cross-validation quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    "Recall that we can compare models/hyperparameters using <code>cross_val_score</code>, right?   But this can be tiring....Scikit-Learn provides automated tools to do this called <code>GridSearchCV</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=5, random_state=0, test_size=0.3, train_size=None),\n",
       "             estimator=LinearRegression(),\n",
       "             param_grid={'fit_intercept': [True, False],\n",
       "                         'positive': [True, False]})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'fit_intercept': [True, False],\n",
    "              'positive': [True, False]}\n",
    "\n",
    "#GridSearchCV(algorithm, dictionary, cross-validation strategy)\n",
    "grid = GridSearchCV(model, param_grid, cv=shuffle_cv, refit=True)\n",
    "\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that this is fit, we can ask for the best parameters as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_intercept': True, 'positive': False}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we got the new model, we can test it on the final final test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3180.1621027594665"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = grid.predict(X_test)  #note that here i can use grid right away, because i specify refit=True\n",
    "mean_squared_error(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please note that there is also other form of gridsearchcv such as randomizedgridsearch which can be more efficient.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coming back to nested cross-validation\n",
    "\n",
    "Once we learn about grid search, we can utilize both <code>grid search</code> and <code>cross_val_score</code> to perform nested cross validation like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2903.10000132, -3315.33400852, -3144.27434507, -3045.8808507 ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#specify the inner cv and outer cv\n",
    "inner_cv = KFold(n_splits=4, shuffle=True, random_state=1)\n",
    "outer_cv = KFold(n_splits=4, shuffle=True, random_state=1)\n",
    "\n",
    "#inner loop\n",
    "inner_model = GridSearchCV(model, param_grid=param_grid, cv=inner_cv)\n",
    "\n",
    "#outer loop\n",
    "nested_score = cross_val_score(inner_model, X, y, scoring='neg_mean_squared_error', cv=outer_cv)\n",
    "                              \n",
    "nested_score #higher mean better....(sklearn wants to keep this convention)\n",
    "\n",
    "#see this ==> https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you just average them, and this is your very robust estimates of the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3102.1473014039575"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nested_score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two big problems of nested cross-validation:\n",
    "    \n",
    "1. It is time consuming and resource hungry.\n",
    "2. You no longer know what hyperparameters or best models....because in the inner loop, the model varies....**so yes, nested cross-validation do not give you a final model!! XD**\n",
    "\n",
    "**So how to use nested cross-validation**\n",
    "\n",
    "1. First, use nested cross-validation to look for **model instability**.  If there is a lot of instability, you want to **skip the model or change the search space**.  \n",
    "\n",
    "2. Once you got a model that is very stable, run a typical (simple, no nested) cross-validation to find the best version of that model, so you can deploy.\n",
    "    - you can either train (1) on the entire dataset (if you are super sure), or (2) training set (preferable).\n",
    "\n",
    "**It takes too much time!!!**\n",
    "\n",
    "1. Make the outer loop smaller OR\n",
    "2. Don't use it"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
