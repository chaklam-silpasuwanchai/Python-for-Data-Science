{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Study\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "\n",
    "The only difference is how we calculate the loss and gradient which is based on only **one sample**.\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\theta_j} = (h^{(i)}-y^{(i)})x_j$$\n",
    "\n",
    "### Mini-Batch Gradient Descent\n",
    "\n",
    "The only difference is how we calculate the loss and gradient which is based on only **subset of samples**.\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\theta_j} = \\sum_{i=start}^{batch}(h^{(i)}-y^{(i)})x_j$$\n",
    "\n",
    "where start is a randomized number within the range of $m$ and batch is a predefined batch size, typically around 100 to 500\n",
    "\n",
    "### Closed Form\n",
    "\n",
    "Gradient descent gives one possible mean for minimizing $J$, which uses iterative approach and may take time.  In the situation where we know that our cost function is strictly concave or convex, we can explicitly take its derivative to zero.  This process of such derivation is called obtaining the **normal equations** or **closed form**. \n",
    "\n",
    "The **closed form** of linear regression can be derived easily.  Let $\\mathbf{X}$ be a matrix of shape $(m, n)$, $\\boldsymbol{\\theta}$ as shape $(n, )$, and $\\mathbf{y}$ as vector of shape $(m, )$.  Instead of writing the cost function as power of square, we shall write it in matrix multiplication as follows:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\boldsymbol{\\theta}} (\\mathbf{X}\n",
    " - \\mathbf{y})^T*(\\mathbf{X}\n",
    "-\\mathbf{y})$$\n",
    "\n",
    "Recall the following properties:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{X}} \\mathbf{X}^T\\mathbf{X}=2\\mathbf{X} \\tag{A}$$\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{X}} \\mathbf{A}\\mathbf{X}=\\mathbf{A}^T$$\n",
    "$$(\\mathbf{X}\\mathbf{y})^T = \\mathbf{y}^T\\mathbf{X}^T$$\n",
    "\n",
    "Therefore\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial \\boldsymbol{\\theta}} (\\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{y})^T*(\\mathbf{X}\\boldsymbol{\\theta}-\\mathbf{y}) &= \\frac{\\partial J}{\\partial \\boldsymbol{\\theta}} (\\boldsymbol{\\theta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\theta} - \\boldsymbol{\\theta}^T\\mathbf{X}^T\\mathbf{y} - \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\theta} + \\mathbf{y}^T\\mathbf{y})\\\\\n",
    "&= 2\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\theta} - 2\\mathbf{X}^T\\mathbf{y} \\tag{see note*}\\\\\n",
    "&= \\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{X}^T\\mathbf{y} = 0 \\tag{set derivative to 0}\\\\\n",
    "&= \\boldsymbol{\\theta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Note*: Since $\\mathbf{X}\\boldsymbol{\\theta}$ is a vector, and so is $\\mathbf{y}$, it doesn't matter what the order is, thus we can simply add them to 2.  Also, we got 2 in front of the first part because we have two $\\theta$ (used the property A)\n",
    "\n",
    "\n",
    "**Why not closed form always**.  The answer is simple.  It does not always exists or possible, for example, the cost function is not convex or concave.  But of course, if it exists, we usually prefer closed form given that it is usually faster than gradient descent.  Nevertheless, as you can see, taking inverse of huge number of features can be expensive, thus it is also not always straightforward thing to always prefer closed form.\n",
    "\n",
    "Yes, that's it for most of the theoretical stuff.  Let's start implementing some of these concepts so we can better understand them.\n",
    "\n",
    "The closed form is a normal equations derived from setting the derivatives = 0.  By performing only some inverse operations and matrix multiplication, we will be able to get the theta.\n",
    "\n",
    "$$\\boldsymbol{\\theta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n",
    "\n",
    "When closed form is available, is doable (can be inversed - can use pseudoinverse), and with not many features (i.e., inverse can be slow), it is recommended to always use closed form.  \n",
    "\n",
    "**Implementation steps:**\n",
    "\n",
    "1. Prepare your data\n",
    "    - add intercept\n",
    "    - $\\mathbf{X}$ and $\\mathbf{y}$ and $\\mathbf{w}$ in the right shape\n",
    "        - $\\mathbf{X}$ -> $(m, n)$\n",
    "        - $\\mathbf{y}$ -> $(m, )$\n",
    "        - $\\mathbf{w}$ -> $(n, )$\n",
    "        - where $m$ is number of samples\n",
    "        - where $n$ is number of features\n",
    "    - train-test split\n",
    "    - feature scale\n",
    "    - clean out any missing data\n",
    "    - (optional) feature engineering\n",
    "2. Plug everything into the equation.  Here we shall use X_train to retrieve the $\\boldsymbol{\\theta}$\n",
    "$$\\boldsymbol{\\theta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n",
    "\n",
    "3. We simply using the $\\boldsymbol{\\theta}$, we can perform a dot product with our X_test which will give us $\\mathbf{\\hat{y}}$.\n",
    "\n",
    "4. We then calculate the errors using mean-squared-error function:\n",
    "\n",
    "$$\\frac{1}{m}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "Note that it's a bit different from our $J(\\boldsymbol{\\theta})$ because $J(\\boldsymbol{\\theta})$ puts $\\frac{1}{2}$ instead of $\\frac{1}{m}$ for mathematical convenience for derivatives, since we know changing constants do not change the optimization results.\n",
    "\n",
    "\n",
    "Let's implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare data\n",
    "\n",
    "#### 1.1 Get your X and y in the right shape for Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = diabetes.data\n",
    "X.shape #number of samples, number of features\n",
    "\n",
    "m = X.shape[0]  #number of samples\n",
    "n = X.shape[1]  #number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = diabetes.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Feature scale your data to reach faster convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Train test split your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "assert len(X_train)  == len(y_train)\n",
    "assert len(X_test) == len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Add intercepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "\n",
    "# concatenate the intercept based on axis=1\n",
    "X_train = np.concatenate((intercept, X_train), axis=1)\n",
    "\n",
    "# np.ones((shape))\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "\n",
    "# concatenate the intercept based on axis=1\n",
    "X_test = np.concatenate((intercept, X_test), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Fit your algorithm \n",
    "\n",
    "#### 1. Define your algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "# order of operation DOES NOT MATTER\n",
    "# But don't flip y before X^T for example\n",
    "def closed_form(X, y):\n",
    "    return inv(X.T @ X) @ X.T @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.50394523e+02,  1.24951263e-01, -9.56655783e+00,  2.74079297e+01,\n",
       "        1.38021711e+01, -2.82122671e+01,  1.24108051e+01,  1.50711994e+00,\n",
       "        5.52299386e+00,  3.33037406e+01,  2.89952607e+00])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's use the closed_form to find the theta\n",
    "theta = closed_form(X_train, y_train)\n",
    "theta  #<------this is our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Compute accuracy/loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the accuracy/loss\n",
    "\n",
    "yhat = X_test @ theta #==> X (m, n+1)  @ (n+1, ) w ==> (m, ) y\n",
    "\n",
    "# if I want to compare yhat and y, I need to make sure they are the same shape\n",
    "assert y_test.shape == yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared errors:  2738.022740831692\n"
     ]
    }
   ],
   "source": [
    "# get the mse\n",
    "mse = ((y_test - yhat)**2).sum() / X_test.shape[0]\n",
    "print(\"Mean squared errors: \", mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
