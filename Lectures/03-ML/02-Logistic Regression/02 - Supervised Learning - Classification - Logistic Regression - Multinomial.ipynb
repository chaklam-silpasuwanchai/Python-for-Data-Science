{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science and Artificial Intelligence\n",
    "\n",
    "## Supervised Learning - Classification - Logistic Regression - Multinomial\n",
    "\n",
    "### Readings: \n",
    "- [GERON] Ch4\n",
    "- [VANDER] Ch5\n",
    "- [HASTIE] Ch4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Logistic Regression\n",
    "\n",
    "This is logistic regression when number of classes are more than 2.\n",
    "\n",
    "### Scratch\n",
    "\n",
    "**Implementation steps:**\n",
    "    \n",
    "The gradient descent has the following steps:\n",
    "\n",
    "1. Prepare your data\n",
    "    - add intercept\n",
    "    - $\\mathbf{X}$ and $\\mathbf{Y}$ and $\\mathbf{W}$ in the right shape\n",
    "        - $\\mathbf{X}$ -> $(m, n)$\n",
    "        - $\\mathbf{Y}$ -> $(m, k)$\n",
    "        - $\\mathbf{W}$ -> $(n, k)$\n",
    "        - where $k$ is number of classes\n",
    "    - train-test split\n",
    "    - feature scale\n",
    "    - clean out any missing data\n",
    "    - (optional) feature engineering\n",
    "2. Predict using the softmax function\n",
    "   $$ h = \\mathsf{P}(y = a \\mid \\theta) = \\frac{e^{\\theta^{T}_ax}}{\\Sigma_{i=1}^{k} e^{\\theta_k^{T}x}}$$\n",
    "   --->why this function?<----\n",
    "   - First, mathematically, this is just an extension of the sigmoid formula for multi-class classification\n",
    "   - $e$ will always give non-negative outputs which helps, since probability is never negative\n",
    "   - $e$ has a similar effect as argmax, which will turn larger input to larger outputs.\n",
    "   - $e$ is super easy to differentiate, because derivative of $e$ is $e$\n",
    "   - $e$ nicely cancel out the $\\log$ in the cross entropy loss (see below)\n",
    "   - By dividing, it make sure all the probability adds up to one.  You can think the softmax function as some form of normalization.   Why not normalization?  Because normalization cares only about proportion, while softmax reacts to change in scale better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  (2, 3)\n",
      "Y:  (2, 4)\n",
      "W:  (3, 4)\n",
      "X @ W: [[ 8 14 20  7]\n",
      " [15 26 35 13]]\n",
      "softmax(X @ W): [[0.00000613 0.0024726  0.99751901 0.00000225]\n",
      " [0.         0.00012339 0.9998766  0.        ]]\n",
      "Try to confirm it adds up to 1: [1. 1.]\n",
      "if I want to know which one is the answer, use argmax:  [2 2]\n",
      "normalization(X @ W): [[0.30044631 0.52578104 0.75111577 0.26289052]\n",
      " [0.31311215 0.54272772 0.73059501 0.27136386]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "\n",
    "X = np.array([[1, 2, 3],\n",
    "             [2, 4, 5]])\n",
    "\n",
    "print(\"X: \", X.shape)  #(m, n) two samples, three features.  We ignore the y-intercept\n",
    "\n",
    "Y = np.array([[0, 0, 1, 0],\n",
    "              [1, 0, 0, 0]])  #(m, k) let's say four classes\n",
    "\n",
    "print(\"Y: \", Y.shape)\n",
    "\n",
    "W = np.array([[1, 2, 3, 4],\n",
    "              [2, 3, 1, 0],\n",
    "              [1, 2, 5, 1],\n",
    "              ])  #(n, k)  three features, four classes\n",
    "\n",
    "print(\"W: \", W.shape)\n",
    "\n",
    "print(\"X @ W:\",  X @ W)  #X @ W should be the same shape as our y\n",
    "\n",
    "print(\"softmax(X @ W):\", softmax(X@W))\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(\"Try to confirm it adds up to 1:\", softmax(X@W).sum(axis=1))\n",
    "\n",
    "print(\"if I want to know which one is the answer, use argmax: \", np.argmax(softmax(X@W), axis=1))\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "print(\"normalization(X @ W):\", normalize(X@W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Calculate the loss using the cross entropy loss\n",
    "    $$J = -\\sum_{i=1}^m y^{(i)}\\log(h^{(i)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y:  [[0 0 1 0]\n",
      " [1 0 0 0]]\n",
      "h:  [[0.00000613 0.0024726  0.99751901 0.00000225]\n",
      " [0.         0.00012339 0.9998766  0.        ]]\n",
      "log:  [[ -0.          -0.          -0.00248407  -0.        ]\n",
      " [-20.0001234   -0.          -0.          -0.        ]]\n",
      "log loss:  [[ 0.          0.          0.00248407  0.        ]\n",
      " [20.0001234   0.          0.          0.        ]]\n",
      "sum of log loss:  20.00260747339262\n"
     ]
    }
   ],
   "source": [
    "print(\"Y: \", Y)\n",
    "print(\"h: \", softmax(X@W))\n",
    "print(\"log: \", Y * np.log(softmax(X@W)))\n",
    "print(\"log loss: \", -(Y * np.log(softmax(X@W))))\n",
    "print(\"sum of log loss: \", np.sum(-(Y * np.log(softmax(X@W)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Calculate the gradient of theta of feature $j$ based on the loss function $J$\n",
    "    - The gradient is defined as\n",
    "       $$\\frac{\\partial J}{\\partial \\theta_j} = \\sum_{i=1}^{m}(h^{(i)}-y^{(i)})x_j$$\n",
    "    - This gradient can be derived from the following simple example:\n",
    "        - Suppose given 2 classes (k = 2) and 3 features (n = 3), we have the loss function as\n",
    "       $$ J = -y_1 \\log h_1 - y_2 \\log h_2 $$\n",
    "       where $h_1$ and $h_2$ are\n",
    "       $$ h_1 = \\frac{\\exp(g_1)}{\\exp(g_1)+\\exp(g_2)} $$\n",
    "       $$ h_2 = \\frac{\\exp(g_2)}{\\exp(g_1)+\\exp(g_2)} $$\n",
    "       where $g_1$ and $g_2$ are\n",
    "       $$ g_1 = w_{11}x_1 + w_{21}x_2 + w_{31}x_3 $$\n",
    "       $$ g_2 = w_{12}x_1 + w_{22}x_2 + w_{32}x_3  $$\n",
    "       where in $w_{ij}$, $i$ stands for feature and $j$ stands for class \n",
    "    - For example, to find the gradient of $J$ in respect to $w_{21}$, we simply can use the chain rule (or backpropagation) to calculate like this:\n",
    "       $$ \\frac{\\partial J}{\\partial w_{21}} = \\frac{\\partial J}{\\partial h_{1}}\\frac{\\partial h_{1}}{\\partial g_{1}}\\frac{\\partial g_{1}}{\\partial w_{21}} + \\frac{\\partial J}{\\partial h_{2}}\\frac{\\partial h_{2}}{\\partial g_{1}}\\frac{\\partial g_{1}}{\\partial w_{21}}$$\n",
    "   - If we know each of them, it is easy, where\n",
    "       $$\\frac{\\partial J}{\\partial h_{1}} = -\\frac{y_1}{h_1}$$\n",
    "       $$\\frac{\\partial J}{\\partial h_{2}} = -\\frac{y_2}{h_2}$$\n",
    "       $$\\frac{\\partial h_{1}}{\\partial g_{1}} = \\frac{\\exp(g_{1})}{\\exp(g_{1}) + \\exp(g_{2})} - (\\frac{\\exp(g_1)}{\\exp(g_1)+\\exp(g_2)})^2 = h_1 (1 - h_1)$$\n",
    "       $$\\frac{\\partial h_{2}}{\\partial g_{1}} = \\frac{-exp(g_2)exp(g_1)}{(\\exp(g_1) + \\exp(g_2)^2} = -h_2h_1$$\n",
    "       $$\\frac{\\partial g_1}{\\partial w_{21}} = x_2$$\n",
    "    - For those who forgets how to do third and fourth, recall that the quotient rule\n",
    "        $$ (\\frac{f}{g})' = \\frac{f'g - fg'}{g^2}$$\n",
    "    - Putting everything together, we got\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial J}{\\partial w_{21}} &= \\frac{\\partial J}{\\partial h_{1}}\\frac{\\partial h_{1}}{\\partial g_{1}}\\frac{\\partial g_{1}}{\\partial w_{21}} + \\frac{\\partial J}{\\partial h_{2}}\\frac{\\partial h_{2}}{\\partial g_{1}}\\frac{\\partial g_{1}}{\\partial w_{21}}\\\\\n",
    "&= -\\frac{y_1}{h_1} * h_1 (1 - h_1) * x_2 + -\\frac{y_2}{h_2} * -h_2h_1 * x_2 \\\\\n",
    "&= x_2 (-y_1 + y_1h_1 + y_2h_1)\\\\\n",
    "&= x_2 (-y_1 + h_1(y_1 + y_2))\\\\\n",
    "&= x_2 (h_1 - y_1)\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "4. Update the theta with this update rule\n",
    "    $$\\theta_j := \\theta_j - \\alpha * \\frac{\\partial J}{\\partial \\theta_j}$$\n",
    "    where $\\alpha$ is a typical learning rate range between 0 and 1\n",
    "5. Loop 2-4 until max_iter is reached, or the difference between old loss and new loss are smaller than some predefined threshold tol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Step 1: Prepare data\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, 2:]  # we only take the first two features.\n",
    "y = iris.target  #now our y is three classes thus require multinomial\n",
    "\n",
    "# feature scaling helps improve reach convergence faster\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# add intercept to our X\n",
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "X_train   = np.concatenate((intercept, X_train), axis=1)  #add intercept\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "X_test    = np.concatenate((intercept, X_test), axis=1)  #add intercept\n",
    "\n",
    "# make sure our y is in the shape of (m, k)\n",
    "# we will convert our output vector in \n",
    "# matrix where no. of columns is equal to the no. of classes. \n",
    "# The values in the matrix will be 0 or 1. For instance the rows \n",
    "# where we have output 2 the column 2 will contain 1 and the rest are all 0.\n",
    "# in simple words, y will be of shape (m, k)\n",
    "k = len(set(y))  # no. of class  (can also use np.unique)\n",
    "m = X_train.shape[0]  # no.of samples\n",
    "n = X_train.shape[1]  # no. of features\n",
    "Y_train_encoded = np.zeros((m, k))\n",
    "for each_class in range(k):\n",
    "    cond = y_train==each_class\n",
    "    Y_train_encoded[np.where(cond), each_class] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1.1 (optional): Visualize our data\n",
    "\n",
    "# your code here\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(X[:, 0], X[:, 1], label='class 0', c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at iteration 0 1.2165698831321565\n",
      "Cost at iteration 500 0.08503928446436061\n",
      "Cost at iteration 1000 0.07540368804940104\n",
      "Cost at iteration 1500 0.07208374635871027\n",
      "Cost at iteration 2000 0.07051629843305525\n",
      "Cost at iteration 2500 0.06965815235758366\n",
      "Cost at iteration 3000 0.06914405970854572\n",
      "Cost at iteration 3500 0.06881605728529293\n",
      "Cost at iteration 4000 0.0685964053247815\n",
      "Cost at iteration 4500 0.06844333824381736\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Fit your data\n",
    "\n",
    "def logistic_regression_GD(X, Y, k, n, max_iter=1000):\n",
    "    '''\n",
    "    Inputs: \n",
    "        X shape: (m, n)\n",
    "        w shape: (n, k)\n",
    "    '''\n",
    "    W = np.random.rand(n, k)\n",
    "    l_rate = 0.01\n",
    "    for i in range(max_iter):\n",
    "        cost, grad =  gradient(X, Y, W)\n",
    "        if i % 500 == 0:\n",
    "            print(f\"Cost at iteration {i}\", cost)\n",
    "        W = W - l_rate * grad\n",
    "    return W, i\n",
    "\n",
    "# for those who tend to feel overwhelmed with lots of code\n",
    "# I recommend you to write each part of the code separately as function\n",
    "# it helps!\n",
    "def gradient(X, Y, W):\n",
    "    m = X.shape[0]\n",
    "    h = h_theta(X, W)\n",
    "    cost = - np.sum(Y * np.log(h)) / m\n",
    "    error = h - Y\n",
    "    grad = softmax_grad(X, error)\n",
    "    return cost, grad\n",
    "\n",
    "def softmax(theta_t_x):\n",
    "    return np.exp(theta_t_x) / np.sum(np.exp(theta_t_x), axis=1, keepdims=True)\n",
    "\n",
    "def softmax_grad(X, error):\n",
    "    return  X.T @ error\n",
    "        \n",
    "def h_theta(X, W):\n",
    "    '''\n",
    "    Input:\n",
    "        X shape: (m, n)\n",
    "        w shape: (n, k)\n",
    "    Returns:\n",
    "        yhat shape: (m, k)\n",
    "    '''\n",
    "    return softmax(X @ W)\n",
    "\n",
    "W, i = logistic_regression_GD(X_train, Y_train_encoded, k, X_train.shape[1], max_iter=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========Classification report=======\n",
      "Report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        18\n",
      "           1       0.94      0.94      0.94        16\n",
      "           2       0.91      0.91      0.91        11\n",
      "\n",
      "    accuracy                           0.96        45\n",
      "   macro avg       0.95      0.95      0.95        45\n",
      "weighted avg       0.96      0.96      0.96        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "yhat = np.argmax(h_theta(X_test, W), axis=1)\n",
    "print(\"=========Classification report=======\")\n",
    "print(\"Report: \", classification_report(y_test, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average precision score.  Recall that in a multiclass format, we have to calculate for each class, and then take the average.  And before we do that, we need to binarize our y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========Average precision score=======\n",
      "Class 0 score:  1.0\n",
      "Class 1 score:  0.9011284722222223\n",
      "Class 2 score:  0.8486685032139577\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "print(\"=========Average precision score=======\")\n",
    "y_test_binarized = label_binarize(y_test, classes=[0, 1, 2])\n",
    "yhat_binarized   = label_binarize(yhat, classes=[0, 1, 2])\n",
    "\n",
    "n_classes = len(np.unique(y_test))\n",
    "\n",
    "for i in range(n_classes):\n",
    "    class_score = average_precision_score(y_test_binarized[:, i], yhat_binarized[:, i])\n",
    "    print(f\"Class {i} score: \", class_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Step 1: Prepare data\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, 2:]  # we only take the first two features.\n",
    "y = iris.target  #now our y is three classes thus require multinomial\n",
    "\n",
    "#feature scaling helps improve reach convergence faster\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "#data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "#add intercept to our X\n",
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "X_train   = np.concatenate((intercept, X_train), axis=1)  #add intercept\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "X_test    = np.concatenate((intercept, X_test), axis=1)  #add intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========Average precision score=======\n",
      "Class 0 score:  1.0\n",
      "Class 1 score:  0.8905982905982905\n",
      "Class 2 score:  0.8947368421052632\n",
      "=========Classification report=======\n",
      "Report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        15\n",
      "           1       1.00      0.85      0.92        13\n",
      "           2       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.96        45\n",
      "   macro avg       0.96      0.95      0.95        45\n",
      "weighted avg       0.96      0.96      0.95        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "model = LogisticRegression(multi_class=\"ovr\")  #set this to multiclass=\"ovr\" to perform multinomial logistic\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "print(\"=========Average precision score=======\")\n",
    "y_test_binarized = label_binarize(y_test, classes=[0, 1, 2])\n",
    "yhat_binarized   = label_binarize(yhat, classes=[0, 1, 2])\n",
    "\n",
    "n_classes = len(np.unique(y_test))\n",
    "\n",
    "for i in range(n_classes):\n",
    "    class_score = average_precision_score(y_test_binarized[:, i], yhat_binarized[:, i])\n",
    "    print(f\"Class {i} score: \", class_score)\n",
    "\n",
    "print(\"=========Classification report=======\")\n",
    "print(\"Report: \", classification_report(y_test, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === Task ===\n",
    "\n",
    "1. With the iris data given in class, implement train_test_split from scratch.\n",
    "\n",
    "2. Put everything into a class called LogisticRegression, this class should allow you choose any of the training methods you'd like including \"batch\", \"minibatch\" and \"sto\". However, if the input method is not one of the three, it should \"raise ValueError\".\n",
    "\n",
    "3. Calculate time taken to fit your models using different training methods.\n",
    "\n",
    "4. Perform a classification on the dataset using all 3 methods and also show what happens if your defined training method is not either \"batch\", \"minibatch\" or \"sto\". Make sure to plot the training losses.\n",
    "\n",
    "5. Simply, use classification_report from sklearn.metrics to evaluate your models.\n",
    "\n",
    "6. Discuss your results ie. training losses of the three methods and time taken to fit models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
