{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science and Artificial Intelligence\n",
    "\n",
    "## Deep Learning -  NLP - Level 0\n",
    "\n",
    "- https://pytorch.org/tutorials/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network (RNN)\n",
    "\n",
    "<img src = \"../figures/rnn_weight.png\" width=\"300\">\n",
    "\n",
    "$$h_t = \\text{tanh}(\\mathbf{W}_{xh}x_t + b_i + \\mathbf{W}_{hh}h_{t-1} + b_h)$$\n",
    "\n",
    "### Type of RNN\n",
    "\n",
    "<img src = \"../figures/karpathy.jpg\" width=\"500\">\n",
    "\n",
    "Examples:\n",
    "- **One to one**: Image Classification\n",
    "- **One to many**: Image Captioning\n",
    "- **Many to one**:  Sentiment Analysis\n",
    "- **Many to many**:  Machine translation\n",
    "- **Exactly matched many to many**:  Video labeling frame by frame\n",
    "\n",
    "### Case study: Predicting the next words\n",
    "\n",
    "Given some initial word (e.g., good), let's create some model that can predict the next characters til the specified length (e.g., good I am fine).  To link with RNN, you can imagine each $x$ as each of the character, i.e., 'g', 'o', 'o', 'd' depicted in integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Defining text input\n",
    "\n",
    "First, we'll define the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['hey whats up','good day','they are cool']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since computers don't understand characters, let's make some mapping between some integers and characters, which will be useful for making one hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = set(''.join(text))  #chars are simply python list of all characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'y', 1: 'w', 2: ' ', 3: 'a', 4: 'e', 5: 'g', 6: 'd', 7: 'r', 8: 'c', 9: 'l', 10: 'u', 11: 'h', 12: 'p', 13: 'o', 14: 't', 15: 's'}\n"
     ]
    }
   ],
   "source": [
    "int2char = dict(enumerate(chars))\n",
    "print(int2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'y': 0, 'w': 1, ' ': 2, 'a': 3, 'e': 4, 'g': 5, 'd': 6, 'r': 7, 'c': 8, 'l': 9, 'u': 10, 'h': 11, 'p': 12, 'o': 13, 't': 14, 's': 15}\n"
     ]
    }
   ],
   "source": [
    "char2int = {char: ind for ind, char in int2char.items()}\n",
    "print(char2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Padding\n",
    "\n",
    "We'll be padding our input sentences to ensure that all the sentences are of the sample length. While RNNs are typically able to take in variably sized inputs, we will usually want to feed training data in batches to speed up the training process. In order to used batches to train on our data, we'll need to ensure that each sequence within the input data are of equal size.\n",
    "\n",
    "Therefore, in most cases, padding can be done by filling up sequences that are too short with **0** values and trimming sequences that are too long. In our case, we'll be finding the length of the longest sequence and padding the rest of the sentences with blank spaces to match that length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'they are cool'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(text, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest string has 13 characters\n"
     ]
    }
   ],
   "source": [
    "maxlen = len(max(text, key=len)) \n",
    "print(\"The longest string has {} characters\".format(maxlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey whats up ', 'good day     ', 'they are cool']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Padding\n",
    "for i in range(len(text)):  #loop each of the sentence\n",
    "    while len(text[i]) < maxlen:  #if that sentence length is shorter than max len, keep adding white space\n",
    "        text[i] += ' '\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Defining target sequences\n",
    "\n",
    "As we're going to predict the next character in the sequence at each time step, we'll have to divide each sentence into\n",
    "\n",
    "- Input data\n",
    "    - The last input character should be excluded as it does not need to be fed into the model\n",
    "- Target/Ground Truth Label\n",
    "    - One time-step ahead of the Input data as this will be the \"correct answer\" for the model at each time step corresponding to the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence: hey whats up\n",
      "Target Sequence: ey whats up \n",
      "Input Sequence: good day    \n",
      "Target Sequence: ood day     \n",
      "Input Sequence: they are coo\n",
      "Target Sequence: hey are cool\n"
     ]
    }
   ],
   "source": [
    "input_seq = []\n",
    "target_seq = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    # Remove last character for input sequence\n",
    "    input_seq.append(text[i][:-1])\n",
    "    \n",
    "    # Remove firsts character for target sequence\n",
    "    target_seq.append(text[i][1:])\n",
    "    print(\"Input Sequence: {}\\nTarget Sequence: {}\".format(input_seq[i], target_seq[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can convert our input and target sequences to sequences of integers instead of characters by mapping them using the dictionaries we created above. This will allow us to one-hot-encode our input sequence subsequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[11, 4, 0, 2, 1, 11, 3, 14, 15, 2, 10, 12],\n",
       " [5, 13, 13, 6, 2, 6, 3, 0, 2, 2, 2, 2],\n",
       " [14, 11, 4, 0, 2, 3, 7, 4, 2, 8, 13, 13]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(text)):\n",
    "    input_seq[i] = [char2int[character] for character in input_seq[i]]\n",
    "    target_seq[i] = [char2int[character] for character in target_seq[i]]\n",
    "    \n",
    "input_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. One-hot embedding\n",
    "\n",
    "We are now ready to make our input_sequences into the form of <code>(batch_size, seq_len, vocab_size)</code> via using one-hot embedding.  This is the common shape of any text input.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = len(text)  #batch is the number of sentences\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = maxlen - 1  #we minus 1 because we remove the last character\n",
    "seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(char2int)  #number of vocab size; this is also called the dimensions/features\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(sequence, vocab_size, seq_len, batch_size):    \n",
    "    input_seq_encoded = np.zeros((batch_size, seq_len, vocab_size), dtype=np.float32)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            input_seq_encoded[i, u, sequence[i][u]] = 1\n",
    "    return input_seq_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also defined a helper function that creates arrays of zeros for each character and replaces the corresponding character index with a **1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (3, 12, 16) --> (Batch Size, Sequence Length, One-Hot Encoding Size)\n"
     ]
    }
   ],
   "source": [
    "input_seq_encoded = one_hot_encode(input_seq, vocab_size, seq_len, batch_size)\n",
    "print(\"Input shape: {} --> (Batch Size, Sequence Length, One-Hot Encoding Size)\".format(input_seq_encoded.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(input_seq_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're done with all the data pre-processing, we can now move the data from numpy arrays to PyTorch's very own data structure - **Torch Tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_tensor = torch.from_numpy(input_seq_encoded)  #from numpy\n",
    "target_seq_tensor = torch.Tensor(target_seq) #from list, automatically float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(target_seq_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Implementing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining hyperparameters for model\n",
    "input_dim = vocab_size\n",
    "output_dim = vocab_size\n",
    "hidden_dim = 10 #this is similar to what hidden dim in fc layer; i just arbitrarily think about it\n",
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True)   \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(num_layers, batch_size, hidden_dim).to(device)\n",
    "\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        \n",
    "        out = out.reshape(-1, hidden_dim)\n",
    "        out = self.fc(out) \n",
    "        \n",
    "        return out, hn  #output shape: batch, seq_len, hidden_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN output:  torch.Size([3, 12, 10])\n",
      "Hn output:  torch.Size([1, 3, 10])\n",
      "Reshape output:  torch.Size([36, 10])\n",
      "After linear output:  torch.Size([36, 16])\n",
      "Target seq shape:  torch.Size([3, 12])\n",
      "Target seq new shape:  torch.Size([36])\n"
     ]
    }
   ],
   "source": [
    "#let understand basic RNN\n",
    "\n",
    "# defining input\n",
    "input_test = torch.rand_like(input_seq_tensor)\n",
    "h0_test = torch.zeros(num_layers, batch_size, hidden_dim)\n",
    "\n",
    "# defining rnn\n",
    "rnn_test = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True)   \n",
    "\n",
    "# 1. run rnn\n",
    "out, hn = rnn_test(input_test)\n",
    "\n",
    "print(\"RNN output: \", out.shape)  #batch, seq_len, hidden_dim\n",
    "\n",
    "print(\"Hn output: \", hn.shape)  #num_layer, batch, hidden_dim\n",
    "\n",
    "# 2. reshape\n",
    "out = out.reshape(-1, hidden_dim)\n",
    "\n",
    "print(\"Reshape output: \", out.shape)  #batch*seq_len, hidden_dim\n",
    "\n",
    "# 3. linear\n",
    "linear_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "out = linear_layer(out)\n",
    "\n",
    "print(\"After linear output: \", out.shape)  #batch*seq_len, output_dim\n",
    "\n",
    "# 4. loss\n",
    "\n",
    "print(\"Target seq shape: \", target_seq_tensor.shape) #need to match the dimension of out, thus combine\n",
    "\n",
    "print(\"Target seq new shape: \", target_seq_tensor.view(-1).shape)  #view is similar to reshape; share memory\n",
    "\n",
    "#now you can use CrossEntropyLoss comparing out and target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = RNN().to(device)\n",
    "\n",
    "# Define hyperparameters for learning\n",
    "num_epochs = 100\n",
    "lr = 0.01\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100/100.............Loss: 0.1797"
     ]
    }
   ],
   "source": [
    "input_seq_tensor = input_seq_tensor.to(device)\n",
    "target_seq_tensor = target_seq_tensor.to(device)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    \n",
    "    #1. predict\n",
    "    output, hidden = model(input_seq_tensor)\n",
    "\n",
    "    #2. calculate loss\n",
    "    loss = criterion(output, target_seq_tensor.view(-1).long())  #.view(-1) simply squeeze everything into 1 dimension; \n",
    "\n",
    "    #3. backprop\n",
    "    optimizer.zero_grad() \n",
    "    loss.backward() \n",
    "    optimizer.step() \n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        sys.stdout.write('\\rEpoch: {}/{}.............Loss: {:.4f}'.format(epoch, num_epochs,loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s test our model now and see what kind of output we will get. Before that, let’s define some helper function to convert our model output back to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _predict(model, character):\n",
    "    # One-hot encoding our input to fit into the model\n",
    "    character = np.array([[char2int[c] for c in character]])\n",
    "    character = one_hot_encode(character, vocab_size, character.shape[1], 1)\n",
    "    character = torch.from_numpy(character)\n",
    "    character = character.to(device)\n",
    "    \n",
    "    out, hidden = model(character)\n",
    "\n",
    "    prob = nn.functional.softmax(out[-1], dim=0).data  #out[-1] refers to the last character\n",
    "    \n",
    "    char_ind = torch.max(prob, dim=0)[1].item()\n",
    "\n",
    "    return int2char[char_ind], hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, out_len, input_text):\n",
    "    model.eval() # eval mode\n",
    "    input_text = input_text.lower()\n",
    "    # First off, run through the starting characters\n",
    "    chars = [ch for ch in input_text]\n",
    "    size = out_len - len(chars)\n",
    "    # Now pass in the previous characters and get a new one\n",
    "    for ii in range(size):\n",
    "        char, _ = _predict(model, chars)  #does not need h so underscored\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey whats up wh'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model, 15, 'hey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice\n",
    "\n",
    "- Add 'hey i am ok' in the text and see what happens in the prediction (the results theoretically can either output 'hey whats up' or 'hey i am ok')\n",
    "- Add 2 'hey i am ok' in the text and see what happens in the prediction (the results theoretically should output 'hey i am ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
