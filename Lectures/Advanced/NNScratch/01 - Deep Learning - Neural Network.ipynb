{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science and Artificial Intelligence\n",
    "\n",
    "## 9.1 Deep Learning - Neural Network\n",
    "\n",
    "### Readings\n",
    "\n",
    "- [WEIDMAN] Ch1,2\n",
    "- [CHARU] Ch1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "%matplotlib inline\n",
    "\n",
    "from typing import Callable\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foundations\n",
    "\n",
    "To understand the essence of Neural Network, you are required to understand these components:\n",
    "\n",
    "1. Functions\n",
    "2. Derivatives\n",
    "3. Nested Functions\n",
    "4. The Chain Rule\n",
    "    - Simple example\n",
    "    - Longer example\n",
    "    - Multiple inputs\n",
    "    - Multiple vector inputs\n",
    "    - 2D matrix inputs\n",
    "    \n",
    "To make sure we understand, we shall use 3 perspectives - (1) Math, (2) Diagrams, and (3) Code\n",
    "   \n",
    "### Functions\n",
    "\n",
    "Here are two examples of functions, described in mathematical notation, where this notation is typically *non-standard*:\n",
    "\n",
    "#### Mathematical representations\n",
    "\n",
    "A function $f_1$ that takes one **input** $x$ and **output** $x^2$\n",
    "\n",
    "$$f_1(x) = x^2$$\n",
    "\n",
    "Here is another function $f_2$ that takes one **input** $x$ and **output** $max(x, 0)$\n",
    "\n",
    "$$f_2(x) = max(x, 0)$$\n",
    "\n",
    "#### Diagrams\n",
    "\n",
    "Diagrams are another useful way to depict.  Not so useful for calculus, but useful when thniking about deep learning mdoels.\n",
    "\n",
    "<img src = \"../figures/functions.png\" width=400>\n",
    "\n",
    "#### Code\n",
    "\n",
    "Of course, as our name of our class suggests, last way to think about functions are using the code format.  Throughout our exploration of Neural Network, we shall use Numpy as our tool to understand Neural Network, as well as enhance our skills in Numpy and Python in general.  The code of $f_1$ and $f_2$ are given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x: ndarray) -> ndarray:\n",
    "    '''\n",
    "    Square each element in the input ndarray.\n",
    "    '''\n",
    "    return np.power(x, 2)\n",
    "\n",
    "def leaky_relu(x: ndarray) -> ndarray:\n",
    "    '''\n",
    "    Apply \"Leaky ReLU\" function to each element in ndarray\n",
    "    '''\n",
    "    return np.maximum(0.2 * x, x)\n",
    "\n",
    "def sigmoid(x: ndarray) -> ndarray:\n",
    "    '''\n",
    "    Apply the sigmoid function to each element in the input ndarray.\n",
    "    '''\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'output')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, sharey=True, figsize=(12, 6))  # 2 Rows, 1 Col\n",
    "\n",
    "input_range = np.arange(-2, 2, 0.01)\n",
    "ax[0].plot(input_range, square(input_range))\n",
    "ax[0].plot(input_range, square(input_range))\n",
    "ax[0].set_title('Square function')\n",
    "ax[0].set_xlabel('input')\n",
    "ax[0].set_ylabel('input')\n",
    "\n",
    "ax[1].plot(input_range, leaky_relu(input_range))\n",
    "ax[1].plot(input_range, leaky_relu(input_range))\n",
    "ax[1].set_title('\"ReLU\" function')\n",
    "ax[1].set_xlabel('input')\n",
    "ax[1].set_ylabel('output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivatives\n",
    "\n",
    "Derivative of a **function** at a **point** is the **rate of change** of the **output** of the function with respect to its **input** at that point. \n",
    "\n",
    "Rate of change is simply a ratio between change in one variable divided by change in another variable.\n",
    "\n",
    "$$\\text{rate of change} = \\frac{\\text{change in variable 1}}{\\text{change in variable 2}}$$\n",
    "\n",
    "If we tried all points, it become a derivative of the whole function.\n",
    "\n",
    "#### Mathematical representations\n",
    "\n",
    "How much the output of $f$ changes as we change its input at a particular value $a$ of the input, can be described as limit:\n",
    "\n",
    "$$ \\frac{df}{da} = \n",
    "\\lim_{\\Delta \\to 0} \\frac{{f \\left( {a + \\Delta } \\right) - f\\left(a - \\Delta \\right)}}{2 * \\Delta } $$\n",
    "\n",
    "This limit can be approximated numerically by setting a very small value for $\\Delta$, such as 0.001, so we can compute the derivative as\n",
    "\n",
    "$$ \\frac{df}{da} = \n",
    "\\lim_{\\Delta \\to 0} \\frac{{f \\left( {a + 0.001 } \\right) - f\\left(a - 0.001 \\right)}}{0.002} $$\n",
    "\n",
    "#### Diagrams\n",
    "\n",
    "If we draw a tangent line to the function $f$, the derivative of $f$ at point $a$ is just the slope of this line at $a$.  One easy way to find the slope is to use Calculus.  Another way is to use limits.\n",
    "\n",
    "<img src = \"../figures/derivatives.png\" width=400>\n",
    "\n",
    "As an exercise, try find the derivative of a **square** function when $x = 2$.  Then attempt to verify using calculus.\n",
    "\n",
    "<img src = \"../figures/derivatives2.png\" width=400>\n",
    "\n",
    "Here is the calculations:\n",
    "\n",
    "$$ \\frac{df}{dx} = \\frac{2.001^2 - 1.999^2}{0.002} = 4 $$\n",
    "\n",
    "We can also get this number 4 using calculus\n",
    "\n",
    "$$ \\frac{df(x^2)}{dx} = 2x$$\n",
    "\n",
    "Replace $x$ with 2, we get\n",
    "\n",
    "$2(2) = 4$\n",
    "\n",
    "**The key question of derivative is here, so what does 4 here means?**\n",
    "\n",
    "This 4 means that **for every 0.001 increase when $x=2$, it will increase 4 $\\times$ 0.001 in the output**.\n",
    "\n",
    "We can easily verify this:\n",
    "\n",
    "$$2.001^2 = 4.004$$\n",
    "\n",
    "Thus, to rephrase our definition of derivative in the context of this example, derivative of the square function is the **rate of change** of the output $4$ in respect to its input $x = 2$.  \n",
    "\n",
    "Here we found that the rate of change is $4$, which mean every 1 unit increase in $x = 2$ will increase 4 units on the output.   Similarly, this rate of change will change for other points.  For example, if $x = 3$, then the rate of change is $6$ (verify by yourself).   When we tried different x, and we get different corresponding rate of change, we are getting a **derivative function** which describe the derivatives at every point $x$\n",
    "\n",
    "#### Code\n",
    "\n",
    "Finally, we can code up the derivative function as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9999999999995595"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Callable\n",
    "\n",
    "# Here, my Callable takes in an ndarray as an argument and produces an ndarray\n",
    "def deriv(func: Callable[[ndarray], ndarray],\n",
    "          input_: ndarray,\n",
    "          diff: float = 0.001) -> ndarray:\n",
    "    '''\n",
    "    Evaluates the derivative of a function \"func\" at every element in the \"input_\" array.\n",
    "    '''\n",
    "    return (func(input_ + diff) - func(input_ - diff)) / (2 * diff)\n",
    "\n",
    "deriv(square, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested Functions\n",
    "\n",
    "Functions can be nested to form **composite** functions.  \n",
    "\n",
    "#### Mathematical representations\n",
    "\n",
    "If we have two functions, let's say $f_1$ and $f_2$, the output of one of the functions becomes the input to the next one, \n",
    "\n",
    "$$ f_2(f_1(x)) = y $$\n",
    "\n",
    "This is a little bit counterintuitive but this reads first apply $f_1$ to $x$, then apply $f_2$ to the result of applying $f_1$ to $x$\n",
    "\n",
    "#### Diagrams\n",
    "\n",
    "The most natural way to represent a nested function is with the \"minifactory\" or \"box\" representations as follows:\n",
    "\n",
    "<img src = \"../figures/nested.png\" width=400>\n",
    "\n",
    "#### Code\n",
    "\n",
    "First, we'll define a data typoe for nested functions and called it *Chain*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# A Function takes in an ndarray as an argument and produces an ndarray\n",
    "Array_Function = Callable[[ndarray], ndarray]\n",
    "\n",
    "# A Chain is a list of functions\n",
    "Chain = List[Array_Function]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll define how data goes through a chain, first of length 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_length_2(chain: Chain,\n",
    "                   x: ndarray) -> ndarray:\n",
    "    '''\n",
    "    Evaluates two functions in a row, in a \"Chain\".\n",
    "    '''\n",
    "    assert len(chain) == 2, \\\n",
    "    \"Length of input 'chain' should be 2\"\n",
    "\n",
    "    f1 = chain[0]\n",
    "    f2 = chain[1]\n",
    "\n",
    "    return f2(f1(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try some simple example that the code works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9820137900379085"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_1 = [square, sigmoid]\n",
    "chain_length_2(chain_1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Chain rule\n",
    "\n",
    "The chain rule is a mathematical theorem that compute derivatives of composite functions.\n",
    "\n",
    "#### Mathematical representations\n",
    "\n",
    "Mathematically, the theorem states that, in the context of the above composite function, the derivatives of the composite function for a given value $x$ is\n",
    "\n",
    "$$ \\frac{df_2}{dx}(x) = \\frac{df_2}{du}(f_1(x)) * \\frac{df_1}{dx}(x) $$\n",
    "\n",
    "where $u$ is simply a dummy variable representing the input to a function\n",
    "\n",
    "#### Diagrams\n",
    "\n",
    "The idea behind is that the derivative of the composite function should be a sort of product of the derivatives of its constituent functions.\n",
    "\n",
    "![](../figures/chainrule2.png)\n",
    "\n",
    "Let's say that we feed the value 5 into the first function, and let's say further that computing the derivative of the first function at $x = 5$ gives us a value of 3, i.e., $\\frac{df_1(5)}{dx} = 3$.\n",
    "\n",
    "Let's say that we then take the value of the function that comes out of the first box — let’s suppose it is 1, so that $f_1(5) = 1$ and compute the derivative of the second function $f_2$ at this value: we found that, $\\frac{df_2(1)}{dx} = -2$. \n",
    "\n",
    "If we think about these functions as being literally strung together, then if changing the input to box two by 1 unit yields a change of –2 units in the output of box two, changing the input to box two by 3 units should change the output to box two by –2 × 3 = –6 units. This is why in the formula for the chain rule, the final result is ultimately a product: \n",
    "\n",
    "$$ \\frac{df_2}{dx}(x) = \\frac{df_2}{du}(f_1(x)) * \\frac{df_1}{dx}(x) $$\n",
    "\n",
    "#### Code\n",
    "\n",
    "Let's code this up along with <code>sigmoid</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_deriv_2(chain: Chain,\n",
    "                  x: ndarray) -> ndarray:\n",
    "    '''\n",
    "    Uses the chain rule to compute the derivative of two nested functions:\n",
    "    (f2(f1(x))' = f2'(f1(x)) * f1'(x)\n",
    "    '''\n",
    "\n",
    "    assert len(chain) == 2, \\\n",
    "    \"This function requires 'Chain' objects of length 2\"\n",
    "\n",
    "    f1 = chain[0]\n",
    "    f2 = chain[1]\n",
    "\n",
    "    # df1/dx\n",
    "    f1_of_x = f1(x)\n",
    "\n",
    "    # df1/dx\n",
    "    df1dx = deriv(f1, x)\n",
    "\n",
    "    # df2/du(f1(x))\n",
    "    df2du = deriv(f2, f1(x))\n",
    "\n",
    "    # Multiplying these quantities together at each point\n",
    "    return df1dx * df2du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward:  0.9820137900379085\n",
      "Backward:  0.0706508353807067\n"
     ]
    }
   ],
   "source": [
    "chain = [square, sigmoid]\n",
    "\n",
    "forward_pass = chain_length_2(chain, 2)\n",
    "print(\"Forward: \", forward_pass)\n",
    "\n",
    "backward_pass = chain_deriv_2(chain, 2)\n",
    "print(\"Backward: \", backward_pass)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify, we can do a manual calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward:  0.07065082585744342\n"
     ]
    }
   ],
   "source": [
    "delta_1 = chain_length_2(chain, 2 + 0.0001)\n",
    "delta_2 = chain_length_2(chain, 2 - 0.0001)\n",
    "manual_deriv = (delta_1 - delta_2)/ 0.0002\n",
    "\n",
    "print(\"Forward: \", manual_deriv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This manual_deriv means that if we increase the input to the composite function by 0.0001 unit, it will increase the output by <code>manual_deriv</code> * 0.001.  \n",
    "\n",
    "Let's check whether that's true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward_pass_increase:  0.9820208539349344\n",
      "forward manual:  0.9820208551204942\n"
     ]
    }
   ],
   "source": [
    "forward_pass_increase = chain_length_2(chain, 2 + 0.0001)\n",
    "print(\"forward_pass_increase: \", forward_pass_increase)\n",
    "\n",
    "rate_of_change = manual_deriv * 0.0001\n",
    "forward_pass = chain_length_2(chain, 2)\n",
    "print(\"forward manual: \", forward_pass + rate_of_change)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slightly Longer Example\n",
    "\n",
    "Let's look at a slighly longer example including $f_1, f_2, f_3$\n",
    "\n",
    "#### Mathematical representations\n",
    "\n",
    "Mathematically, the result turns out to be the following expression:\n",
    "\n",
    "$$ \\frac{df_3}{dx}(x) = \\frac{df_3}{du}(f_2(f_1(x))) * \\frac{df_2}{du}(f_1(x)) * \\frac{df_1}{dx}(x) $$\n",
    "\n",
    "\n",
    "#### Diagrams\n",
    "\n",
    "Here is the equivalent diagram:\n",
    "\n",
    "<img src = \"../figures/chain_3.png\" width=400>\n",
    "\n",
    "#### Code\n",
    "\n",
    "Interestingly, already in this simple example, we see the beginnings of what will become the forward and backward passes of a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_length_3(chain: Chain,\n",
    "                   x: ndarray) -> ndarray:\n",
    "    '''\n",
    "    Evaluates three functions in a row, in a \"Chain\".\n",
    "    '''\n",
    "    assert len(chain) == 3, \\\n",
    "    \"Length of input 'chain' should be 3\"\n",
    "\n",
    "    f1 = chain[0]\n",
    "    f2 = chain[1]\n",
    "    f3 = chain[2]\n",
    "\n",
    "    return f3(f2(f1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_deriv_3(chain: Chain,\n",
    "                  x: ndarray) -> ndarray:\n",
    "    '''\n",
    "    Uses the chain rule to compute the derivative of three nested functions:\n",
    "    (f3(f2(f1)))' = f3'(f2(f1(x))) * f2'(f1(x)) * f1'(x)\n",
    "    '''\n",
    "\n",
    "    assert len(chain) == 3, \\\n",
    "    \"This function requires 'Chain' objects to have length 3\"\n",
    "\n",
    "    f1 = chain[0]\n",
    "    f2 = chain[1]\n",
    "    f3 = chain[2]\n",
    "\n",
    "    # f1(x)\n",
    "    f1_of_x = f1(x)\n",
    "\n",
    "    # f2(f1(x))\n",
    "    f2_of_x = f2(f1_of_x)\n",
    "\n",
    "    # df3du\n",
    "    df3du = deriv(f3, f2_of_x)\n",
    "\n",
    "    # df2du\n",
    "    df2du = deriv(f2, f1_of_x)\n",
    "\n",
    "    # df1dx\n",
    "    df1dx = deriv(f1, x)\n",
    "\n",
    "    # Multiplying these quantities together at each point\n",
    "    return df1dx * df2du * df3du"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify this with $x = -4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward:  0.6547534606063192\n",
      "Backward:  -0.07233643296206184\n"
     ]
    }
   ],
   "source": [
    "chain = [leaky_relu, square, sigmoid]\n",
    "\n",
    "forward_pass = chain_length_3(chain, -4)\n",
    "print(\"Forward: \", forward_pass)\n",
    "\n",
    "backward_pass = chain_deriv_3(chain, -4)\n",
    "print(\"Backward: \", backward_pass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I will simply skip performing the manual_deriv and simply use backward_pass.  This backward_pass means that if we increase the input at point -4 to the composite function by 0.1 unit, it will increase the output by <code>backward_pass</code> * 0.1.  \n",
    "\n",
    "Let's check whether that's true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward_pass_increase:  0.6475757341144874\n",
      "forward manual:  0.647519817310113\n"
     ]
    }
   ],
   "source": [
    "forward_pass_increase = chain_length_3(chain, -4 + 0.1)\n",
    "print(\"forward_pass_increase: \", forward_pass_increase)\n",
    "\n",
    "rate_of_change = backward_pass * 0.1\n",
    "forward_pass = chain_length_3(chain, -4)\n",
    "print(\"forward manual: \", forward_pass + rate_of_change)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions with Multiple Inputs\n",
    "\n",
    "By this point, we have a conceptual understanding of how functions can be strung together to form composite functions. We also have a sense of how to represent these functions as series of boxes that inputs go into and outputs come out of. Finally, we’ve walked through how to compute the derivatives of these functions so that we understand these derivatives both mathematically and as quantities computed via a step-by-step process with a “forward” and “backward” component.\n",
    "\n",
    "Oftentimes, the functions we deal with in deep learning don’t have just one input. Instead, they have several inputs that at certain steps are added together, multiplied, or otherwise combined. As we’ll see, computing the derivatives of the outputs of these functions with respect to their inputs is still no problem: let’s consider a very simple scenario with multiple inputs, where two inputs are added together and then fed through another function.\n",
    "\n",
    "#### Mathematical representations\n",
    "\n",
    "If our inputs are x and y, then we could think of the function as occurring in two steps. \n",
    "\n",
    "In Step 1, $x$ and $y$ are fed through a function that adds them together. We’ll denote that function as $\\alpha$ (we’ll use Greek letters to refer to function names throughout) and the output of the function as $a$\n",
    "\n",
    "$$ a = \\alpha(x, y) = x + y $$\n",
    "\n",
    "Step 2 would be to feed a through some function $\\sigma$ ($\\sigma$ can be any continuous function, such as sigmoid, or the square function, or even a function whose name doesn’t start with s). We’ll denote the output of this function as s:\n",
    "\n",
    "$$ s = \\sigma(a) $$\n",
    "\n",
    "We could, equivalently, denote the entire function as $f$ and write:\n",
    "\n",
    "$$ f(x, y) = \\sigma(\\alpha(x, y)) $$\n",
    "\n",
    "Since we have two inputs, $x$ and $y$, we shall find the derivatives of the composite function in respect bo both $x$ and $y$.  Thus, we shall use partial derivatives.\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial x} = \\frac{\\partial \\sigma}{\\partial u}(\\alpha(x, y)) * \\frac{\\partial \\alpha}{\\partial x}((x, y))$$\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial y} = \\frac{\\partial \\sigma}{\\partial u}(\\alpha(x, y)) * \\frac{\\partial \\alpha}{\\partial y}((x, y))$$\n",
    "\n",
    "Now note that:\n",
    "\n",
    "$$\\frac{\\partial \\alpha}{\\partial x}((x, y)) = 1$$\n",
    "\n",
    "$$\\frac{\\partial \\alpha}{\\partial y}((x, y)) = 1$$\n",
    "\n",
    "since for every unit increase in $x$, a increases by one unit, no matter the value of x (the same holds for y).\n",
    "\n",
    "#### Diagrams\n",
    "\n",
    "Here is the equivalent diagram:\n",
    "\n",
    "<img src = \"../figures/multiple_inputs.png\" width=400>\n",
    "\n",
    "#### Code\n",
    "\n",
    "This is simple as follows.  \n",
    "\n",
    "Note: Whenever we deal with an operation that takes multiple ndarrays as inputs, we have to check their shapes to ensure they meet whatever conditions are required by that operation. Here, for a simple operation such as addition, all we need to check is that the shapes are identical so that the addition can happen elementwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_inputs_add(x: ndarray,\n",
    "                        y: ndarray,\n",
    "                        sigma: Array_Function) -> float:\n",
    "    '''\n",
    "    Function with multiple inputs and addition, forward pass\n",
    "    '''\n",
    "    assert x.shape == y.shape\n",
    "\n",
    "    a = x + y\n",
    "    return sigma(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_inputs_add_backward(x: ndarray,\n",
    "                                 y: ndarray,\n",
    "                                 sigma: Array_Function) -> float:\n",
    "    '''\n",
    "    Computes the derivative of this simple function with respect to\n",
    "    both inputs\n",
    "    '''\n",
    "    # Compute \"forward pass\"\n",
    "    a = x + y\n",
    "\n",
    "    # Compute derivatives\n",
    "    dsda = deriv(sigma, a)\n",
    "\n",
    "    dadx, dady = 1, 1\n",
    "\n",
    "    return dsda * dadx, dsda * dady"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward pass:  0.9990889488055994\n",
      "backward pass:  (0.0009102213310852036, 0.0009102213310852036)\n"
     ]
    }
   ],
   "source": [
    "forward_pass = multiple_inputs_add(np.array(3), np.array(4), sigmoid)\n",
    "print(\"forward pass: \", forward_pass)\n",
    "\n",
    "backward_pass = multiple_inputs_add_backward(np.array(3), np.array(4), sigmoid)\n",
    "print(\"backward pass: \", backward_pass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to verify as usual, starting with $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward pass:  0.9990898585726491\n",
      "forward pass:  0.9990898590269305\n"
     ]
    }
   ],
   "source": [
    "forward_pass_xincrease = multiple_inputs_add(np.array(3+0.001), np.array(4), sigmoid)\n",
    "print(\"forward pass: \", forward_pass_xincrease)\n",
    "print(\"forward pass: \", forward_pass + backward_pass[0] * 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward pass:  0.9990898585726491\n",
      "forward pass:  0.9990898590269305\n"
     ]
    }
   ],
   "source": [
    "forward_pass_yincrease = multiple_inputs_add(np.array(3), np.array(4+0.001), sigmoid)\n",
    "print(\"forward pass: \", forward_pass_yincrease)\n",
    "print(\"forward pass: \", forward_pass + backward_pass[1] * 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward pass:  0.9990907674320407\n",
      "forward pass:  0.9990907692482616\n"
     ]
    }
   ],
   "source": [
    "forward_pass_xyincrease = multiple_inputs_add(np.array(3+0.001), np.array(4+0.001), sigmoid)\n",
    "print(\"forward pass: \", forward_pass_xyincrease)\n",
    "print(\"forward pass: \", forward_pass + backward_pass[0] * 0.001 + backward_pass[1] * 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions with multiple vector inputs\n",
    "\n",
    "In deep learning, we deal with functions whose inputs are vectors or matrices. Not only can these objects be added, multiplied, and so on, but they can also combined via a dot product or a matrix multiplication. We shall show how the mathematics of the chain rule and the logic of computing the derivatives of these functions using a forward and backward pass can still apply.\n",
    "\n",
    "These techniques will end up being central to understanding why deep learning works. In deep learning, our goal will be to fit a model to some data. More precisely, this means that we want to find a mathematical function that maps observations from the data — which will be inputs to the function—to some desired predictions from the data — which will be the outputs of the function—in as optimal a way as possible. It turns out these observations will be encoded in matrices, typically with row as an observation and each column as a numeric feature for that observation. \n",
    "\n",
    "#### Mathematical representations\n",
    "\n",
    "We shall use this as our example:\n",
    "\n",
    "$$S = f(X, W) = \\sigma(\\nu(X, W)) = \\sigma(x_1 * w_1 + x_2 * w_2 + x_3 * w_3)$$\n",
    "\n",
    "That is, we first feed in $\\nu$ function which performs a dot product between $X$ and $W$.  Then we feed the output to a $\\sigma$ function.\n",
    "\n",
    "For details, let's first work on the $\\nu$ part.  Assume a $X$ with only one sample and can be depicted as vector as follows:\n",
    "\n",
    "$$X = [x_1, x_2, \\cdots, x_n]$$\n",
    "\n",
    "We shall also define weight as follows:\n",
    "\n",
    "$$ W = \\begin{bmatrix}\n",
    "w_{1} \\\\\n",
    "w_{2} \\\\\n",
    "\\vdots \\\\\n",
    "w_{n} \\\\\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "Then we could define a function $\\nu$ that calculates the weighted sum, which the output is denotes as $N$ as follows:\n",
    "\n",
    "$$N = \\nu(X, W) = X @ W = x_1 * w_1 + x_2 * w_2 + \\cdots + x_n * w_n$$\n",
    "\n",
    "The question is then what is $$\\frac{\\partial \\nu}{\\partial X}, \\frac{\\partial \\nu}{\\partial W}$$ \n",
    "\n",
    "How would we even define \"the derivative with respect to a matrix\"?  Recalling that the matrix syntas is just a shorthand for a bunch of numbers arranged in a particular form.  \n",
    "\n",
    "Since $X$ is a row, and let's say we have three features, we can define\n",
    "\n",
    "$$\\frac{\\partial \\nu}{\\partial X} = \\big[\\frac{\\partial \\nu}{\\partial x_1} \\ \\frac{\\partial \\nu}{\\partial x_2} \\ \\frac{\\partial \\nu}{\\partial x_3}\\big]$$\n",
    "\n",
    "However, the output of $\\nu$ is just a number: \n",
    "\n",
    "$$N = x_1 * w_1 + x_2 * w_2 + x_3 * w_3$$\n",
    "\n",
    "And looking at this, we can see that if, for example, $x_1$ change by $k$ units, then $N$ will change by $w_1 \\ \\times \\ k$ units - and the same logic applies to the other $x_i$ elements.  Thus\n",
    "\n",
    "$$\\frac{\\partial \\nu}{\\partial x_1} = w_1$$\n",
    "\n",
    "$$\\frac{\\partial \\nu}{\\partial x_2} = w_2$$\n",
    "\n",
    "$$\\frac{\\partial \\nu}{\\partial x_3} = w_3$$\n",
    "\n",
    "And so\n",
    "\n",
    "$$\\frac{\\partial \\nu}{\\partial X} = \\big[w_1 \\ w_2 \\ w_3\\big] = W^\\top$$\n",
    "\n",
    "Using the similar reasoning, we can see that\n",
    "\n",
    "$$\\frac{\\partial \\nu}{\\partial W} = \\begin{bmatrix}\n",
    "x_{1} \\\\\n",
    "x_{2} \\\\\n",
    "x_{3} \\\\\n",
    "\\end{bmatrix} = X^\\top$$\n",
    "\n",
    "Now, if we feed the output of $\\nu$ to $\\sigma$, and calculate the derivatives, we got:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial X} = \\frac{\\partial \\sigma} {\\partial u}(\\nu(X, W) \\ \\times \\ \\frac{\\partial \\nu}{\\partial X} (X, W)$$\n",
    "\n",
    "The first part is:\n",
    "\n",
    "$$\\frac{\\partial \\sigma} {\\partial u}(\\nu(X, W) = \\frac{\\partial \\sigma}{\\partial u}(x_1 * w_1 + x_2 * w_2 + x_3 * w_3)$$\n",
    "\n",
    "The second part is\n",
    "\n",
    "$$\\frac{\\partial \\nu}{\\partial X} (X, W) = W^\\top$$\n",
    "\n",
    "Thus \n",
    "\n",
    "$$\\frac{\\partial f}{\\partial X} = \\frac{\\partial \\sigma}{\\partial u}(x_1 * w_1 + x_2 * w_2 + x_3 * w_3) \\ @ \\ W^\\top$$\n",
    "\n",
    "\n",
    "#### Diagrams\n",
    "\n",
    "Here is the equivalent diagram:\n",
    "\n",
    "<img src = \"../figures/vector_op3.png\" width=400>\n",
    "\n",
    "and the derivative diagram:\n",
    "\n",
    "<img src = \"../figures/vector_op4.png\" width=400>\n",
    "\n",
    "#### Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_forward_extra(X: ndarray,\n",
    "                         W: ndarray,\n",
    "                         sigma: Array_Function) -> ndarray:\n",
    "    '''\n",
    "    Computes the forward pass of a function involving matrix multiplication, one extra function\n",
    "    '''\n",
    "    assert X.shape[1] == W.shape[0]\n",
    "\n",
    "    # matrix multiplication\n",
    "    N = np.dot(X, W)\n",
    "\n",
    "    # feeding the output of the matrix multiplication through sigma\n",
    "    S = sigma(N)\n",
    "\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_function_backward_1(X: ndarray,\n",
    "                               W: ndarray,\n",
    "                               sigma: Array_Function) -> ndarray:\n",
    "    '''\n",
    "    Computes the derivative of our matrix function with respect to\n",
    "    the first element.\n",
    "    '''\n",
    "    assert X.shape[1] == W.shape[0]\n",
    "\n",
    "    # matrix multiplication\n",
    "    N = np.dot(X, W)\n",
    "\n",
    "    # feeding the output of the matrix multiplication through sigma\n",
    "    S = sigma(N)\n",
    "\n",
    "    # backward calculation\n",
    "    dSdN = deriv(sigma, N)\n",
    "\n",
    "    # dNdX\n",
    "    dNdX = np.transpose(W, (1, 0))\n",
    "\n",
    "    # multiply them together; since dNdX is 1x1 here, order doesn't matter\n",
    "    return np.dot(dSdN, dNdX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  [[ 0.47231121  0.61514271 -1.72622715]]\n",
      "Forward:  [[0.89779986]]\n",
      "Backward:  [[ 0.08516695 -0.05574581 -0.11206627]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(190203)\n",
    "\n",
    "X = np.random.randn(1,3)\n",
    "W = np.random.randn(3,1)\n",
    "\n",
    "print(\"X: \", X)\n",
    "\n",
    "forward_pass = matrix_forward_extra(X, W, sigmoid)\n",
    "print(\"Forward: \", forward_pass)\n",
    "\n",
    "backward_pass = matrix_function_backward_1(X, W, sigmoid)\n",
    "print(\"Backward: \", backward_pass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify as usual.  If $x_1$ is increase by $k$ unit, the output should increase by <code>backward[0][0]</code> * $k$ units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward pass:  [[0.897885]]\n",
      "forward pass:  [[0.89788503]]\n"
     ]
    }
   ],
   "source": [
    "X_new = X.copy()\n",
    "X_new[:, 0] += 0.001\n",
    "forward_pass_add1 = matrix_forward_extra(X_new, W, sigmoid)\n",
    "print(\"forward pass: \", forward_pass_add1)\n",
    "print(\"forward pass: \", forward_pass + backward_pass[0][0] * 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions with matrix multiplication\n",
    "\n",
    "Now let's extend the previous section into matrix instead of vectors. \n",
    "\n",
    "#### Mathematical representations\n",
    "\n",
    "Let's suppose that\n",
    "\n",
    "$$ X = \\begin{bmatrix}\n",
    "x_{11} & x_{12} & x_{13} \\\\\n",
    "x_{21} & x_{22} & x_{23} \\\\\n",
    "x_{31} & x_{32} & x_{33}\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ W = \\begin{bmatrix}\n",
    "w_{11} & w_{12} \\\\\n",
    "w_{21} & w_{22} \\\\\n",
    "w_{31} & w_{32} \\\\\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "This could correspond to a dataset in which each observation has three features, and the three rows could correspond to three different observations for which we want to make predictions.\n",
    "\n",
    "We shall use the same function as the previous example where we first perform dot product and then feed into a sigma function.\n",
    "\n",
    "As before, the question now is: what are the gradients of the output $S$ with respect to $X$ and $W$? Can we simply use the chain rule again? Why or why not?\n",
    "\n",
    "If you think about this for a bit, you may realize that something is different from the previous examples that we’ve looked at: $S$ is now a matrix, not simply a number. And what, after all, does the gradient of one matrix with respect to another matrix mean?\n",
    "\n",
    "This leads us to a subtle but important idea: we may perform whatever series of operations on multidimensional arrays we want, but for the notion of a \"gradient\" with respect to some output to be well defined, we need to sum (or otherwise aggregate into a single number) the final array in the sequence so that the notion of \"how much will changing each element of $X$ affect the output\" will even make sense.\n",
    "\n",
    "So we’ll tack onto the end a third function, Lambda $\\Lambda$, that simply takes the elements of $S$ and sums them up.\n",
    "\n",
    "Let’s make this mathematically concrete. First, let’s multiply $X$ and $W$:\n",
    "\n",
    "$$ \\nu(X, W) = X @ W = \\begin{bmatrix}\n",
    "x_{11} * w_{11} + x_{12} * w_{21} + x_{13} * w_{31} &\n",
    "x_{11} * w_{12} + x_{12} * w_{22} + x_{13} * w_{32}\n",
    "\\\\\n",
    "x_{21} * w_{11} + x_{22} * w_{21} + x_{23} * w_{31} &\n",
    "x_{21} * w_{12} + x_{22} * w_{22} + x_{23} * w_{32}\n",
    "\\\\\n",
    "x_{31} * w_{11} + x_{32} * w_{21} + x_{33} * w_{31} &\n",
    "x_{31} * w_{12} + x_{32} * w_{22} + x_{33} * w_{32}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "XW_{11} &\n",
    "XW_{12}\n",
    "\\\\\n",
    "XW_{21} &\n",
    "XW_{22}\n",
    "\\\\\n",
    "XW_{31} &\n",
    "XW_{32}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here we denote row $i$ and column $j$ in the resulting matrix as $XW_{ij}$ for convenience.\n",
    "\n",
    "Next, we’ll feed this result through $\\sigma$, which just means applying $\\sigma$ to every element of the matrix $X @ W$\n",
    "\n",
    "$$ \\sigma(X @ W) = \\begin{bmatrix}\n",
    "\\sigma(x_{11} * w_{11} + x_{12} * w_{21} + x_{13} * w_{31}) &\n",
    "\\sigma(x_{11} * w_{12} + x_{12} * w_{22} + x_{13} * w_{32})\n",
    "\\\\\n",
    "\\sigma(x_{21} * w_{11} + x_{22} * w_{21} + x_{23} * w_{31}) &\n",
    "\\sigma(x_{21} * w_{12} + x_{22} * w_{22} + x_{23} * w_{32})\n",
    "\\\\\n",
    "\\sigma(x_{31} * w_{11} + x_{32} * w_{21} + x_{33} * w_{31}) &\n",
    "\\sigma(x_{31} * w_{12} + x_{32} * w_{22} + x_{33} * w_{32})\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "\\sigma(XW_{11}) & \\sigma(XW_{12})\\\\\n",
    "\\sigma(XW_{21}) & \\sigma(XW_{22})\\\\\n",
    "\\sigma(XW_{31}) & \\sigma(XW_{32})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Finally, we can simply sum up these elements:\n",
    "\n",
    "$$ L = \\Lambda(\\sigma(X * W)) = \\Lambda(\\begin{bmatrix}\n",
    "\\sigma(XW_{11}) & \\sigma(XW_{12})\\\\\n",
    "\\sigma(XW_{21}) & \\sigma(XW_{22})\\\\\n",
    "\\sigma(XW_{31}) & \\sigma(XW_{32})\n",
    "\\end{bmatrix}) =  \\sigma(XW_{11}) + \\sigma(XW_{12}) + \\sigma(XW_{21}) + \\sigma(XW_{22}) + \\sigma(XW_{31}) + \\sigma(XW_{32})\n",
    "$$\n",
    "\n",
    "Now we are back in a pure calculus setting: we have a number, $L$, and we want to figure out the gradient of $L$ with respect to $X$ and $W$; that is, we want to know how much changing each element of these input matrices ($x11$, $w21$, and so on) would change $L$. We can write this as:\n",
    "\n",
    "$$ \\frac{\\partial \\Lambda}{\\partial u}(X) = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial \\Lambda}{\\partial u}(x_{11}) & \n",
    "\\frac{\\partial \\Lambda}{\\partial u}(x_{12}) & \n",
    "\\frac{\\partial \\Lambda}{\\partial u}(x_{13}) \\\\\n",
    "\\frac{\\partial \\Lambda}{\\partial u}(x_{21}) & \n",
    "\\frac{\\partial \\Lambda}{\\partial u}(x_{22}) & \n",
    "\\frac{\\partial \\Lambda}{\\partial u}(x_{23}) \\\\\n",
    "\\frac{\\partial \\Lambda}{\\partial u}(x_{31}) & \n",
    "\\frac{\\partial \\Lambda}{\\partial u}(x_{32}) & \n",
    "\\frac{\\partial \\Lambda}{\\partial u}(x_{33}) \n",
    "\\end{bmatrix} $$\n",
    "\n",
    "This could be then solved by the following chain rule problem as follows: \n",
    "\n",
    "$$ \\frac{\\partial \\Lambda}{\\partial u}(X) = \n",
    "\\frac{\\partial \\nu}{\\partial X}(X, W) *\n",
    "\\frac{\\partial \\sigma}{\\partial u}(N) *\n",
    "\\frac{\\partial \\Lambda}{\\partial u}(S) $$\n",
    "\n",
    "Last term is the easiest.  We want to know how much $L$ will increase if each element of $S$ increases.  Since $L$ is the sum of all the element of $S$, this derivative is simply:\n",
    "\n",
    "$$ \\frac{\\partial \\Lambda}{\\partial u}(S) = \\begin{bmatrix}\n",
    "1 & 1\\\\\n",
    "1 & 1\\\\\n",
    "1 & 1\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "since increasing any element of $S$ by, say, 0.46 units would increase $\\Lambda$ by 0.46 units.\n",
    "\n",
    "Second term $\\frac{\\partial \\sigma}{\\partial u}(N)$ is also easy.  It is simply the partial derivative of $\\sigma$ at that point.  Since we didn't exactly define what is $\\sigma$, we shall leave it as like this:\n",
    "\n",
    "$$ \\frac{\\partial \\sigma}{\\partial u}(N) = \\begin{bmatrix}\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{11}) &\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{12}) \\\\\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{21}) &\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{22}) \\\\\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{31}) &\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{32})\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "Now, we can already find the product of second and last term which is:\n",
    "\n",
    "$$ \\frac{\\partial \\Lambda}{\\partial u}(N) = \n",
    "\\frac{\\partial \\Lambda}{\\partial u}(S) *\n",
    "\\frac{\\partial \\sigma}{\\partial u}(N) =  \\begin{bmatrix}\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{11}) &\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{12}) \\\\\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{21}) &\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{22}) \\\\\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{31}) &\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{32})\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "How about $$\\frac{\\partial \\nu}{\\partial X}(X, W)$$ and $$\\frac{\\partial \\nu}{\\partial W}(X, W)$$\n",
    "\n",
    "This is asking how much increasing each element of $X$ will increase each element of $N$.  Same for $W$ on $N$. This is actually difficult to think about since it is about asking how much increasing each element of $X$ (a $3 \\ \\times \\ 3$ matrix) will increase each element of $N$ (a $3 \\ \\times \\ 2$ matrix). \n",
    "\n",
    "Remember that L is literally:\n",
    "\n",
    "$$\\sigma(XW_{11}) + \\sigma(XW_{12}) + \\sigma(XW_{21}) + \\sigma(XW_{22}) + \\sigma(XW_{31}) + \\sigma(XW_{32})$$\n",
    "\n",
    "where this is shorthand for the fact that:\n",
    "\n",
    "$$\\sigma(XW_{11}) = \\sigma(x_{11} * w_{11} + x_{12} * w_{21} + x_{13} * w_{31})$$\n",
    "\n",
    "$$\\sigma(XW_{12}) = \\sigma(x_{11} * w_{12} + x_{12} * w_{22} + x_{13} * w_{32})$$\n",
    "\n",
    "Let's just zoom in on one expression.  What would it look like if we took the partial derivatives of say $\\sigma(XW_{11})$ with respect to every element of $X$ (which is utimately what we'll want to do with all six components of $L$)?\n",
    "\n",
    "Well, since:\n",
    "\n",
    "$$\\sigma(XW_{11}) = \\sigma(x_{11} * w_{11} + x_{12} * w_{21} + x_{13} * w_{31})$$\n",
    "\n",
    "It isn't too hard to see that the partial derivatives of this with respect to $x_1$, via a very simple applications of the chain rule is:\n",
    "\n",
    "$$\\frac{\\partial \\sigma(XW_{11})}{\\partial x_1} = \\frac{\\partial \\sigma}{\\partial u}(XW_{11}) \\ \\times \\ w_{11}$$\n",
    "\n",
    "Since the only thing that $x_{11}$ is multiplied by in the $XW_{11}$ expression is $w_{11}$, the partial derivative with respect to everything else is 0.\n",
    "\n",
    "Thus, using similar reasoning, we get\n",
    "\n",
    "$$\\frac{\\partial \\sigma(XW_{11})}{\\partial X} = \\begin{bmatrix}\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{11}) * w_{11} & \n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{11}) * w_{21} & \n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{11}) * w_{31} \\\\\n",
    "0 &\n",
    "0 & \n",
    "0 \\\\\n",
    "0 & \n",
    "0 & \n",
    "0 \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Similarly, we can also get for other part like this:\n",
    "\n",
    "$$\\sigma(XW_{32}) = \\sigma(x_{31} * w_{12} + x_{32} * w_{22} + x_{33} * w_{32})$$\n",
    "\n",
    "$$\\frac{\\partial \\sigma(XW_{32})}{\\partial X} = \\begin{bmatrix}\n",
    "0 & \n",
    "0 & \n",
    "0 \\\\\n",
    "0 &\n",
    "0 & \n",
    "0 \\\\\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{32}) * w_{12} & \n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{32}) * w_{22} & \n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{32}) * w_{32} \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "We shall omit other parts for brevity.\n",
    "\n",
    "Now we have all the components to actually compute $\\frac{\\partial \\Lambda}{\\partial X}(S)$\n",
    "directly. We can simply compute six matrices of the same form as the preceding matrices and add the results together.\n",
    "\n",
    "First, we'll actually write out explicitly that $\\frac{\\partial \\Lambda}{\\partial X}(S)$ is a sum of six matrices as follows:\n",
    "\n",
    "$$ \\frac{\\partial \\Lambda}{\\partial X}(S) = \n",
    "\\frac{\\partial \\sigma(XW_{11})}{\\partial X} + \n",
    "\\frac{\\partial \\sigma(XW_{12})}{\\partial X} + \n",
    "\\frac{\\partial \\sigma(XW_{21})}{\\partial X} + \n",
    "\\frac{\\partial \\sigma(XW_{22})}{\\partial X} + \n",
    "\\frac{\\partial \\sigma(XW_{31})}{\\partial X} + \n",
    "\\frac{\\partial \\sigma(XW_{32})}{\\partial X} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{11}) * w_{11} & \n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{11}) * w_{21} & \n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{11}) * w_{31} \\\\\n",
    "0 &\n",
    "0 & \n",
    "0 \\\\\n",
    "0 & \n",
    "0 & \n",
    "0 \\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{12}) * w_{12} & \n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{12}) * w_{22} & \n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{12}) * w_{32} \\\\\n",
    "0 &\n",
    "0 & \n",
    "0 \\\\\n",
    "0 & \n",
    "0 & \n",
    "0 \\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "0 & \n",
    "0 & \n",
    "0 \\\\\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{21}) * w_{11} &\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{21}) * w_{21} & \n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{21}) * w_{31} \\\\\n",
    "0 & \n",
    "0 & \n",
    "0 \\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "0 & \n",
    "0 & \n",
    "0 \\\\\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{22}) * w_{12} &\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{22}) * w_{22} & \n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{22}) * w_{32} \\\\\n",
    "0 & \n",
    "0 & \n",
    "0 \\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "0 & \n",
    "0 &\n",
    "0 \\\\\n",
    "0 &\n",
    "0 & \n",
    "0 \\\\\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{31}) * w_{11} &\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{31}) * w_{21} & \n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{31}) * w_{31} \\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "0 &\n",
    "0 &\n",
    "0 \\\\\n",
    "0 &\n",
    "0 & \n",
    "0 \\\\\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{32}) * w_{12} & \n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{32}) * w_{22} & \n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{32}) * w_{32} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Combining that yields:\n",
    "\n",
    "$$ \\frac{\\partial \\Lambda}{\\partial X}(S) = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{11}) * w_{11} + \\frac{\\partial \\sigma}{\\partial u}(XW_{12}) * w_{12} & \n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{11}) * w_{21} + \\frac{\\partial \\sigma}{\\partial u}(XW_{12}) * w_{22} & \n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{11}) * w_{31} + \\frac{\\partial \\sigma}{\\partial u}(XW_{12}) * w_{32} \\\\ \n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{21}) * w_{11} + \\frac{\\partial \\sigma}{\\partial u}(XW_{22}) * w_{12} & \n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{21}) * w_{21} + \\frac{\\partial \\sigma}{\\partial u}(XW_{22}) * w_{22} & \n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{21}) * w_{31} + \\frac{\\partial \\sigma}{\\partial u}(XW_{22}) * w_{32} \\\\ \n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{31}) * w_{11} + \\frac{\\partial \\sigma}{\\partial u}(XW_{32}) * w_{12} & \n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{31}) * w_{21} + \\frac{\\partial \\sigma}{\\partial u}(XW_{32}) * w_{22} & \n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{31}) * w_{31} + \\frac{\\partial \\sigma}{\\partial u}(XW_{32}) * w_{32} \\end{bmatrix} \n",
    "$$\n",
    "\n",
    "Now comes the most amazing part!  Recall that:\n",
    "\n",
    "$$ W = \\begin{bmatrix}\n",
    "w_{11} & w_{12} \\\\\n",
    "w_{21} & w_{22} \\\\\n",
    "w_{31} & w_{32} \\end{bmatrix} $$\n",
    "\n",
    "Well, $W$ is hidden in the preceding matrix — it is just transposed. \n",
    "\n",
    "Recalling that:\n",
    "\n",
    "$$ \\frac{\\partial \\Lambda}{\\partial u}(N) = \n",
    "\\frac{\\partial \\Lambda}{\\partial u}(S) *\n",
    "\\frac{\\partial \\sigma}{\\partial u}(N) =  \\begin{bmatrix}\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{11}) &\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{12}) \\\\\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{21}) &\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{22}) \\\\\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{31}) &\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{32})\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "It turns out that the previous matrix is equivalent to:\n",
    "\n",
    "$$ \\frac{\\partial \\Lambda}{\\partial X}(X) = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{11}) &\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{12}) \\\\\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{21}) &\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{22}) \\\\\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{31}) &\n",
    "\\frac{\\partial \\sigma}{\\partial u}(XW_{32})\n",
    "\\end{bmatrix} * \n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{21} & w_{31} \\\\\n",
    "w_{12} & w_{22} & w_{32} \\\\\n",
    "\\end{bmatrix} = \\frac{\\partial \\Lambda}{\\partial u}(S) *\n",
    "\\frac{\\partial \\sigma}{\\partial u}(N) \\ @ \\ W^T\n",
    "$$\n",
    "\n",
    "Thus, \n",
    "\n",
    "$$ \\frac{\\partial \\Lambda}{\\partial u}(X) = \n",
    "\\frac{\\partial \\Lambda}{\\partial u}(S) * \n",
    "\\frac{\\partial \\sigma}{\\partial u}(N) \\ @ \\\n",
    "W^T\n",
    "$$\n",
    "\n",
    "So nice, right!?\n",
    "\n",
    "Note: The expression for the gradient of $L$ with respect to $W$ would be $X^\\top$. However, because of the order in which the $X^\\top$ expression factors out of the derivative for $L$, $X^\\top$ would be on the left side of the expression for the gradient of $L$ with respect to $W$:\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial \\Lambda}{\\partial u}(W) = X^T \\ @ \\\n",
    "\\frac{\\partial \\Lambda}{\\partial u}(S) * \n",
    "\\frac{\\partial \\sigma}{\\partial u}(N)$$\n",
    "\n",
    "In code, therefore, while we would have <code>dNdW = np.transpose(X, (1, 0))</code>, the next step would be: <code>dLdW = np.dot(dNdW, dSdN)</code>\n",
    "instead of dLdX = np.dot(dSdN, dNdX) as before.\n",
    "\n",
    "#### Diagrams\n",
    "\n",
    "We are simply sending inputs forward as before. We claim that even in this more complicated scenario, we should be able to calculate the gradients we need using the chain rule.\n",
    "\n",
    "<img src = \"../figures/neuralnet1.png\" width=400>\n",
    "\n",
    "We simply need to calculate the partial derivative of each constituent function and evaluate it at its input, multiplying the results together to get the final derivative. \n",
    "\n",
    "<img src = \"../figures/neuralnet1_1.png\" width=400>\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_function_forward_sum(X: ndarray,\n",
    "                                W: ndarray,\n",
    "                                sigma: Array_Function) -> float:\n",
    "    '''\n",
    "    Computing the result of the forward pass of this function with\n",
    "    input ndarrays X and W and function sigma.\n",
    "    '''\n",
    "    assert X.shape[1] == W.shape[0]\n",
    "\n",
    "    # matrix multiplication\n",
    "    N = np.dot(X, W)\n",
    "\n",
    "    # feeding the output of the matrix multiplication through sigma\n",
    "    S = sigma(N)\n",
    "\n",
    "    # sum all the elements\n",
    "    L = np.sum(S)\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_function_backward_sum_1(X: ndarray,\n",
    "                                   W: ndarray,\n",
    "                                   sigma: Array_Function) -> ndarray:\n",
    "    '''\n",
    "    Compute derivative of matrix function with a sum with respect to the\n",
    "    first matrix input\n",
    "    '''\n",
    "    assert X.shape[1] == W.shape[0]\n",
    "\n",
    "    # matrix multiplication\n",
    "    N = np.dot(X, W)\n",
    "\n",
    "    # feeding the output of the matrix multiplication through sigma\n",
    "    S = sigma(N)\n",
    "\n",
    "    # sum all the elements\n",
    "    L = np.sum(S)\n",
    "\n",
    "    # note: I'll refer to the derivatives by their quantities here,\n",
    "    # unlike the math where we referred to their function names\n",
    "\n",
    "    # dLdS - just 1s\n",
    "    dLdS = np.ones_like(S)\n",
    "\n",
    "    # dSdN\n",
    "    dSdN = deriv(sigma, N)\n",
    "    \n",
    "    # dLdN\n",
    "    dLdN = dLdS * dSdN\n",
    "\n",
    "    # dNdX\n",
    "    dNdX = np.transpose(W, (1, 0))\n",
    "\n",
    "    # dLdX\n",
    "    dLdX = np.dot(dSdN, dNdX)\n",
    "\n",
    "    return dLdX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "[[-1.57752816 -0.6664228   0.63910406]\n",
      " [-0.56152218  0.73729959 -1.42307821]\n",
      " [-1.44348429 -0.39128029  0.1539322 ]]\n",
      "L:\n",
      "2.3755\n",
      "\n",
      "dLdX:\n",
      "[[ 0.2488887  -0.37478057  0.01121962]\n",
      " [ 0.12604152 -0.27807404 -0.13945837]\n",
      " [ 0.22992798 -0.36623443 -0.02252592]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(190204)\n",
    "X = np.random.randn(3, 3)\n",
    "W = np.random.randn(3, 2)\n",
    "\n",
    "print(\"X:\")\n",
    "print(X)\n",
    "\n",
    "print(\"L:\")\n",
    "print(round(matrix_function_forward_sum(X, W, sigmoid), 4))\n",
    "print()\n",
    "print(\"dLdX:\")\n",
    "print(matrix_function_backward_sum_1(X, W , sigmoid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2489\n"
     ]
    }
   ],
   "source": [
    "X1 = X.copy()\n",
    "X1[0, 0] += 0.001\n",
    "\n",
    "print(round(\n",
    "        (matrix_function_forward_sum(X1, W, sigmoid) - \\\n",
    "         matrix_function_forward_sum(X, W, sigmoid)) / 0.001, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression as Neural Network\n",
    "\n",
    "In the last section, we have learned roughly the basic components of neural network.  Let's put them further and implement linear regression as neural network.  Linear regression is a good example since it is something we already understand and would be perfect example to see that neural network in the most basic form, is actually linear regression!\n",
    "\n",
    "Given the following neural network for linear regression:\n",
    "\n",
    "![](../figures/linearneuralnet.png)\n",
    "\n",
    "Code can be simply written as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, Tuple, List\n",
    "\n",
    "def forward(X: ndarray, y: ndarray,\n",
    "            weights: Dict[str, ndarray]\n",
    "           )-> Tuple[float, Dict[str, ndarray]]:\n",
    "    '''\n",
    "    Forward pass for the step-by-step linear regression.\n",
    "    '''\n",
    "    # assert batch sizes of X and y are equal\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "\n",
    "    # assert that matrix multiplication can work\n",
    "    assert X.shape[1] == weights['W'].shape[0]\n",
    "\n",
    "    # assert that B is simply a 1x1 ndarray\n",
    "    assert weights['B'].shape[0] == weights['B'].shape[1] == 1\n",
    "\n",
    "    # compute the operations on the forward pass\n",
    "    N = X @ weights['W']\n",
    "\n",
    "    P = N + weights['B']\n",
    "\n",
    "    loss = np.mean(np.power(y - P, 2))\n",
    "\n",
    "    # save the information computed on the forward pass\n",
    "    forward_info: Dict[str, ndarray] = {}\n",
    "    forward_info['X'] = X\n",
    "    forward_info['N'] = N\n",
    "    forward_info['P'] = P\n",
    "    forward_info['y'] = y\n",
    "\n",
    "    return forward_info, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating gradients\n",
    "\n",
    "The next step is to train the model using gradient descent.  To calculate the gradient, we simply apply chain rule and multiply their products together.  The main derivatives we need is $\\frac{\\partial \\Lambda}{\\partial W}$ and $\\frac{\\partial \\Lambda}{\\partial B}$  where \n",
    "\n",
    "$$ \\frac{\\partial \\Lambda}{\\partial W} = \n",
    "    \\frac{\\partial \\Lambda}{\\partial P}(P, Y) *\n",
    "    \\frac{\\partial \\alpha}{\\partial N}(N, B) *\n",
    "    \\frac{\\partial \\nu}{\\partial W}(X, W)\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$ \\frac{\\partial \\Lambda}{\\partial B} = \n",
    "    \\frac{\\partial \\Lambda}{\\partial P}(P, Y) *\n",
    "    \\frac{\\partial \\alpha}{\\partial B}(N, B)\n",
    "$$\n",
    "\n",
    "Since $$ \\Lambda(P, Y) = (Y - P)^2 $$, thus\n",
    "\n",
    "$$ \\frac{\\partial \\Lambda}{\\partial P}(P, Y) = \n",
    "-1 * (2 * (Y - P)^2)\n",
    "$$\n",
    "\n",
    "Next, for $$ \\frac{\\partial \\alpha}{\\partial N}(N, B) $$\n",
    "\n",
    "Since $\\alpha$ is simply addition, increasing any element of N by one unit will increase N + B by one unit, thus $$ \\frac{\\partial \\alpha}{\\partial N}(N, B) $$ is simply a list of ones with shape of N like:\n",
    "\n",
    "<code>dPdN = np.ones_like(N) </code>\n",
    "\n",
    "Similarly, $$ \\frac{\\partial \\alpha}{\\partial B}(N, B) $$ is simply a list of ones with shape of B like:\n",
    "\n",
    "<code>dPdB = np.ones_like(B) </code>\n",
    "\n",
    "Last, we have $$ \\frac{\\partial \\nu}{\\partial W}(X, W) $$  This is easy as we know already that \n",
    "\n",
    "$$ \\frac{\\partial \\nu}{\\partial W}(X, W) = X^T $$ \n",
    "\n",
    "Thus\n",
    "\n",
    "$$ \\frac{\\partial \\nu}{\\partial W}(X, W) $$\n",
    "\n",
    "Last, there is small caveat is that we sum the expressions we compute for dLdB along axis = 0.  This is because $b$ is added to each row of the matrix, thus if b changes, it will affect all rows.  Thus the gradient in respect to b shall be summed along all the rows. This is why we sum the expressions for dLdB along axis=0.\n",
    "\n",
    "Now, without further do....let's code right now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_gradients(forward_info: Dict[str, ndarray],\n",
    "                   weights: Dict[str, ndarray]) -> Dict[str, ndarray]:\n",
    "\n",
    "    dLdP = -2 * (forward_info['y'] - forward_info['P'])\n",
    "\n",
    "    dPdN = np.ones_like(forward_info['N'])\n",
    "\n",
    "    dPdB = np.ones_like(weights['B'])\n",
    "\n",
    "    dLdN = dLdP * dPdN\n",
    "\n",
    "    dNdW = forward_info['X'].T\n",
    "    \n",
    "    # need to use matrix multiplication here,\n",
    "    # with dNdW on the left    \n",
    "    dLdW = dNdW @ dLdN\n",
    "\n",
    "    # need to sum along dimension since b is added to every row   \n",
    "    dLdB = (dLdP * dPdB).sum(axis=0)\n",
    "\n",
    "    loss_gradients: Dict[str, ndarrayy] = {}\n",
    "    loss_gradients['W'] = dLdW\n",
    "    loss_gradients['B'] = dLdB\n",
    "\n",
    "    return loss_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting everything together into a train method\n",
    "\n",
    "To combine the loss_gradients and forward into a single train method, as well as some shuffling, we can do as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(n_in: int) -> Dict[str, ndarray]:\n",
    "    '''\n",
    "    Initialize weights on first forward pass of model.\n",
    "    '''\n",
    "    \n",
    "    weights: Dict[str, ndarray] = {}\n",
    "    W = np.random.randn(n_in, 1)\n",
    "    B = np.random.randn(1, 1)\n",
    "    \n",
    "    weights['W'] = W\n",
    "    weights['B'] = B\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X: ndarray, \n",
    "          y: ndarray, \n",
    "          n_iter: int = 1000,\n",
    "          learning_rate: float = 0.01,\n",
    "          batch_size: int = 100,\n",
    "          return_losses: bool = False, \n",
    "          return_weights: bool = False) -> None:\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    start = 0\n",
    "\n",
    "    #initialize randomom weights\n",
    "    weights = init_weights(X.shape[1])\n",
    "\n",
    "    # Permute data\n",
    "    perm = np.random.permutation(X.shape[0])\n",
    "    X, y = X[perm], y[perm]\n",
    "    \n",
    "    if return_losses:\n",
    "        losses = []\n",
    "\n",
    "    for i in range(n_iter):\n",
    "\n",
    "        # in case all data is used\n",
    "        if start >= X.shape[0]:\n",
    "            perm = np.random.permutation(X.shape[0])\n",
    "            X, y = X[perm], y[perm]\n",
    "            start = 0\n",
    "        \n",
    "        #reduce batch size if exceeds\n",
    "        if start+batch_size > X.shape[0]:\n",
    "            batch_size = X.shape[0] - start\n",
    "    \n",
    "        X_batch, y_batch = X[start:start+batch_size], y[start:start+batch_size]\n",
    "        start += batch_size\n",
    "    \n",
    "        # Train net using generated batch\n",
    "        forward_info, loss = forward(X_batch, y_batch, weights)\n",
    "\n",
    "        if return_losses:\n",
    "            losses.append(loss)\n",
    "\n",
    "        loss_grads = loss_gradients(forward_info, weights)\n",
    "        \n",
    "        #loss_grads and weights have same keys\n",
    "        for key in weights.keys():\n",
    "            weights[key] -= learning_rate * loss_grads[key]\n",
    "\n",
    "    if return_weights:\n",
    "        return losses, weights\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's test our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "features = boston.feature_names\n",
    "s = StandardScaler()\n",
    "X = s.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "#since our train function assumes y to be shape of (n, 1)\n",
    "y_train, y_test = y_train.reshape(-1, 1), y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info = train(X_train, y_train,\n",
    "                   n_iter = 10000,\n",
    "                   learning_rate = 0.001,\n",
    "                   batch_size=23, \n",
    "                   return_losses=True, \n",
    "                   return_weights=True)\n",
    "losses = train_info[0]\n",
    "weights = train_info[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X: ndarray,\n",
    "            weights: Dict[str, ndarray]):\n",
    "    \n",
    "    N = X @ weights['W']\n",
    "\n",
    "    return N + weights['B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  21.962219458334552\n",
      "$R^2$:  0.7052569177141323\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(X_test, weights)\n",
    "print(\"MSE: \", mean_squared_error(y_test, y_pred))\n",
    "print(\"$R^2$: \", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the actual vs. predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x124d8e610>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de5RcZZnv8e8vTQMdbg0YOdAQgoIwIErGHoGJwwFGxQtKBAXRcVAZkXNmxjtjmDVrRI8u48IRnVFHURyZw12BgOgYGYI3FCQxKHI73rjYIERJA0KrSXjOH/utUF2py67q2lXVvX+ftXqlalfV3m9XpZ/91vO++3kVEZiZWXnM63cDzMystxz4zcxKxoHfzKxkHPjNzErGgd/MrGQc+M3MSsaB3woj6YuSPphu/4Wku3p03JC0by+ONUgkHSnpVwXs942Svtvt/Vr/OPCXnKS7JU1J+p2kB1Ow3r7bx4mI70TE/jnaM2uDjKSz0knn0JzPX5Sev1XRbTOr5sBvAK+IiO2BPwXGgX+qfYKDU3OSBPw18HD612xgOfDbZhExAfwX8GzYnDL5W0k/BX6ath0r6RZJk5K+J+k5lddLWizph5Iek3QpsG3VY9PSEJL2knSFpHWSfivpk5L+BPgMcHj6BjKZnruNpI9Kujd9K/mMpJGqfZ0h6QFJ90t6c6PfT9JJklbXbHunpKvT7ZdJuj21f0LSe9p4+/4C2B14G/BaSVtXHWNE0r9IukfSI5K+m9r/7fSUyfT7Hp6+NVxQ9dpp3wokvUnSHamNv5D01jyNk/Tvkj5as+0qSe9Kt5dJ+nna7+2SXtVgP1t8S5H0TUl/U3X/zamN6yWtlLR32i5J50h6SNKjkm6V9Ow87bfucuC3zSTtBbwMWFu1eSlwKHCgpMXAF4C3ArsCnwWuToF5a2AF8H+BXYAvASc0OM4QcA1wD7AIGAMuiYg7gNOB70fE9hExml6yHHgWcAiwb3r+P6d9vQR4D/AiYD/ghU1+xa8A+0var2rb64CL0u3zgLdGxA5kJ79VTfZV65S0/8vS/VdUPfZR4HnAn5O9N/8APAkckR4fTb/v93Mc5yHgWGBH4E3AOZL+NMfrLgZOSt9MkLQz8GLgkvT4z8lOXjsB7wcukLR7jv1OI+k44B+B44EFwHfSsUnHO4Lss9wJOBH4bbvHsC6ICP+U+Ae4G/gdMEkWiD8NjKTHAji66rn/DvyfmtffBfxPsj/o+wFVPfY94IPp9pHAr9Ltw4F1wFZ12vNG4LtV9wU8DjyzatvhwC/T7S8Ay6see1Zq974Nft8LgH9Ot/cDHgPmp/v3kp3UdmzzPZwPPAosTfc/C1yVbs8DpoDn1nndotTWraq2nQVc0Ow5NftYAby99j2u8zyl3++IdP8twKomv9MtwHG1n0mDNn8T+Jt0+7+AU6semwc8AewNHA38P+AwYF6//++X+cc9foMsYI1GxN4R8b8jYqrqsfuqbu8NvDuleSZTKmYvYI/0MxHprz25p8Hx9gLuiYiNOdq2gCywrqk65tfTdtJxq9vY6JgVFwEnp9uvA1ZExBPp/glk33jukfQtSYfnaB/Aq4CNwNfS/QuBl0paADyNLOX185z7akrSSyXdKOnh9F68LB2jqfS5XML03/3Cqv3+dVUKb5LsG0/L/daxN/CJqv08THbSGYuIVcAngU8BD0k6V9KOHRzDZsiB31qpDuT3AR9KJ4nKz/yIuBh4ABirpBKShQ32eR+wUPUHjGvLxf6GrMd8UNUxd4psMJp03L1yHLPiWmCBpEPIgmAlzUNE3BwRxwFPJ+tJX1Z/F1s4BdgeuFfSr8nSXMNkwfU3wO+BZ9Z5Xb3SuI+Tnegq/kflhqRtgMvJUke7RZYK+xpZYM3jYuDVKed+aNoX6f7ngL8Ddk37/UmD/T6e/q3bRrLP9q01/0dGIuJ7ABHxrxHxPOBAsm9nZ+Rsu3WRA7+143PA6ZIOTQN120l6uaQdgO+T9XrfJmlY0vHA8xvs5wdkAXt52se2kpakxx4E9qwMjkbEk+m450h6OoCkMUnHpOdfBrxR0oGS5gPva/YLRMQGssB8Nlm+/dq0z60lvV7STuk5j5Ll4ZuSNAb8JVne/ZD081zgI8Bfp/Z/AfiYpD0kDaVB3G3I0l1PAs+o2uUtwBGSFkraCTiz6rGtgcrrNkp6KVnePJeIWEt2Ivo8sDIiJtND25GdhNal3+lNpAH+OvtYB0wAf5V+lzcz/aT2GeBMSQelfe0k6TXp9p+l/zvDZCeQ35PjPbbuc+C33CJiNVlu+JPAeuBnZPlfIuKPZAN6byT7en8ScEWD/WwiG/zclyzv/Kv0fMgGVG8Dfi3pN2nbe9OxbpT0KPDfwP5pX/8FfDy97mfkG5C9iGwQ+Es16aY3AHenY5wOvB4gBeHfSar3beINwC0R8Y2I+HXlB/hX4Dlp1sp7gFuBm9N78xGyHPcTwIeAG1Jq5LCIuBa4FPgxsIZsELzyvj1GNmvoMrL3/3XA1Tl+33q/e/U3nduBfyE7eT8IHAzc0GQfbyHrqf8WOIhsLKeyryvT73dJeh9/Arw0Pbwj2Ul8PVlK7rdkJ2DrMU1PyZqZ2VznHr+ZWck48JuZlYwDv5lZyTjwm5mVzKwovPW0pz0tFi1a1O9mmJnNKmvWrPlNRCyo3V5o4Jd0N9kl8ZuAjRExLmkXsulqi8jKBZwYEeub7WfRokWsXr262VPMzKyGpLpXsveix39URPym6v4y4LqIWC5pWbr/3h60w2zGVqyd4OyVd3H/5BR7jI5wxjH7s3TxWL+bVUqVz2JicoohiU0RjOX8TMr+OfYj1XMcWTEpgPPJCjw58NvAW7F2gjOvuJWpDZsAmJic4swrbgUoVdAYBLWfxaZ0PVKez8SfY/GDuwF8Q9IaSaelbbtFxAPp9q+B3Qpug1lXnL3yrs3BomJqwybOXtmTFSWtSr3PoqLVZ+LPsfge/wsiYiLVWLlW0p3VD0ZESKp76XA6UZwGsHBhq7pbZsW7f3Kqre1WnFbvebPH/TkW3OOPbEUnIuIh4Eqyol0PVhZ4SP8+1OC150bEeESML1iwxaC0Wc/tMTrS1nYrTqv3vNnj/hwLDPyp6uIOldtkVQR/QlZU6pT0tFOAq4pqg1k3nXHM/owMD03bNjI8xBnHtFxD3rqs3mdR0eoz8edYbKpnN+DKVJ59K+CiiPi6pJuByySdSlah78QC22DWNZWBvzLPBhkU1Z9FvVk9AEuWr6r7OflznCXVOcfHx8Pz+M0sj9pZO5D16D98/MGlCu4AktZExHjtdpdsMLM5xbN2WnPgN7M5xbN2WnPgN7M5xbN2WnPgN7M5xbN2WpsV1TnNzPLyrJ3WHPjNbM5ZunjMgb4Jp3rMzErGgd/MrGQc+M3MSsaB38ysZBz4zcxKxrN6zGxglH1JxF5x4DezgeAlEXvHqR4zGwgurtY7DvxmNhBcXK13HPjNbCC4uFrvOPCb2UBwcbXe8eCumQ0EF1frHQd+MxsYLq7WGw78ZjbQPLe/+xz4zWxgeW5/MTy4a2YDy3P7i+HAb2YDy3P7i+HAb2YDy3P7i+HAb2YDy3P7i+HBXTMbWJ7bXwwHfjMbaJ7b331O9ZiZlYwDv5lZyTjwm5mVjAO/mVnJOPCbmZWMA7+ZWck48JuZlUzhgV/SkKS1kq5J9/eRdJOkn0m6VNLWRbfBzMye0ose/9uBO6rufwQ4JyL2BdYDp/agDWbWoRVrJ1iyfBX7LPsqS5avYsXaiX43yWao0MAvaU/g5cDn030BRwNfTk85H1haZBvMrHOVevgTk1MET9XDd/Cf3Yru8X8c+AfgyXR/V2AyIjam+78C6l6LLek0SaslrV63bl3BzTSzelwPf24qLPBLOhZ4KCLWdPL6iDg3IsYjYnzBggVdbp2Z5eF6+HNTkUXalgCvlPQyYFtgR+ATwKikrVKvf0/A3xnNBtQeoyNM1Anyroc/uxXW44+IMyNiz4hYBLwWWBURrweuB16dnnYKcFVRbTCzmXE9/LmpH/P43wu8S9LPyHL+5/WhDWaWw9LFY3z4+IMZGx1BwNjoCB8+/mCXSZ7lFBH9bkNL4+PjsXr16n43w8xsVpG0JiLGa7d7IRYzA7Kpm17pqhwc+M1s83z9ytTNynx9wMF/DnKtHjPzfP2SceA3M8/XLxkHfjNrOC/f8/XnJgd+M/N8/ZLx4K6ZbR7A9ayecnDgN+uB2TBVcunisYFrkxXDgd+sYJ4qaYPGOX6zgnmqpA0aB36zgnmqpA0ap3rMClZ0aeOixg9mw7iEdcY9frOCFTlVsqilEb3k4tzmwG9WsCJLGxc1fuBxibnNqR6zHihqqmSjcYKJySlWrJ3o+Jgel5jb3OM3m8WajRPMJDXjEg5zmwO/2SxWb/ygYiapGZdwmNuc6jGbxSqpnHdcekvdxztNzbiEw9zmwG82yy1dPMbZK+/q+pRRl3CYu5zqMZsDnJqxdrjHbzYHODVj7XDgN5sjnJqxvJzqMTMrGQd+M7OScarHzArjQm+DyYHfzArhBWgGl1M9ZlYIF3obXLl6/JJeAOwXEf8haQGwfUT8stimmdlM1KZZFu06wo2/WM+mCIYkTj50Lz649ODCju9Cb4OrZeCX9D5gHNgf+A9gGLgAWFJs08ysU/XSLNVX9m6K4IIb7wUoLPgXvQCNdS5PqudVwCuBxwEi4n5ghyIbZWYzUy/NUs/FN91XWBt8NfHgypPq+WNEhKQAkLRdwW0ysw5Up3Yi52s2Rd5nts9XEw+uPIH/MkmfBUYlvQV4M/C5YptlZu2oTe3kNSQV1KKMryYeTC0Df0R8VNKLgEfJ8vz/HBHXFt4yM8stb2qn1smH7lVAa2zQ5ZrVkwK9g73ZgGo2U0bQl1k9NrjyzOp5DDanDLcmm9XzeETs2OJ12wLfBrZJx/lyRLxP0j7AJcCuwBrgDRHxx85/BTNrNINmbHSEG5Yd3YcW2SBrOasnInaIiB1ToB8BTgA+nWPffwCOjojnAocAL5F0GPAR4JyI2BdYD5zacevNBtCKtRMsWb6KfZZ9lSXLV3W87m07PIPG2tHWlbuRWQEck/O5v0t3h9NPAEcDX07bzweWttMGs0FWGWSdSDNrKmUKig7+SxeP8eHjD2ZsdASR9fQ/fPzBHli1uvKkeo6vujuP7GKu3+fZuaQhsnTOvsCngJ8DkxGxMT3lV0Dd/5mSTgNOA1i4cGGew5n1XbMyBUUHYc+gsbzyDO6+our2RuBu4Lg8O4+ITcAhkkaBK4ED8jYsIs4FzgUYHx8vbrKxWRf1okxBq4qXjR53pUyryDOd800zPUhETEq6Hjic7HqArVKvf0+g+ASoWY8UXaagVcXLRo+vvudhLl8z4UqZBjQJ/JL+DRpfABgRb2u241TMbUMK+iPAi8gGdq8HXk02s+cU4KoO2m02kM44Zv8tLqTq5iBrq1RSo8cvvum+La7Sra6U6W8C5dKsx796hvveHTg/5fnnAZdFxDWSbgcukfRBYC1w3gyPYzYwii5T0CqV1OjxRqUZKj1/fxMol4aBPyLOn8mOI+LHwOI6238BPH8m+zYbZEUOsrZKJTV6vJEhqW+D0dY/LadzSlog6aOSviZpVeWnF40zs+lazdev93gjI8NDDb8JuGb+3JZnHv+FwB3APsD7yWb13Fxgm8ysgVbz9asfb6byukbPc838uS3PdM5dI+I8SW+PiG8B35LkwD+H9XPan6ccttYqlVR5fJ9lX607O0MwrYxDkYPRNpjyBP4N6d8HJL0cuB/YpbgmWT/1c4FsL87dXXmmlrpmfjnlCfwflLQT8G7g34AdgXcW2irrm35eedrPY89FeaeW+orf8skT+G+KiEeAR4CjCm6P9Vk/F8j24tzd5d68NZIn8N8g6W7gUuCKiFhfbJOsn/q5QPZcWJx70MYo3Ju3evKUZX4W8E/AQcAaSddI+qvCW2Z90c/yvrO9tHC/KnOatStXWeaI+EFEvIvswquHycop2xzUz/K+s720cLMxCrNBkqcs847Aq4DXAs8kq7LpK2/nsH6mB2ZzasJjFDZb5Mnx/whYAXwgIr5fcHvMZq25MEZh5ZAn8D8josF13Wa2eUB3YnIKMb2kbfUYxaAN/Fp55anH76Bvc0o3A/A/rbiVC2+8d3OwD9gc/MdqFkHp5OI0nyysCHl6/GZzRicBuNmKVtVBvyLIql5WB+lOLk7zlcxWlLYWWzeb7dqdedNsiuZZV9/WcKWiTRHTpnJ2MvDrWUJWlMJW4DIbRHkCcHUPf55Ud+Wqd112C0+2SIJW9+g7Gfj1LCErSpErcJn1TN5ceKsAXJteaVSvvlXQr6gE6U6WZGzU1tH5wyxZvsp5f+tYYStwmfVKO7nwVgG4XnplJkbnD09rRzsDtfXaOjwkfvf7jax/Iiua67y/dSLPBVwLgPcCBwLbVrZHxNENX2TWQ+0MnLYKwN1Oo1R/Yai+OK3yDeWdl96yuQ2r73l486LoQxInH7oXHz7+4GltffwPG5mc2jDtGK5gau3KM6vnQrICbS8HTgdOAdYV2SizdrSbC292dXC7a9a28khNkIb631Bqxww2RXDBjfcC0xdN2WfZV+sex3l/a0eeWT27RsR5wIaI+FZEvBlwb98GRqMB0j1GR1ixdoIly1exz7KvsmT5qpYF09pZs7bTttX7htJozODim+5rub9m283qyRP4p63AJWkxXoHLBshRByxANdtGhoc46oAFbVfLrFcobueUp29Xo8HbdnrntYPLs72CqQ0Gr8Bls9qKtRNcvmZi2rxjASc8b4zr71zX0YpetamgFWsneMelt+Rqz5DEkxFbXOhVnacfnT+8eXA2z/5q2wZeXMVmJk/JhmvSTa/AZQOnXtokgOvvXNfxPPh6U0NHR4a3GFSt58kIfrn85dP2VZvPH54nhofEhk1Pna7mqX66Z9vheaxYOzEtsM/mCqY2GPLM6vkP6lzIlXL9Zn3VLLjv1CBY7zTSOHXTaGroCc8b4/I1Ey2netbm2uudmDY8GYyODLPdNltNO7msvufhLUpAPP7HTZ6uaV2XJ9VzTdXtbclq899fTHPM2tPsgqwn/rix7mtUOyBQpdHU0OvvXMeHjz+4ZcqnNtfe6MT0yNQGbnnfi6dtW7o4S0/V/j6ermndlmfpxcurfi4ETgTGi2+aWWtnHLM/w/OmR/LheVmBtMkGefT1T2xoOMOn2TeIpYvHGGsye2Z0ZHiL4NzuLByXabBe6KRI237A07vdELOO1fbg0/1mUxwbzfBp9JpKmYRGc/yH54mzXnnQFtvbnYXj6ZrWCy0Dv6THJD1a+QG+QnYlr1nfnb3yrmmDpAAbNgVnr7yr7jTPalMbNnHW1bdNm+d/1AELtgjU85R9S2gU9EdHhjn7Nc+tm4ppdx1hT9e0XtBsWGdlfHw8Vq92zTjb0j7LvtqwhOzI8FDbdXdGhoc2TwVtNkAMWRCvvqq2W7z4inWLpDURsUVqPs+snusi4i9bbTObiU6DXbM58Z0UW5vasIlrfvQA222T/Wk89vv6A8RQXN7d0zWtaM3q8W8LzAeeJmlnnsqk7gj4f6U11U4gn8lKU0V8YZ2c2rC5l9+oLDM0z7u7126DrFmP/63AO4A9gDU8FfgfBT5ZcLtsFms3kHeyLGFFvSJoeYgsd7+pwxOH2HLqZoWXTLRB16we/yeAT0j6+4j4t3Z3LGkv4D+B3cguADs3Ij4haReyap+LgLuBEyNifQdttwGSZ9WqRoF8JlMYO62mGcws6L/+sIUNg/hMTmRmvZDnAq4nJY1GxCRASvucHBGfbvG6jcC7I+KHknYA1ki6FngjcF1ELJe0DFiGZwnNanlXrWoUyPOsilU5qYzOHyYiS8cMpROMaLJGaJfUq8HTiOfi26DLE/jfEhGfqtyJiPWS3gI0DfwR8QDwQLr9mKQ7yMYGjgOOTE87H/gmDvx9N5OcdN5VqxrlxJutilV7UqkeyK2cYLod9GtnA40MDzWdglmrk/V1zXopzwVcQ9JTF7lLGgK2bucgkhYBi4GbgN3SSQHg12SpIOujSnBtp3xxtbw92cf/sLHuPpvNdW9nKcTRJjV42jFP2b6q2wLkruvvufg26PL0+L8OXCrps+n+W9O2XCRtD1wOvCMiHq06hxARIaluh03SacBpAAsXLsx7OOvATHPSefPsk1MbGg5yNprC2E565JGpDYw1aUslNdTsOZAVRhObNn+TWH3Pw9MKtLUarHXpZBt0LS/gkjSPLAC/MG26FvhcRDzZcufSMFmRt5UR8bG07S7gyIh4QNLuwDcjomlXyBdwFavRRVCCaSWGG6lNx7RSfeFTqxRTszIJtRrl/Oulap555teaTtWs1mgMoagLuMy6pdEFXHmKtD0ZEZ+JiFdHxKuB28kWZGl1QAHnAXdUgn5yNdm6vaR/r8rzC1hxZlofppKqyZtqqfTi86SY2lkKsTrnX/le2ahEQt6gX9lfs9/DbLbJk+ohLbd4Mlllzl8CV+R42RLgDcCtkiq1bP8RWA5cJulU4J60T+uBRr3rZoOreVVSNYsaLAZerXJCaZRiOvOKH/Puy37EpgiGJA57xs7c/sBjuVetgixYN+uRt0r35OHBWputml25+yyyYH8y8BuyufeKiFyrcEXEd9mybmKFyz30WJ6LirqRkx6qM4e/WvUJpVGPeWrDU1nETRHc8POHGe6gjmyzHnm9k10z9dJHHqy12apZj/9O4DvAsRHxMwBJXmt3lmo1gNut+jAnH7oXF9x4b93HxmpOKO1cfLWh5YjSlpr1yGtPdjuNDKNUhbNekK8u3ObBWpvtmgX+44HXAtdL+jpwCY178NZFRdR56dVFRR9cmk19vPim+6alau7+7RT3T05x9sq7ADanmN556S2FXHyVp0fe6GTnOjs21zUr2bACWCFpO7KLrt4BPF3SvwNXRsQ3etTGUimqzksRFxU1CpAfXHow43vvwtkr72Jicorv/fzhzcG99vdptZRhOyo99dpvFu1ydUyb61oO7kbE48BFwEWpXMNryK60deAvQFF1XurltAUcdcCC3PuoBPqJyakt0iHVAR2YdqzaHv3Uhk28+7IfAe0Nsu48f5j5Wz+1QPlRByxw+sWsA7lm9VSkYmrnph8rQFEpmaWLx1h9z8NceOO9mwNxAJevmWB8711aBszabyL10jOVE1TldjObIjjzils54Xlj0y6OguzK2SdrDjAyPMT7XpEtbVj5lnH9neu6Euyd2rGy6WTNXStQkWuuXn/nurq970qwbiZv6YT7J6dyn6SmNmzi+jvXbVGu4WMnHsLHTzpkixIOwBbz/t956S0sylFGoZGZlqswm43a6vFb8boxp76RmXybyBvMKyeovOmb+yenGubUa7ctWb5qi5NPo7GDvFxC2crIgX/AFFnnZSYDvHmmXlafoPLOkW907Hrpl1Ynn04CtksoWxk58A+gomaVzOTbRKsLnnaeP8z7XnHQtHY3W5gF6q9itWLtBGddfdu0Bc4rvflm6+tWP3fJ8lW5T5YuoWxl5MBfIjP5NlH92onJqWmVLuvto/rkVa+IW71VrJoVe5vasIlttpq3Ra38etpJ+xSZWjMbVA78JVP7bWLF2gmWLF+V60RQeW11GibvMaH1CafVAPIjUxs456RDtvhGUE/etI9LKFsZOfCXWCcXi3V6gVme9FWrE0kl/fKHjfnqN7RzYnKgtzLxdM4Sazajpd3XvP8rtzV8TeVbRavVq5rl1Svpl3ZW5HKe3qw+B/4SazajpVGwbvSa9U9sqBvQ25kn36j2/s7zhzfX1M/bi+9Fnj7vCc1s0DjVU2KNZrSMzh9umM5pNq2zXk69nXnyefLtDds8Msx222zVszx9UTWVzHrBgb/EjjpgwbQSDpD1lCO2LLlQCdZnHLN/w8Jq9XrjjXrojaZdtsq3N5qFc9YrD+ppwPWFXzabOdVTUivWTnD5molpQV/ACc8b45EGM2YqV9k2WmJxj9GRLdIfOzVZjrGTkguVZR5ryzn0Otj6wi+bzdzjL6l6PdYgq6PfqD5+ZbD0rFceVLfXfdQBC7ZIfwwPieF5YkNt1bWqY1aemzdVMgizcHzhl81m7vGXTKVH3ihP32jZxOrB0upeN2TLLU5t2MTFN923xclkw6Zg+2232vzcZvIWjBsE9QaifeGXzRYO/CVSPcOmHbXplNq6/JWTRaOTxuQTG7hh2dEMqfUCbrMlVTIoKSezTjjVMwc1qi/fzhz4CgE3LDt62r5b1eWvVUl/NFuEvfa5s8EgpJzMOuHAP8c0m2bYSW+6NhC3e/KoTn+0Wm2rnVSJF08x65xTPXNMs2mGjXrTjRIw9QJxnpPHkFQ3/VEvL145djupEi+eYjYz7vHPUo16vM2mGZ5z0iF1q2TWS8AMSXUDcau6/CPDQw0DeL0LtKrXza0M7LYK/p5DbzYzDvyzULN0TqOa9aPzh+sG3kZB/MmIukG00aLtAQ1LNFdrVq4575ROz6E3mxkH/lmoWY+30fhpZXvtgGSjqZ2N0kLdLGPcac/dc+jNZsaBfxZq1Etv1uNtdDVuJwuRdGs2S6c9dy+eYjYzDvyzzIq1Ew3z8s0WOu9GD77bM2k67bl78RSzmXHgn2XOXnlX3aBfvX5tET34IqpRzqTn7jn0Zp1z4J9lGqVBgukBuNu94SJm0rjnbtYfDvyzTKP0SHUtnCJ6w0XMpPFFWGb94Qu4ZplGF0EddcCCQo/bKO/e6UwaX4Rl1j8O/C0M2vJ6SxePccLzxqZdbRvA5WsmCm1bt6tRdrLer5l1hwN/E/V6pe0uHFKE6+9ct8UAb9FBs9vVKH0Rlln/FJbjl/QF4FjgoYh4dtq2C3ApsAi4GzgxItYX1YaZarRYCfR3jdV+Bc1ujh34Iiyz/imyx/9F4CU125YB10XEfsB16f7AahVIi+plt0ovdTvf3g9eyMSsfwoL/BHxbeDhms3HAeen2+cDS4s6fjfkCaTd7mXnGfTsZtDs1xiGFzIx659eT+fcLSIeSLd/DezW6ImSTgNOA1i4cGEPmralehcY1ep2LzvPfPluzX8v4qKsdvgiLLP+6Ns8/ogISQ2XZCT43KYAAAjzSURBVIqIc4FzAcbHx/Ms9NR11QG2ssxgdUOKSE3kzd93I2i6vLFZOfU68D8oafeIeEDS7sBDvTjoTC4Uqi0jXPQFR70c9PTMGrNy6nXgvxo4BVie/r2q6AN2M53Rbi+7kxNFLytPemaNWTkVNrgr6WLg+8D+kn4l6VSygP8iST8FXpjuF6qIC4XyDIh2emVqLwc9PbPGrJwK6/FHxMkNHvrLoo5ZT7fTGXm/Qcwkf96rQU8XSTMrpzlfpK3b6Yy8AX225M89s8asfOZ8yYZupzPyBvS5cJGVmc1Ncz7wdztnnjegO39uZoNqzqd6oLvpjLyzbpw/N7NBVYrA303tBPSZnHC8SImZFcWBvwNFD4j2u5SCmc1tcz7HPxt5kRIzK5ID/wCaLVNBzWx2cuAfQJ4KamZFcuAfQJ4KamZF8uDuAPJUUDMrkgP/gHIpBTMrigN/C55Pb2ZzjQN/E55Pb2ZzkQd3m/B8ejObixz4m/B8ejObixz4m/B8ejObixz4m/B8ejObizy424Tn05vZXOTA34Ln05vZXONUj5lZyTjwm5mVjAO/mVnJOPCbmZWMA7+ZWck48JuZlYwDv5lZyTjwm5mVjAO/mVnJOPCbmZWMA7+ZWck48JuZlYwDv5lZyTjwm5mVTF8Cv6SXSLpL0s8kLetHG8zMyqrngV/SEPAp4KXAgcDJkg7sdTvMzMqqHz3+5wM/i4hfRMQfgUuA4/rQDjOzUurHClxjwH1V938FHFr7JEmnAaelu3+Q9JMetG2QPQ34Tb8bMQD8PmT8Pvg9qGj2Puxdb+PALr0YEecC5wJIWh0R431uUl/5Pcj4fcj4ffB7UNHJ+9CPVM8EsFfV/T3TNjMz64F+BP6bgf0k7SNpa+C1wNV9aIeZWSn1PNUTERsl/R2wEhgCvhARt7V42bnFt2zg+T3I+H3I+H3we1DR9vugiCiiIWZmNqB85a6ZWck48JuZlcxAB/6ylnaQ9AVJD1VfuyBpF0nXSvpp+nfnfraxFyTtJel6SbdLuk3S29P20rwXkraV9ANJP0rvwfvT9n0k3ZT+Ni5NEyXmPElDktZKuibdL937IOluSbdKukXS6rStrb+JgQ38JS/t8EXgJTXblgHXRcR+wHXp/ly3EXh3RBwIHAb8bfo/UKb34g/A0RHxXOAQ4CWSDgM+ApwTEfsC64FT+9jGXno7cEfV/bK+D0dFxCFV8/fb+psY2MBPiUs7RMS3gYdrNh8HnJ9unw8s7Wmj+iAiHoiIH6bbj5H9wY9RovciMr9Ld4fTTwBHA19O2+f0e1AhaU/g5cDn031Rwvehgbb+JgY58Ncr7TDWp7YMgt0i4oF0+9fAbv1sTK9JWgQsBm6iZO9FSm/cAjwEXAv8HJiMiI3pKWX52/g48A/Ak+n+rpTzfQjgG5LWpNI20ObfxMCWbLDGIiIklWYerqTtgcuBd0TEo1lHL1OG9yIiNgGHSBoFrgQO6HOTek7SscBDEbFG0pH9bk+fvSAiJiQ9HbhW0p3VD+b5mxjkHr9LO0z3oKTdAdK/D/W5PT0haZgs6F8YEVekzaV8LyJiErgeOBwYlVTpuJXhb2MJ8EpJd5OlfY8GPkH53gciYiL9+xBZR+D5tPk3MciB36UdprsaOCXdPgW4qo9t6YmUwz0PuCMiPlb1UGneC0kLUk8fSSPAi8jGOq4HXp2eNqffA4CIODMi9oyIRWSxYFVEvJ6SvQ+StpO0Q+U28GLgJ7T5NzHQV+5KehlZXq9S2uFDfW5ST0i6GDiSrNzqg8D7gBXAZcBC4B7gxIioHQCeUyS9APgOcCtP5XX/kSzPX4r3QtJzyAbrhsg6apdFxAckPYOs57sLsBb4q4j4Q/9a2jsp1fOeiDi2bO9D+n2vTHe3Ai6KiA9J2pU2/iYGOvCbmVn3DXKqx8zMCuDAb2ZWMg78ZmYl48BvZlYyDvxmZiXjwG+zgqRNqRrhTyR9SdL8Gezri5JenW5/vlnxP0lHSvrzDo5xt6SnddrGbu/HrJoDv80WU6ka4bOBPwKnVz9YdfVmWyLibyLi9iZPORJoO/CbDTIHfpuNvgPsm3rj35F0NXB7KmZ2tqSbJf1Y0lshuwJY0ieVre3w38DTKzuS9E1J4+n2SyT9MNW+vy4VhjsdeGf6tvEX6Uray9Mxbpa0JL12V0nfSDXzPw+IGpJOl3R21f03Svpkur0iFd26rarwVvVrF2n6+gzvkXRWuv1MSV9Pr/+OpNLV8rH2uEibzSqpZ/9S4Otp058Cz46IX6aA+UhE/JmkbYAbJH2DrKrn/mTrOuwG3A58oWa/C4DPAUekfe0SEQ9L+gzwu4j4aHreRWT1378raSGwEvgTsqurv5uuqn059evCXw58Hzgj3T8JqFyN/uZ0vBHgZkmXR8Rvc74t5wKnR8RPJR0KfJqslo1ZXQ78NluMpNLEkPX4zyNLwfwgIn6Ztr8YeE4lfw/sBOwHHAFcnKpc3i9pVZ39HwZ8u7KvJpe7vxA4sKpC6I6peugRwPHptV+VtL72hRGxTtIvlC2k8lOyKps3pIffJulV6fZeqd0tA3869p8DX6pq0zatXmfl5sBvs8VURBxSvSEFuserNwF/HxEra573si62Yx5wWET8vk5b8rgEOBG4E7gyldA9kuyEcnhEPCHpm8C2Na/byPTUbOXxeWQ16Q/BLCfn+G0uWQn8r1TKGUnPShUMvw2clMYAdgeOqvPaG4EjJO2TXrtL2v4YsEPV874B/H3ljqRKwP028Lq07aVAozVPryRbLelkspMAZN9M1qegfwDZt49aDwJPT2MJ2wDHAkTEo8AvJb0mHVuSntvg2GaAA7/NLZ8ny9//MA2EfpbsW+2VZKmV24H/JMuzTxMR64DTgCsk/Qi4ND30FeBVlcFd4G3AeBo8vp2nZhe9n+zEcRtZyufeeg2MiPVkZZX3jogfpM1fB7aSdAewnOwkVPu6DcAHgB+QrcJVvfjG64FTU7tvoyRLlFrnXJ3TzKxk3OM3MysZB34zs5Jx4DczKxkHfjOzknHgNzMrGQd+M7OSceA3MyuZ/w9lyBJtO2O38gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel(\"Predicted value\")\n",
    "plt.ylabel(\"Actual value\")\n",
    "plt.title(\"Predicted vs. Actual values\")\n",
    "plt.xlim([0, 51])\n",
    "plt.ylim([0, 51])\n",
    "plt.scatter(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importance of each feature\n",
    "\n",
    "As we already know, since we standardize our features in the beginning, we can check the size of coefficients to know which one is important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.21213267],\n",
       "       [ 0.81433624],\n",
       "       [ 0.30894756],\n",
       "       [ 0.84788429],\n",
       "       [-1.78763297],\n",
       "       [ 2.87254465],\n",
       "       [-0.2811529 ],\n",
       "       [-2.9113813 ],\n",
       "       [ 2.12380982],\n",
       "       [-1.34482524],\n",
       "       [-1.90944831],\n",
       "       [ 1.14951932],\n",
       "       [-4.008822  ]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights['W']\n",
    "\n",
    "#so which one is the most important feature?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare with sklearn\n",
    "\n",
    "To make sure our code from scratch is correct, let's compare with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  21.517444231177276\n",
      "$R^2$:  0.7112260057484923\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "print(\"MSE: \", mean_squared_error(y_test, y_pred))\n",
    "print(\"$R^2$: \", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved model: Neural Network\n",
    "\n",
    "As we see from above, the score is not really nice.  One possibility is due to the linear assumption of linear regression.  We may be able to improve the accuracy if we can deploy a non-linear model.  Here we shall modify linear regression to non-linear form in the neural network.  Here we shall further clearly see that adding non-linearity is basically adding more activation functions!   In fact, you will be surprised that neural network, at its most basic form, is actually a linear regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Adding many linear regressions\n",
    "\n",
    "In one linear regression. if our X had dimensions <code>[batch_size, num_features]</code>, then we multiplied it by a weight matrix W with dimensions <code>[num_features, 1]</code> to get an output of dimension <code>[batch_size, 1]</code>; this output is, for each observation in the batch, simply a weighted sum of the original features. \n",
    "\n",
    "To do multiple linear regressions, we’ll simply multiply our input by a weight matrix with dimensions <code>[num_features, num_outputs]</code>, resulting in an output of dimensions <code>[batch_size, num_outputs]</code>; now, for each observation, we have multiple (num_outputs) different weighted sums of the original features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: A non-linear function\n",
    "\n",
    "Next, we’ll feed each of these weighted sums through a nonlinear function; the first function we’ll try is the sigmoid function\n",
    "\n",
    "Why is using this nonlinear function a good idea? Why not the square function f(x) = $x^2$, for example? There are a couple of reasons. First, we want the function we use here to be monotonic so that it “preserves” information about the numbers that were fed in. Let’s say that, given the date that was fed in, two of our linear regressions produced values of –3 and 3, respectively. Feeding these through the square function would then produce a value of 9 for each, so that any function that receives these numbers as inputs after they were fed through the square function would “lose” the information that one of them was originally –3 and the other was 3.\n",
    "\n",
    "The second reason, of course, is that the function is nonlinear; this nonlinearity will enable our neural network to model the inherently nonlinear relationship between the features and the target.\n",
    "\n",
    "Finally, the sigmoid function has the nice property that its derivative can be expressed in terms of the function itself:\n",
    "\n",
    "$$ \\frac{\\partial \\sigma}{\\partial u}(x) = \\sigma(x) * (1 - \\sigma(x)) $$\n",
    "\n",
    "#### Step 3: Add another linear regression\n",
    "\n",
    "Finally, we’ll take the resulting 13 elements—each of which is a combination of the original features, fed through the sigmoid function so that they all have values between 0 and 1—and feed them into a regular linear regression, using them the same way we used our original features previously.\n",
    "\n",
    "Then, we’ll try training the entire resulting function in the same way we trained the standard linear regression earlier in this chapter: we’ll feed data through the model, use the chain rule to figure out how much increasing the weights would increase (or decrease) the loss, and then update the weights in the direction that decreases the loss at each iteration. Over time (we hope) we’ll end up with a more accurate model than before, one that has “learned” the inherent nonlinearity of the relationship between our features and our target.”\n",
    "\n",
    "\n",
    "Finally, consider the following network:\n",
    "\n",
    "![](../figures/neuralnet2.png)\n",
    "\n",
    "Let's code them accordingly:\n",
    "\n",
    "Starting with the sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: ndarray) -> ndarray:\n",
    "    return 1 / (1 + np.exp(-1.0 * x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's start coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since our weights will depend on the size of nonlinearity,\n",
    "#we can num_outputs as hidden_size\n",
    "def init_weights(input_size: int, \n",
    "                 hidden_size: int) -> Dict[str, ndarray]:\n",
    "    weights: Dict[str, ndarray] = {}\n",
    "    weights['W1'] = np.random.randn(input_size, hidden_size)\n",
    "    weights['B1'] = np.random.randn(1, hidden_size)\n",
    "    weights['W2'] = np.random.randn(hidden_size, 1)\n",
    "    weights['B2'] = np.random.randn(1, 1)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similar as before\n",
    "def forward(X: ndarray,\n",
    "                 y: ndarray,\n",
    "                 weights: Dict[str, ndarray]\n",
    "                 ) -> Tuple[Dict[str, ndarray], float]:\n",
    "\n",
    "    M1 = X @ weights['W1']\n",
    "\n",
    "    N1 = M1 + weights['B1']\n",
    "\n",
    "    O1 = sigmoid(N1)\n",
    "    \n",
    "    M2 = O1 @ weights['W2']\n",
    "\n",
    "    P = M2 + weights['B2']    \n",
    "\n",
    "    loss = np.mean(np.power(y - P, 2))\n",
    "\n",
    "    forward_info: Dict[str, ndarray] = {}\n",
    "    forward_info['X'] = X\n",
    "    forward_info['M1'] = M1\n",
    "    forward_info['N1'] = N1\n",
    "    forward_info['O1'] = O1\n",
    "    forward_info['M2'] = M2\n",
    "    forward_info['P'] = P\n",
    "    forward_info['y'] = y\n",
    "\n",
    "    return forward_info, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the gradients, that we are interested to find is dLdW2, dLdB2, dLdW1, dLdB1 by performing the chain rule.  This can get messy but things are simply the same as above.  Also, a clean derivative of the sigmoid function is also very nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_gradients(forward_info: Dict[str, ndarray], \n",
    "                   weights: Dict[str, ndarray]) -> Dict[str, ndarray]:\n",
    " \n",
    "    #remove constant does not change the direction\n",
    "    dLdP = -(forward_info['y'] - forward_info['P'])\n",
    "    \n",
    "    dPdM2 = np.ones_like(forward_info['M2'])\n",
    "    \n",
    "    dPdB2 = np.ones_like(weights['B2'])\n",
    "\n",
    "    dLdM2 = dLdP * dPdM2\n",
    "  \n",
    "    dLdB2 = (dLdP * dPdB2).sum(axis=0)\n",
    "    \n",
    "    dM2dW2 = forward_info['O1'].T\n",
    "    \n",
    "    dM2dO1 = weights['W2'].T\n",
    "    \n",
    "    dLdW2 = dM2dW2 @ dLdP\n",
    "\n",
    "    dLdO1 = dLdM2 @ dM2dO1\n",
    "    \n",
    "    dO1dN1 = sigmoid(forward_info['N1']) * (1- sigmoid(forward_info['N1']))\n",
    "    \n",
    "    dLdN1 = dLdO1 * dO1dN1\n",
    "    \n",
    "    dN1dB1 = np.ones_like(weights['B1'])\n",
    "    \n",
    "    dN1dM1 = np.ones_like(forward_info['M1'])\n",
    "    \n",
    "    dLdB1 = (dLdN1 * dN1dB1).sum(axis=0)\n",
    "    \n",
    "    dLdM1 = dLdN1 * dN1dM1\n",
    "    \n",
    "    dM1dW1 = forward_info['X'].T \n",
    "\n",
    "    dLdW1 = dM1dW1 @ dLdM1\n",
    "\n",
    "    loss_gradients: Dict[str, ndarray] = {}\n",
    "    loss_gradients['W2'] = dLdW2\n",
    "    loss_gradients['B2'] = dLdB2.sum(axis=0)\n",
    "    loss_gradients['W1'] = dLdW1\n",
    "    loss_gradients['B1'] = dLdB1.sum(axis=0)\n",
    "    \n",
    "    return loss_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our train method looks very unchanged...except adding the hidden size parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X: ndarray, \n",
    "          y: ndarray, \n",
    "          n_iter: int = 1000,\n",
    "          learning_rate: float = 0.01,\n",
    "          batch_size: int = 100,\n",
    "          return_losses: bool = False, \n",
    "          return_weights: bool = False,\n",
    "          hidden_size = 13) -> None:\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    start = 0\n",
    "\n",
    "    #initialize randomom weights\n",
    "    weights = init_weights(X_train.shape[1], hidden_size = hidden_size)\n",
    "\n",
    "    # Permute data\n",
    "    perm = np.random.permutation(X.shape[0])\n",
    "    X, y = X[perm], y[perm]\n",
    "    \n",
    "    if return_losses:\n",
    "        losses = []\n",
    "\n",
    "    for i in range(n_iter):\n",
    "\n",
    "        # in case all data is used\n",
    "        if start >= X.shape[0]:\n",
    "            perm = np.random.permutation(X.shape[0])\n",
    "            X, y = X[perm], y[perm]\n",
    "            start = 0\n",
    "        \n",
    "        #reduce batch size if exceeds\n",
    "        if start+batch_size > X.shape[0]:\n",
    "            batch_size = X.shape[0] - start\n",
    "    \n",
    "        X_batch, y_batch = X[start:start+batch_size], y[start:start+batch_size]\n",
    "        start += batch_size\n",
    "    \n",
    "        # Train net using generated batch\n",
    "        forward_info, loss = forward(X_batch, y_batch, weights)\n",
    "\n",
    "        if return_losses:\n",
    "            losses.append(loss)\n",
    "\n",
    "        loss_grads = loss_gradients(forward_info, weights)\n",
    "        \n",
    "        #loss_grads and weights have same keys\n",
    "        for key in weights.keys():\n",
    "            weights[key] -= learning_rate * loss_grads[key]\n",
    "\n",
    "    if return_weights:\n",
    "        return losses, weights\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our network change, we also need to revise our predict method slightly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X: ndarray, \n",
    "            weights: Dict[str, ndarray]) -> ndarray:\n",
    "\n",
    "    M1 = np.dot(X, weights['W1'])\n",
    "\n",
    "    N1 = M1 + weights['B1']\n",
    "\n",
    "    O1 = sigmoid(N1)\n",
    "\n",
    "    M2 = np.dot(O1, weights['W2'])\n",
    "\n",
    "    P = M2 + weights['B2']    \n",
    "\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info = train(X_train, y_train,\n",
    "                   n_iter = 10000,\n",
    "                   learning_rate = 0.001,\n",
    "                   batch_size=23, \n",
    "                   return_losses=True, \n",
    "                   return_weights=True)\n",
    "losses = train_info[0]\n",
    "weights = train_info[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the prediction accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  10.839123211769357\n",
      "$R^2$:  0.8545339831990069\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(X_test, weights)\n",
    "print(\"MSE: \", mean_squared_error(y_test, y_pred))\n",
    "print(\"$R^2$: \", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We can learn several things here:\n",
    "1. Non-linearity in the sigmoid function helps improve the performance\n",
    "2. The math behind neural network, in fact, is much simpler than some traditional models!  It relies only the chain rule to derive the gradients of L in respect to W and B\n",
    "3. Neural network, on its simplest form, is simply a linear regression.  In simple word, neural work can be seen as simply linear combinations of X.  But if we introduce sigmoid or other functions, it become non-linear\n",
    "4. Key thing I want to focus is not on the programing side.  In fact, coding from scratch is mainly for the purpose of understanding neural network from ground up.  It is my intention that once you understand them, you can build on of your own in your research in the future.  Who knows some genius could be hiding here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
