{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where are we now\n",
    "1. Python - general problem solving\n",
    "2. Data Science - NumPy, Pandas, Sklearn, Matplotlib \n",
    "3. ML from Scratch - Intuition (so for those who want to further advance....)\n",
    "4. Signal Processing - Energy, Telecommunciations, Biosignals, Time Series\n",
    "5. Deep Learning - PyTorch\n",
    "   1. One of the most popular DL framework (against TensorFlow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning vs. Machine Learning\n",
    "\n",
    "Good News\n",
    "- Deep Learning can automatically feature engineer / feature selection\n",
    "- Deep Learning can benefit from huge amount of data, while Machine Learning cannot\n",
    "  - 100 samples vs 1000 samples, ML will get the same accuracy\n",
    "  - But DL will see increased accuracy\n",
    "- Deep Learning is basically stacking a lot of linear regression together\n",
    "  - DL can learn very complex patterns\n",
    "  - DL is perfect for (1) images, (2) text, (3) signal (very random)\n",
    "\n",
    "Bad News\n",
    "- Deep Learning sucks with small data (vs. Machine Learning) - 5000++ samples\n",
    "- For Tabular Data, Deep Learning will ALMOST ALWAYS LOSE TO gradient boosting (or its variants)\n",
    "  - Gradient Boosting is basically decision trees stacking after one another....\n",
    "  - For most competition, XGBoost and LightGBM are always the winners for tabular data\n",
    "  - If you work in a company, mostly they use tabular data, then you should look for gradient boosting types.... \n",
    "- Deep Learning has NO feature importance; so it's mostly blackbox....(Explanable AI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #pip install torch or pip3 install torch or conda install torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.23.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Tensors\n",
    "\n",
    "PyTorch don't use NumPy.  Instead, it has its own data structures, called `Tensor`, which support automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create torch tensors from NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a numpy array of 1 to 5\n",
    "arr = np.arange(1, 6)\n",
    "# arr\n",
    "\n",
    "#print the data type\n",
    "arr.dtype  #int64\n",
    "\n",
    "#print the type()\n",
    "type(arr)  #belongs to Python itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert numpy to tensor\n",
    "\n",
    "#1. from_numpy (copy)\n",
    "torch_arr_from = torch.from_numpy(arr)\n",
    "torch_arr_from.dtype  #torch.int64\n",
    "type(torch_arr_from)  #torch.Tensor\n",
    "torch_arr_from.type() #torch.LongTensor (int64); if torch.IntTensor (int32)\n",
    "                      #torch.FloatTensor (float32); if torch.DoubleTensor (float64)\n",
    "#from_numpy is a copy!!!  This is intended, for easy use between numpy and tensor...\n",
    "# arr[2] = 999\n",
    "# torch_arr_from\n",
    "\n",
    "#2. tensor (not a copy)\n",
    "torch_arr_tensor = torch.tensor(arr)  #everything is the same, except it's NOT a copy\n",
    "arr[2] = 9999999\n",
    "torch_arr_tensor\n",
    "\n",
    "#In our class, mostly we use torch.tensor; it won't fail us :-)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some API to create tensor\n",
    "\n",
    "`torch.empty(size)` - any arbitrary numbers\n",
    "`torch.ones(size)`\n",
    "`torch.zeros(size)`\n",
    "`torch.arange(start, stop(ex), step)`\n",
    "`torch.linspace(start, stop, how many)`\n",
    "`torch.logspace(start, stop, how many)`  - power of 10\n",
    "\n",
    "`torch.rand(size)` - [0, 1)\n",
    "`torch.randn(size)` - std = 1 with uniform distribution\n",
    "`torch.randint(low, high, size)` - [low, high)\n",
    "\n",
    "`torch.ones_like(input)` = `torch.ones(input.shape)`\n",
    "`torch.zeros_like(input)`\n",
    "`torch.rand_like(input)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1])\n"
     ]
    }
   ],
   "source": [
    "#import some deep learning layer\n",
    "#you have to help me create the right shape to insert to this layer\n",
    "\n",
    "import torch.nn as nn  #nn contains a lot of useful deep learning layers\n",
    "\n",
    "linear_layer = nn.Linear(5, 1)  #basically you insert 5 features, output 1 number\n",
    "linear_layer.weight  #they treat this as theta, X @ theta^T\n",
    "linear_layer.bias\n",
    "#[0.1315, 0.3990, 0.0960, 0.0807, 0.2908]\n",
    "#weight - [5, 1]\n",
    "#X @ weight\n",
    "#(anything, 5) @ (5, 1)\n",
    "\n",
    "#can you guys help me generate any pytorch tensor of size (?, ?)\n",
    "data   = torch.rand(1000, 5)\n",
    "output = linear_layer(data)\n",
    "print(output.shape)  #output shape?? - 1000, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(9999)  #this will make sure your weight is always init the same thing\n",
    "#this seed is VERY IMPORTANT for research\n",
    "#you CANNOT FORGET THIS - setting 5 different seeds is basically doing cross validation\n",
    "#please create two linear layers of size (100, 5), (5, 1)\n",
    "layer1 = nn.Linear(4,  3)\n",
    "layer2 = nn.Linear(3,  1)\n",
    "\n",
    "#try some input that pass through these two layers\n",
    "sample_size = 3  #this is VERY UNCLEAN.....\n",
    "_input = torch.rand(sample_size, 4)\n",
    "# _input = layer1(_input)\n",
    "# _input = layer2(_input)\n",
    "# _input.shape\n",
    "\n",
    "#try nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    layer1,\n",
    "    layer2\n",
    ")\n",
    "\n",
    "_input = model(_input)\n",
    "_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#changing the type\n",
    "#format: .type()\n",
    "\n",
    "x = torch.arange(1, 6)\n",
    "x.dtype\n",
    "\n",
    "x = x.type(torch.float64)  #is NOT in-place\n",
    "x.dtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape and view\n",
    "- they are very similar\n",
    "- view will create a copy, while reshape may or may not\n",
    "- view only work for contiguous array, while reshape works for both\n",
    "- contiguous array - share consecutive memory x001 x002\n",
    "- non-contiguous array - memory in different places x001 x140 x004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(10)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x.view(2, 5)\n",
    "y[0, 0] = 9999\n",
    "y.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9999,    1,    2,    3,    4,    5,    6,    7,    8,    9])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#please help me check x, does it change?\n",
    "x  #x and y shares memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  9999, 888833,      2,      3,      4,      5,      6,      7,      8,\n",
       "             9])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x.reshape(2, 5)  #can or cannot be copy #see the documentation\n",
    "z.is_contiguous()\n",
    "\n",
    "z[0, 1] = 888833\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_transpose = z.transpose(1, 0)  #(5, 2)\n",
    "z_transpose.shape\n",
    "z_transpose.is_contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14:26 - 14:35\n",
    "\n",
    "## One step back-propagation\n",
    "\n",
    "The process of using derivates to learn the weights - **gradient descent**.  But when in a deep learning form, because we can have many layers stacked together, we give a new name - **backpropagation**\n",
    "\n",
    "$$y = 2x^4 + x^3 + 3x^2 + 5x + 1$$\n",
    "\n",
    "$$\\frac{dy}{dx} = y' = 8x^3 + 3x^2 + 6x + 5$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#why pytorch is so amazing!!!\n",
    "#because pytorch automatically calculates this gradient, always available\n",
    "#that's why pytorch is very nice for deep learning\n",
    "\n",
    "#requires_grad=True, make sure we gonna let pytorch always calculate the gradient/derivatives\n",
    "#when requires_grad=True, number MUST BE FLOAT\n",
    "#try remove the 2. --> 2 --> there will be error......\n",
    "x = torch.tensor(2., requires_grad=True)\n",
    "print(x.grad)  #no derivative calculated yet (until we call .backward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(63., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = 2*x**4 + x**3 + 3*x**2 + 5*x + 1\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call backward, which gonna calculate all the gradients relevant to y\n",
    "y.backward()  #is a inplace function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(93.)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try now print x.grad  #this will be basically dy/dx at the point of x = 2\n",
    "x.grad\n",
    "\n",
    "#can you check how I get 93????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y' = 8x^3 + 3x^2 + 6x + 5$$\n",
    "\n",
    "- try put x = 2 here\n",
    "- this is the derivative/gradient/slope/rate of change at the point (2, 63)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple steps back-propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = 3x + 2$$\n",
    "$$z = 2y^2  $$\n",
    "$$o = z / 6 $$ \n",
    "\n",
    "- let's assume we have 6 elements\n",
    "\n",
    "$$\\frac{\\partial o}{\\partial x} = \\frac{\\partial o}{\\partial z} * \\frac{\\partial z}{\\partial y} * \\frac{\\partial y}{\\partial x}$$\n",
    "\n",
    "$$\\frac{\\partial o}{\\partial z} = \\frac{1}{6}$$\n",
    "\n",
    "$$\\frac{\\partial z}{\\partial y} = 4(y) = 4(3x + 2)$$\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x} = 3$$\n",
    "\n",
    "$$\\frac{\\partial o}{\\partial x} = 2(3x + 2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1., 2, 3], [3, 2, 1]], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 3*x + 2\n",
    "z = 2*y**2\n",
    "o = z.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 16., 22.],\n",
       "        [22., 16., 10.]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad  #try to find out how did we get 10, 16, 22!?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "#use the same x\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y  &= 10x - 9999 \\\\\n",
    "z  &= 5 - y \\\\\n",
    "o  &= 3z^2 \\\\\n",
    "oo & = \\frac{o}{6} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Task1: calculate all the gradients\n",
    "\n",
    "$$\\frac{\\partial oo}{\\partial x} =  \\frac{\\partial oo}{\\partial o} * \\frac{\\partial o}{\\partial z} * \\frac{\\partial z}{\\partial y} * \\frac{\\partial y}{\\partial x}$$\n",
    "\n",
    "$$\\frac{\\partial oo}{\\partial o} = \\frac{1}{6} \\quad \\frac{\\partial o}{\\partial z} = 6z   \\quad \\frac{\\partial z}{\\partial y} = -1 \\quad, \\frac{\\partial y}{\\partial x} = 10 \\quad, \\frac{\\partial o}{\\partial x} = -10z$$\n",
    "\n",
    "Task2: code and try whether it matches yours\n",
    "\n",
    "Put on the chat if you are done; you can put on the chat for task 1 first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1., 2, 3], [3, 2, 1]], requires_grad=True)\n",
    "y = 10*x - 9999\n",
    "z = 5 - y\n",
    "o = 3*z ** 2\n",
    "oo = o.mean() #we have to make it into one number....for backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "oo.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-99940., -99840., -99740.],\n",
       "        [-99740., -99840., -99940.]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-99940"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-10 * (5 - 10*1 + 9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1., 2, 3], [3, 2, 1]], requires_grad=True)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [2.]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.arange(3.).view(3, 1)\n",
    "w.shape #[3, 1]\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = x @ w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "oo = o.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "oo.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.5000, 1.0000],\n",
       "        [0.0000, 0.5000, 1.0000]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad  #how did i get these numbers??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X = [\n",
    "      X11 X12 X13\n",
    "      X21 X22 X23           \n",
    "                  ]\n",
    "\n",
    "W = [ \n",
    "      W1\n",
    "      W2\n",
    "      W3 ]\n",
    "\n",
    "O = X @ W = [  \n",
    "           X11W1 + X12W2 + X13W3\n",
    "           X21W1 + X21W2 + X23W3\n",
    "                                  ]\n",
    "\n",
    "OO = 1/2 (O)\n",
    "\n",
    "dOO/dX = dOO/dO * dO/dX"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 ('teaching_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "becc4c8e5ad229b2591d820334d85e3db0111492344629bf57f272470dce75a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
