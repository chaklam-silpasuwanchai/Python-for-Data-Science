{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science and Artificial Intelligence\n",
    "\n",
    "## 11 Unsupervised Learning - Dimensionality Reduction\n",
    "\n",
    "### Readings:\n",
    "- [GERON] Ch8\n",
    "- [VANDER] Ch5\n",
    "- [HASTIE] Ch14.5\n",
    "- https://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition\n",
    "- https://scikit-learn.org/stable/modules/manifold.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "Working directly with high-dimensional data, such as images, comes with some difficulties: It is hard to analyze, interpretation is difficult, visualization is nearly impossible, and (from a practical point of view) storage of the data vectors can be expensive. However, high-dimensional data often has properties that we can exploit. For example, high-dimensional data is often overcomplete, i.e., many dimensions are redundant and can be explained by a combination of other dimensions. \n",
    "\n",
    "Furthermore, dimensions in high-dimensional data are often correlated so that the data possesses an intrinsic lower-dimensional structure. Dimensionality reduction exploits structure and correlation and allows us to work with a more compact rep- resentation of the data, ideally without losing information. \n",
    "\n",
    "In this lecture, we will discuss principal component analysis (PCA), an algorithm for linear dimensionality reduction. PCA, proposed by Pearson (1901) and Hotelling (1933), has been around for more than 100 years and is still one of the most commonly used techniques for dimensionality reduction. It also inspires more advanced technique such as \"autoencoders\" in deep neural networks.\n",
    "\n",
    "**Problem**\n",
    "\n",
    "In PCA, we are interested in finding projection $\\tilde{x}^{(i)}$ of data points $x^{(i)}$ that are as similar to the original data points as possible, but which have a significantly lower intrinsic dimensionality.\n",
    "\n",
    "More concretely, we consider an i.i.d. dataset $X = \\{x^{(1)},\\cdots,x^{(m)}\\}, x^{(i)} \\in \\mathbb{R}^n$, with mean 0 and shape of $n\\ \\times m$ size, ($m$ = samples, $n$ = features) that possess the data covariance matrix\n",
    "\n",
    "$$S = \\frac{1}{m}\\sum\\limits_{i=1}^{m} x^{(i)} x^{(i)\\top}$$\n",
    "\n",
    "or \n",
    "\n",
    "$$S = \\frac{1}{m}XX^\\top $$\n",
    "\n",
    "\n",
    "Furthermore, we assume there exists a low dimensional compressed dimension\n",
    "\n",
    "$$z^{(i)} = B^\\top x^{(i)} \\in \\mathbb{R}^{n'}$$\n",
    "\n",
    "of $x^{(i)}$, where we define the projection matrix\n",
    "\n",
    "$$B := [b_1, \\cdots, b_{n'}] \\in \\mathbb{R}^{n \\ \\times \\ n'}$$\n",
    "\n",
    "We assume that the columns of $B$ are orthonormal so that $b_i^\\top b_j = 0$ iff $i \\neq j$ and $b_i^\\top b_i = 1$. We seek an $n'$-dimensional subspace $U \\subseteq \\mathbb{R}^{n}, \\dim(U) = n' < n$ onto which we project the data.  We denote the projected data by $\\tilde{x}^{(i)} \\in U$ and their coordinates (with respect to the basis vectors $b_1, \\cdots, b_{n'}$ of $U$) by $z^{(i)}$.  Our aim is to find projections $\\tilde{x}^{(i)} \\in \\mathbb{R}^{n'}$ so that they are as similar to the original data $x^{(i)}$ and minimize the loss due to compression.\n",
    "\n",
    "**Maximum Variance**\n",
    "\n",
    "PCA is a dimensionality reduction algorithm that maximizes the variance in the low-dimensional representation of the data to retain as much information as possible.  Our aim is to find a matrix $B$ that retains a smuch as possible when compressing data by projecting it on the subspace spanned by the columns $b_1, \\cdots, b_{n'}$ of $B$\n",
    "\n",
    "For the data covariance matrix, we assumed centered data.  We can make this assumpotion without loss of generality: Let us assume $\\mu$ is the mean of the data.  Using the properties of the variance, \n",
    "\n",
    "$$\\mathbb{V}_x \\pm c = \\mathbb{V}_x$$\n",
    "\n",
    "where $c$ is constant; we obtain\n",
    "\n",
    "$$\\mathbb{V}_z[z] = \\mathbb{V}_x[B^\\top (x - \\mu)] = \\mathbb{V}_x[B^\\top x - B^\\top \\mu)] = \\mathbb{V}_x[B^\\top x]$$\n",
    "\n",
    "That is, the variance of the low-dimensional code does not depend on the mean of the data.  Therefore, we assume without loss of generality that the data has mean 0.  This step is actually optional but will make our math easier later on, especially when we calculate the covariance matrix.\n",
    "\n",
    "We use sequential approach to maximize the variance of the low-dimensional code.  We start by seeking a single vector $b_1 \\in \\mathbb{R}^{n}$ that maximizes the variance of the projected data, i.e., we aim to maximize the variance of the first coordinate $z^{(1)}$ of $z \\in \\mathbb{R}^{n'}$\n",
    "so that\n",
    "\n",
    "$$V_1 := \\mathbb{V}[z^{(1)}] = \\frac{1}{m}\\sum\\limits_{i=1}^{m}\\big(z^{(i)}_1\\big)^2 \\tag{A}$$\n",
    "\n",
    "is maximized.  Note that the first component of $z^{(i)}$ is given by\n",
    "\n",
    "$$z^{(i)}_1 = b_1^\\top x^{(i)} \\tag{B}$$\n",
    "\n",
    "i.e., it is the coordinate of the orthogonal projection of $x^{(i)}$ onto the one-dimensional subspace spanned by $b_1$.  We subsitute (B) into (A) yields:\n",
    "\n",
    "$$V_1 = \\frac{1}{m}\\sum\\limits_{i=1}^{m}\\big( b_1^\\top x^{(i)}\\big)^2 = \\frac{1}{m}\\sum\\limits_{i=1}^{m}b_1^\\top x^{(i)}x^{(i)\\top}b_1 \\tag{C}$$\n",
    "\n",
    "$$ =b^\\top_1\\big(\\frac{1}{m}\\sum\\limits_{i=1}^{m}x^{(i)}x^{(i)\\top}\\big)b_1 = b_1^\\top Sb_1 $$\n",
    "\n",
    "where $S$ is the data covariance matrix defined earlier.  Note that in (C), we have used the fact that the dot product of two vectors is symmetric, i.e., $b_1^\\top x^{(i)} = x^{(i)\\top}b_1$.\n",
    "\n",
    "Notice that arbitrarily increasing the magnitude of the vector $b_1$ increases $V_1$, that is, a vector $b_1$ that is two times longer can result in $V_1$ that is potentially four times larger. Therefore, we restrict all solutions to $\\lVert b_1\\rVert^2 = 1$, which results in a constrained optimization problem in which we seek the direction along which the data varies most.\n",
    "\n",
    "This can be written into the constrained optimization problem:\n",
    "\n",
    "$$\n",
    "\\max_{b_1} b_1^\\top Sb_1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{subject to} \\ \\lVert b_1\\rVert^2 = 1\n",
    "$$\n",
    "\n",
    "To solve this, as usual, we use Lagrangian $\\mathscr{L}$\n",
    "\n",
    "$$\\mathscr{L}(b_1, \\beta) = b_1^\\top Sb_1 + \\beta_1(1 - b^\\top_1b_1)$$\n",
    "\n",
    "The partial derivatives of $\\mathscr{L}$ with respect to $b_1$ and $\\beta_1$ are\n",
    "\n",
    "$$\\frac{\\partial \\mathscr{L}}{\\partial b_1} = 2b_1^\\top S - 2\\beta_1b_1^\\top$$\n",
    "\n",
    "$$\\frac{\\partial \\mathscr{L}}{\\partial \\beta_1} = 1 - b_1^\\top b_1 $$\n",
    "\n",
    "Setting these partial derivatives to 0 gives us the relations\n",
    "\n",
    "$$Sb_1 = \\beta_1b_1 \\tag{D}$$\n",
    "$$b^\\top_1b_1 = 1$$\n",
    "\n",
    "Looking at this, we see that $b_1$ is an eigenvector of the data covariance matrix $S$, and the Lagrange multiplier $\\beta_1$ plays the role of the corresponding eigenvalue.\n",
    "\n",
    "Following this eigenvector property (D), we can rewrite our variance objective in (C) as\n",
    "\n",
    "$$V_1 = b^\\top_1Sb_1 = \\beta_1b^\\top_1b_1 = \\beta_1$$\n",
    "\n",
    "i.e., the variance of the data projected onto a one-dimensional subspace equals the eigenvalue that is associated with the basis vector $b_1$ that spans this subspace.  Therefore, to maximize the variance of the low-dimensional code, we choose the basis vector associated with the largest eigenvalue of the data covariance matrix. This eigenvector is called the first *principal component*. We can determine the effect/contribution of the principal component $b_1$ in the original data space by mapping the coordinate $z^{(i)}_1$ back into data space, which gives us the projected data point\n",
    "\n",
    "$$\\tilde{x}^{(i)} = b_1z^{(i)}_1 = b_1b_1^\\top x^{(i)} \\in \\mathbb{R}^n$$\n",
    "\n",
    "in the original data space.  Although $\\tilde{x}^{(i)}$ is a $n$-dimensional vector, it only requires a single coordinate $z^{(i)}_1$ to represent it with respect to the basis vector $b_1 \\in \\mathbb{R}^n$\n",
    "\n",
    "When we consider $n'$-dimensional subspace, for the $j$th principal component where $j = 1, \\cdots, n'$, the variance is\n",
    "\n",
    "$$V_j = b^\\top_jSb_j = \\beta_jb^\\top_jb_j = \\beta_j$$\n",
    "\n",
    "This means that the variance of the data, when projected onto an $n'$-dimensional subspace, equals the sum of the eigenvalues that are associated with the corresponding eigenvectors of the data covariance matrix.\n",
    "\n",
    "Overall, to find an $n'$-dimensional subspace of $\\mathbb{R}^D$ that retains as much information as possible, PCA tells us to choose the columns of the matrix B as the $n'$ eigenvectors of the data covariance matrix $S$ that are associated with the $n'$ largest eigenvalues. The maximum amount of variance PCA can capture with the first $n'$ principal components is\n",
    "\n",
    "$$V_{n'} = \\sum\\limits_{j=1}^{n'} \\beta_j$$\n",
    "\n",
    "where the $\\beta_j$ are the $n'$ largest eigenvalues of the data covariance matrix $S$.  Consequently, the variance lost by data compression via PCA is\n",
    "\n",
    "$$J_{n'} := \\sum\\limits_{j=n'+1}^{n} \\beta_j = V_n - V_{n'}$$\n",
    "\n",
    "Instead of these absolute quantities, it is more useful to define the relative variance ratio as\n",
    "\n",
    "$$J_{n'} = 1 - \\frac{V_{n'}}{V_n} $$\n",
    "\n",
    "**Singular Value Decomposition (SVD)**\n",
    "\n",
    "In the previous sections, we obtained the basis of the principal subspace as the eigenvectors that are associated with the largest eigenvalues of the data covariance matrix\n",
    "\n",
    "$$S = \\frac{1}{m}\\sum\\limits_{i=1}^mx^{(i)}x^{(i)\\top} = \\frac{1}{m}XX^T$$\n",
    "\n",
    "$$X = [x^{(1)}, \\cdots, x^{(m)}] \\in \\mathbb{R}^{n \\ \\times m}$$\n",
    "\n",
    "Note that our X is a transpose of the typical data matrix.  To get the eigenvalues and eigenvectors of $S$, we can perform singular value decomposition as follows, where SVD of $X$ is given by\n",
    "\n",
    "$$\\underbrace{X}_{n \\ \\times \\ m} = \\underbrace{U}_{n \\ \\times \\ n} \\ \\ \\underbrace{\\Sigma}_{n \\ \\times \\ m} \\ \\ \\underbrace{V^\\top}_{m \\ \\times \\ m}$$\n",
    "\n",
    "where $U$ and $V$ are orthogonal matrices and $\\Sigma$ is a matrix whose only nonzero entries are the singular values $\\sigma_{ii} \\geq 0$\n",
    "\n",
    "It then follows that\n",
    "\n",
    "$$S = \\frac{1}{m}XX^\\top = \\frac{1}{m}U\\Sigma \\underbrace{V^\\top V}_I \\Sigma^\\top U^\\top = \\frac{1}{m}U\\Sigma\\Sigma^\\top U^\\top$$\n",
    "\n",
    "Following \n",
    "\n",
    "$$S = \\frac{1}{m}XX^\\top U = \\frac{1}{m}U\\Sigma\\Sigma^\\top$$\n",
    "\n",
    "We get that the columns of $U$ are the eigenvectors of $XX^\\top$ and therefore $S$.  Furthermore, the eigenvalues $\\beta_n$ of $S$ are related to the singular values of $X$ via\n",
    "\n",
    "$$\\beta_n = \\frac{\\sigma_n^2}{m}$$\n",
    "\n",
    "This relationship between the eigenvalues of $S$ and the singular values of $X$ provides the connection between the maximum variance view and the singular value decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.axis('equal')\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Eigenvalue shape:  (2,)\n",
      "Eigenvectors shape:  (2, 2)\n",
      "eigen:  [0.7625315]\n",
      "sum:  0.7625315008826117\n",
      "np.cumsum:  [0.7625315]\n",
      "eigenvector shape:  (2, 1)\n",
      "Variance explained_ratio:  [1.]\n",
      "Eigenvalues:  [0.7625315]\n",
      "Eigenvectors (column-wise):  [[-0.94446029]\n",
      " [-0.32862557]]\n",
      "Mean:  [ 0.03351168 -0.00408072]\n",
      "Old X shape:  (200, 2)\n",
      "Projected X shape:  (200, 1)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b58f9e5d6da8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m#you can see that lots of data in the second components is gone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojected_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojected_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"First component\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Second component\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "\n",
    "def PCA(X, n_components=1):\n",
    "    #1. center the data (center the data, not features, thus axis=0)\n",
    "    mean = np.mean(X, axis=0)\n",
    "    X = (X - mean)\n",
    "    \n",
    "    print(n_components)\n",
    "\n",
    "    #2. find covariance\n",
    "    #the reason we can X.T @ X right away since we already \n",
    "    #center the data)\n",
    "    #did not really use this variable\n",
    "    cov = (X.T @ X) / (X.shape[0] - 1)\n",
    "    \n",
    "    #3. Then simply find the eigenvalues using np.linalg.eig \n",
    "    #by inputting the covariance matrix\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov)\n",
    "    \n",
    "    print(\"Eigenvalue shape: \", eigenvalues.shape) #(n, )\n",
    "    print(\"Eigenvectors shape: \", eigenvectors.shape) #(n, n)\n",
    "\n",
    "    #4. Your n'th principal components will be your first n'th eigenvectors with highest eigenvalues\n",
    "    ix = np.argsort(eigenvalues)[::-1] #sort them from biggest to smallest thus -1\n",
    "\n",
    "    if(n_components > eigenvalues.shape[0]):\n",
    "        raise Exception(\"You cannot have n_components more than number of features!\")\n",
    "    else:\n",
    "        eigenvalues, eigenvectors = eigenvalues[ix], eigenvectors[:, ix]\n",
    "        #get the first nth components\n",
    "        eigenvalues, eigenvectors = eigenvalues[:n_components], eigenvectors[:,:n_components]\n",
    "\n",
    "    print(\"eigen: \", eigenvalues)\n",
    "    print(\"sum: \", np.sum(eigenvalues))\n",
    "    print(\"np.cumsum: \", np.cumsum(eigenvalues))\n",
    "    print(\"eigenvector shape: \", eigenvectors.shape)\n",
    "    #define how much variance is gained after n' component\n",
    "    variance_explained_ratio = np.cumsum(eigenvalues)/np.sum(eigenvalues)\n",
    "\n",
    "    #projected new vector\n",
    "    #during projection, eigenvectors mush be arranged in columns\n",
    "    projected_X = X @ eigenvectors\n",
    "    \n",
    "    #print all info\n",
    "    print(\"Variance explained_ratio: \", variance_explained_ratio) #first component got 97%\n",
    "    print(\"Eigenvalues: \", eigenvalues)\n",
    "    print(\"Eigenvectors (column-wise): \", eigenvectors)\n",
    "    print(\"Mean: \", mean)\n",
    "    print(\"Old X shape: \", X.shape)\n",
    "    print(\"Projected X shape: \", projected_X.shape)\n",
    "    \n",
    "    #return only n_components eigenvalues and vectors\n",
    "    return eigenvalues, eigenvectors, mean, projected_X\n",
    "\n",
    "#copy code from the lectures to plot the eigen values and vectors\n",
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()  #get current axis\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "#main code\n",
    "eigenvalues, eigenvectors, mean, projected_X = PCA(X, n_components=1)\n",
    "\n",
    "#looking at the original graph with the loadings\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "\n",
    "#.T so each row is eigenvector, so we can loop\n",
    "for length, vector in zip(eigenvalues, eigenvectors.T):\n",
    "    #loadings = eigenvector * (unit size) * sqrt (eigenvalue)\n",
    "    v = vector * 3 *  np.sqrt(length)\n",
    "    draw_vector(mean, mean + v)\n",
    "plt.axis('equal')\n",
    "plt.title('PCA with original data')\n",
    "\n",
    "#projection\n",
    "#you can see that lots of data in the second components is gone\n",
    "plt.figure()\n",
    "plt.scatter(projected_X[:, 0], projected_X[:, 1])\n",
    "plt.xlabel(\"First component\")\n",
    "plt.ylabel(\"Second component\")\n",
    "plt.title(\"Projected X\")\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn\n",
    "\n",
    "Principal component analysis is a fast and flexible unsupervised method for dimensionality reduction in data.\n",
    "\n",
    "Consider the following 200 points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.axis('equal');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By eye, it is clear that there is a nearly linear relationship between the x and y variables.\n",
    "This is reminiscent of the linear regression data we explored in Linear Regression, but the problem setting here is slightly different: rather than attempting to *predict* the y values from the x values, the unsupervised learning problem attempts to learn about the *relationship* between the x and y values.\n",
    "\n",
    "In principal component analysis, this relationship is quantified by finding a list of the *principal axes* in the data, and using those axes to describe the dataset.\n",
    "Using Scikit-Learn's ``PCA`` estimator, we can compute this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The* fit learns some quantities from the data, most importantly the \"components\" and \"explained variance\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Eigenvectors: \", pca.components_)\n",
    "print(\"Eigenvalues: \", pca.explained_variance_)\n",
    "print(\"Mean: \", pca.mean_) #center point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what these numbers mean, let's visualize them as vectors over the input data, using the \"components\" to define the direction of the vector, and the \"explained variance\" to define the squared-length of the vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()  #get current axis\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "# plot data\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    #loadings = eigenvector * (unit size) * sqrt (eigenvalue)\n",
    "    v = vector * 3 *  np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These vectors represent the *principal axes* of the data, and the length of the vector is an indication of how \"important\" that axis is in describing the distribution of the datamore precisely, it is a measure of the variance of the data when projected onto that axis.\n",
    "The projection of each data point onto the principal axes are the \"principal components\" of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA as dimensionality reduction\n",
    "\n",
    "Using PCA for dimensionality reduction involves zeroing out one or more of the smallest principal components, resulting in a lower-dimensional projection of the data that preserves the maximal data variance.\n",
    "\n",
    "Here is an example of using PCA as a dimensionality reduction transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)  #you want to transform to 1D hence n_components =1\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformed data has been reduced to a single dimension.\n",
    "To understand the effect of this dimensionality reduction, we can perform the inverse transform of this reduced data and plot it along with the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.zeros(X_pca.shape[0])\n",
    "plt.scatter(X_pca[:, 0], y, alpha=0.3)  #pca data\n",
    "\n",
    "X_new = pca.inverse_transform(X_pca)  #inverse transform back\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)  #original data\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The light points are the original data, while the dark points are the projected version.\n",
    "This makes clear what a PCA dimensionality reduction means: the information along the least important principal axis or axes is removed, leaving only the component(s) of the data with the highest variance.\n",
    "The fraction of variance that is cut out (proportional to the spread of points about the line formed in this figure) is roughly a measure of how much \"information\" is discarded in this reduction of dimensionality.\n",
    "\n",
    "This reduced-dimension dataset is in some senses \"good enough\" to encode the most important relationships between the points: despite reducing the dimension of the data by 50%, the overall relationship between the data points are mostly preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case Study: PCA on Hand-written digits\n",
    "\n",
    "The usefulness of the dimensionality reduction may not be entirely apparent in only two dimensions, but becomes much more clear when looking at high-dimensional data.\n",
    "\n",
    "We start by loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall** that the data consists of 88 pixel images, meaning that they are 64-dimensional.\n",
    "To gain some intuition into the relationships between these points, we can use PCA to project them to a more manageable number of dimensions, say two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(2)  # project from 64 to 2 dimensions\n",
    "projected = pca.fit_transform(digits.data)\n",
    "print(digits.data.shape)\n",
    "print(projected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(projected[:, 0], projected[:, 1],\n",
    "            c=digits.target, alpha=0.5)\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do the components mean?\n",
    "\n",
    "We can go a bit further here, and begin to ask what the reduced dimensions *mean*.\n",
    "This meaning can be understood in terms of combinations of basis vectors.\n",
    "For example, each image in the training set is defined by a collection of 64 pixel values, which we will call the vector $x$:\n",
    "\n",
    "$$\n",
    "x = [x_1, x_2, x_3 \\cdots x_{64}]\n",
    "$$\n",
    "\n",
    "One way we can think about this is in terms of a pixel basis.\n",
    "That is, to construct the image, we multiply each element of the vector by the pixel it describes, and then add the results together to build the image:\n",
    "\n",
    "$$\n",
    "{\\rm image}(x) = x_1 \\cdot{\\rm (pixel~1)} + x_2 \\cdot{\\rm (pixel~2)} + x_3 \\cdot{\\rm (pixel~3)} \\cdots x_{64} \\cdot{\\rm (pixel~64)}\n",
    "$$\n",
    "\n",
    "One way we might imagine reducing the dimension of this data is to zero out all but a few of these basis vectors.\n",
    "For example, if we use only the first eight pixels, we get an eight-dimensional projection of the data, but it is not very reflective of the whole image: we've thrown out nearly 90% of the pixels!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/05.09-digits-pixel-components.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The upper row of panels shows the individual pixels, and the lower row shows the cumulative contribution of these pixels to the construction of the image.\n",
    "Using only eight of the pixel-basis components, we can only construct a small portion of the 64-pixel image.\n",
    "Were we to continue this sequence and use all 64 pixels, we would recover the original image.\n",
    "\n",
    "But the pixel-wise representation is not the only choice of basis. We can also use other basis functions, which each contain some pre-defined contribution from each pixel, and write something like\n",
    "\n",
    "$$\n",
    "image(x) = {\\rm mean} + x_1 \\cdot{\\rm (basis~1)} + x_2 \\cdot{\\rm (basis~2)} + x_3 \\cdot{\\rm (basis~3)} \\cdots\n",
    "$$\n",
    "\n",
    "PCA can be thought of as a process of choosing optimal basis functions, such that adding together just the first few of them is enough to suitably reconstruct the bulk of the elements in the dataset.\n",
    "The principal components, which act as the low-dimensional representation of our data, are simply the coefficients that multiply each of the elements in this series.\n",
    "This figure shows a similar depiction of reconstructing this digit using the mean plus the first eight PCA basis functions:\n",
    "\n",
    "![](figures/05.09-digits-pca-components.png)\n",
    "\n",
    "Unlike the pixel basis, the PCA basis allows us to recover the salient features of the input image with just a mean plus eight components!\n",
    "The amount of each pixel in each component is the corollary of the orientation of the vector in our two-dimensional example.\n",
    "This is the sense in which PCA provides a low-dimensional representation of the data: it discovers a set of basis functions that are more efficient than the native pixel-basis of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many components?\n",
    "\n",
    "A vital part of using PCA in practice is the ability to estimate how many components are needed to describe the data.\n",
    "This can be determined by looking at the cumulative *explained variance ratio* as a function of the number of components:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(digits.data)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This curve quantifies how much of the total, 64-dimensional variance is contained within the first $N$ components.\n",
    "For example, we see that with the digits the first 10 components contain approximately 75% of the variance, while you need around 50 components to describe close to 100% of the variance.\n",
    "\n",
    "Here we see that our two-dimensional projection loses a lot of information (as measured by the explained variance) and that we'd need about 20 components to retain 90% of the variance.  Looking at this plot for a high-dimensional dataset can help you understand the level of redundancy present in multiple observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to / not to use PCA?\n",
    "\n",
    "Because of the versatility and interpretability of PCA, it has been shown to be effective in a wide variety of contexts and disciplines.\n",
    "Given any high-dimensional dataset, we tend to start with PCA in order to visualize the relationship between points (as we did with the digits), to understand the main variance in the data (as we did with the eigenfaces), and to understand the intrinsic dimensionality (by plotting the explained variance ratio).\n",
    "Certainly PCA is not useful for every high-dimensional dataset, but it offers a straightforward and efficient path to gaining insight into high-dimensional data.\n",
    "\n",
    "PCA's **two main weakness** is that it tends to be highly affected by outliers in the data, and its limitation to linear data.  For this reason, many robust variants of PCA have been developed, many of which act to iteratively discard data points that are poorly described by the initial components.\n",
    "Scikit-Learn contains a couple interesting variants on PCA, including ``IncrementalPCA`` and ``SparsePCA``, both also in the ``sklearn.decomposition`` submodule.\n",
    "``IncrementalPCA`` uses a mini-batch method to approximate principal components in very high-dimensional data, while ``SparsePCA`` introduces a regularization term that serves to enforce sparsity of the components and often gives a more interpretable model. ``KernelPCA`` is also useful as a variants of PCA for non-linear dimensionality reduction, through the use of kernels (similar to how svm transform the space)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While PCA is flexible, fast, and easily interpretable, it does not perform well in non-linear situation.  Of course, we got kernel PCA in which it first transform the data space to another dimension in which it can find a linearly separable hyperplane.  \n",
    "\n",
    "Here we gonna describe another class of methods known as manifold learning.  Manifold means a surface of any shape, it can be a curve, a folded sheet, etc.  Thus manifold learning is the idea of understanding the \"shape\" of the data and thus using this information fo cluster the data.  You can imagine paper twirling into very weird shape but it is actually just a 2d paper.  Similarly, Manifold Learning words towards extracting the low-dimensional manifold information that can be used to describe the high dimensional data.\n",
    "\n",
    "As you may have guess, manifold learning has many things to do with distances between neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/swissroll.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manifold Learning: Swiss roll\n",
    "\n",
    "As an example of using manifold learning for visualization, let's take a look at the swiss roll dataset which is a typical dataset to look at for manifold learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold, datasets\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "#elevation and #azimuth\n",
    "ax.view_init(5, -80)\n",
    "\n",
    "X, color = datasets.make_swiss_roll(n_samples=1500, noise=0.3)\n",
    "\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=\"rainbow\",\n",
    "          edgecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at data like this, we can see that the particular choice of *x* and *y* values of the dataset are not the most fundamental description of the data: we can scale, shrink, or rotate the data, and the swiss roll will still be apparent.\n",
    "\n",
    "This tells us that the *x* and *y* values are not necessarily fundamental to the relationships in the data.\n",
    "What *is* fundamental, in this case, is the *distance* between each point and the other points in the dataset.\n",
    "A common way to represent this is to use a distance matrix: for $N$ points, we construct an $N \\times N$ array such that entry $(i, j)$ contains the distance between point $i$ and point $j$.\n",
    "Let's use Scikit-Learn's efficient ``pairwise_distances`` function to do this for our original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "D = pairwise_distances(X)\n",
    "print(D.shape)\n",
    "print(\"Distance between first point and others: \", D[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is interesting is that if we rotate our swissroll, D remains the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://mathworld.wolfram.com/RotationMatrix.html\n",
    "#matrix rotation can be a bit confusing\n",
    "#if we want to rotate around x, x is kept the same\n",
    "def x_rotation(vector,theta):\n",
    "    \"\"\"Rotates 3-D vector around x-axis\"\"\"\n",
    "    R = np.array([[1,0,0],[0,np.cos(theta),-np.sin(theta)],[0, np.sin(theta), np.cos(theta)]])\n",
    "    return np.dot(vector, R)\n",
    "\n",
    "def y_rotation(vector,theta):\n",
    "    \"\"\"Rotates 3-D vector around y-axis\"\"\"\n",
    "    R = np.array([[np.cos(theta),0,np.sin(theta)],[0,1,0],[-np.sin(theta), 0, np.cos(theta)]])\n",
    "    return np.dot(vector, R)\n",
    "\n",
    "def z_rotation(vector,theta):\n",
    "    \"\"\"Rotates 3-D vector around z-axis\"\"\"\n",
    "    R = np.array([[np.cos(theta), -np.sin(theta),0],[np.sin(theta), np.cos(theta),0],[0,0,1]])\n",
    "    return np.dot(vector, R)\n",
    "\n",
    "X_rotate = x_rotation(X, 20)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X_rotate[:, 0], X_rotate[:, 1], c=color, cmap=\"rainbow\",\n",
    "          edgecolor='k')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D2 = pairwise_distances(X_rotate)\n",
    "np.allclose(D, D2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This distance matrix gives us a representation of our data that is invariant to rotations and translations.  However, while computing this distance matrix from the (x, y) coordinates is straightforward, transforming the distances back into *x* and *y* coordinates is rather difficult.  \n",
    "\n",
    "Here, let's introduce the first manifold learning algorithm - the Multidimensional Scaling (MDS): given a distance matrix between points, it recovers a $D$-dimensional coordinate representation of the data.\n",
    "Let's see how it works for our distance matrix, using the ``precomputed`` dissimilarity to specify that we are passing a distance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "#dissimilarity='precomputed' since we already calculate the distance\n",
    "#if not, use euclidean\n",
    "model = MDS(n_components=3, dissimilarity='precomputed', random_state=1)\n",
    "out = model.fit_transform(D)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(out[:, 0], out[:, 1], out[:, 2], c=color, cmap=\"rainbow\",\n",
    "          edgecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now ask the ``MDS`` estimator to input this three-dimensional data, compute the distance matrix, and then determine the optimal two-dimensional embedding for this distance matrix.\n",
    "The result recovers a representation of the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MDS(n_components=2, random_state=1)\n",
    "out2 = model.fit_transform(out)\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.scatter(out[:, 0], out[:, 1], c=color, cmap=\"rainbow\",\n",
    "          edgecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is essentially the goal of a manifold learning estimator: given high-dimensional embedded data, it seeks a low-dimensional representation of the data that preserves certain relationships within the data.\n",
    "In the case of MDS, the quantity preserved is the distance between every pair of points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nonlinear Embeddings: Where MDS Fails\n",
    "\n",
    "Our discussion thus far has considered *linear* embeddings, which essentially consist of rotations, translations, and scalings of data into higher-dimensional spaces.\n",
    "Where MDS breaks down is when the embedding is nonlinearthat is, when it goes beyond this simple set of operations.\n",
    "\n",
    "In non-linear embeddings, you may want to consider:\n",
    "\n",
    "- For toy problems such as the swiss roll we saw before, locally linear embedding (LLE) and its variants (especially *modified LLE*), perform very well. This is implemented in ``sklearn.manifold.LocallyLinearEmbedding``.\n",
    "- For high-dimensional data from real-world sources, LLE often produces poor results, and isometric mapping (IsoMap) seems to generally lead to more meaningful embeddings. This is implemented in ``sklearn.manifold.Isomap``\n",
    "- For data that is highly clustered, *t-distributed stochastic neighbor embedding* (t-SNE) seems to work very well, though can be very slow compared to other methods. This is implemented in ``sklearn.manifold.TSNE``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LocallyLinearEmbedding\n",
    "\n",
    "Provided there is sufficient data, we expect each data point and its neighbors to lie on or close to a locally linear patch.\n",
    "\n",
    "Comprises of the following steps:\n",
    "1. Find the k nearest neighbors\n",
    "2. Do a weighted aggregation of the neighbours of each point to construct a new point, in which the cost function is to minimize the distance to neighbor points\n",
    "\n",
    "Disadvantages:\n",
    "1. Scale poorly to large datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "model = LocallyLinearEmbedding(n_neighbors=500, n_components=2)\n",
    "out = model.fit_transform(X)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(out[:, 0], out[:, 1], c=color, cmap=\"rainbow\",\n",
    "          edgecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isomap\n",
    "\n",
    "One of the earliest approaches to manifold learning is the Isomap algorithm, short for Isometric Mapping. Isomap can be viewed as an extension of Multi-dimensional Scaling (MDS) or Kernel PCA. Isomap seeks a lower-dimensional embedding which maintains geodesic distances between all points. Isomap can be performed with the object Isomap.\n",
    "\n",
    "The basic steps are\n",
    "1. Use euclidean metrics to prepare a graph distance\n",
    "2. Compute all-pairs shortest paths which output the geodesic distance on the above neighborhood graph.\n",
    "3. Run multidimensional scaling using the matrix of shortest-path distances.\n",
    "\n",
    "The disadvantage:\n",
    "- Creating neighorhood graph can be tricky and if the input data is not well sampled (e.g., contain holes), then Isomap may not work well.\n",
    "- Computaionally expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/geo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "model = Isomap(n_neighbors=500, n_components=2)\n",
    "out = model.fit_transform(X)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(out[:, 0], out[:, 1], c=color, cmap=\"rainbow\",\n",
    "          edgecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t distribution-Stochastic neighborhood embedding\n",
    "\n",
    "tsne is about giving you clusters or local structure, and mostly used for visualization purpose.\n",
    "\n",
    "While Isomap, LLE and variants are best suited to unfold a single continuous low dimensional manifold, t-SNE will focus on the local structure of the data and will tend to extract clustered local groups of samples.  This tend to be useful if we want to classify things that contain multiple manifolds such as digits.\n",
    "\n",
    "Comprises of these steps:\n",
    "1. Measure similarity between two data points (e.g., distances)\n",
    "2. Convert this distance to probability according to normal distribution\n",
    "3. Now, it simply map the data points to a lower dimensional space\n",
    "4. Calculate the similarity matrix (similar to two) but with t-distribution.  It uses t-distribution to avoid crowding problem, where points tend to be crowded in low-dimensional space\n",
    "5. Using optimization algorithms (e.g., Descent algorithms with Kullback Leibler Divergence), it aims to reduce the difference between similarity matrix of 2 and 4\n",
    "\n",
    "The disadvantages to using t-SNE are roughly:\n",
    "\n",
    "- t-SNE is computationally expensive, and can take several hours on million-sample datasets where PCA will finish in seconds or minutes\n",
    "- The algorithm is stochastic and multiple restarts with different seeds can yield different embeddings. However, it is perfectly legitimate to pick the embedding with the least error.\n",
    "- Also the solution is not convex, thus descent algorithms can stuck in local minima.  We may want to initialize different random points and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "model = TSNE(n_components=2, perplexity=100)\n",
    "out = model.fit_transform(X)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(out[:, 0], out[:, 1], c=color, cmap=\"rainbow\",\n",
    "          edgecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So what next about Manifold Learning\n",
    "\n",
    "Though this story and motivation is compelling, in practice manifold learning techniques tend to be finicky enough that they are rarely used for anything more than simple qualitative visualization of high-dimensional data.\n",
    "\n",
    "The following are some of the particular challenges of manifold learning, which all contrast poorly with PCA:\n",
    "\n",
    "- No mechanism for handling missing data.  PCA does.\n",
    "- Sensitive to noise.  PCA filters noise from most important components\n",
    "- Choosing optimal number of neighbors or other hyperparameters.  PCA does not need to\n",
    "- PCA has variance_ratio to help you determine to optimal dimension to be reduced. Manifold does not\n",
    "- PCA has high interpretability.  By looking at the magnitude of eigenvectors, we can gauge which features are impactful, whereas manifold learning is not always clear\n",
    "- In manifold learning the computational expense of manifold methods scales as O[N^2] or O[N^3]. For PCA, there exist randomized approaches that are generally much faster \n",
    "\n",
    "With all that on the table, the only clear advantage of manifold learning methods over PCA is their ability to preserve nonlinear relationships in the data.  If for visualization purpose, then manifold learning is ok.  But for dimensionality reduction, we can always go for **Kernel PCA**, which is much faster!!\n",
    "\n",
    "A heads up - in Artifical Neural Network (and Deep Learning), we will explore a concept called **Autoencoders** which is basically Dimensionality Reduction.  As you can already imagine, by adjusting the weights and assign an activation function, we can create a dimensionality reduction that aims to reduce unimportant features, even in non-linear space!\n",
    "\n",
    "Last, I will leave the below code for Kernel PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "model = KernelPCA(kernel=\"rbf\", n_components=2,\n",
    "                 gamma=0.0003)\n",
    "out = model.fit_transform(X)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(out[:, 0], out[:, 1], c=color, cmap=\"rainbow\",\n",
    "          edgecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "model = PCA(n_components=2)\n",
    "out = model.fit_transform(X)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(out[:, 0], out[:, 1], c=color, cmap=\"rainbow\",\n",
    "          edgecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===Task===\n",
    "\n",
    "Your work: Let's modify the above scratch code:\n",
    "- Modify so instead of using np.linalg.eigh, let's replace it with scratch code using SVD approach\n",
    "- Convert it using kernel PCA, where we convert our X using rbf kernels.  You may want to see how to transfer your data to another space via https://en.wikipedia.org/wiki/Radial_basis_function_kernel and this http://rasbt.github.io/mlxtend/user_guide/feature_extraction/RBFKernelPCA/#References. For those who are confused what is x and x' (prime), they are basically each sample, so you may want to first find the squared distances between each sample.  Apply your kernel PCA with a sklearn datasets make_swiss_roll\n",
    "\n",
    "#### More details on Kernel PCA\n",
    "\n",
    "The steps are as follows:\n",
    "\n",
    "1). Calculate (x-x')^2\n",
    "\n",
    "For your convenience, to get the squared distances can be done using the pdist module. Then transform it into a square form using squareform module.\n",
    "\n",
    "<code>from scipy.spatial.distance import pdist </code>\n",
    "\n",
    "<code>distances = pdist(X, 'sqeuclidean')</code>\n",
    "\n",
    "<code>matrix = squareform(squared_distances)</code>\n",
    "\n",
    "BTW, if you prefer the numpy way, it is also perfectly fine!\n",
    "\n",
    "2). Center the data\n",
    "\n",
    "This is a bit tricky but http://rasbt.github.io/mlxtend/user_guide/feature_extraction/RBFKernelPCA/#References has provided us how to center the rbf data.\n",
    "\n",
    "3). Calculate rbf = exp(-gamma * matrix)\n",
    "\n",
    "As for gamma, you can actually pick whatever gamma you prefer. Then you would want to calculate the rbf kernel\n",
    "\n",
    "<code>rbf = np.exp(-gamma * matrix)</code>\n",
    "\n",
    "once you obtain the rbf, you DO NOT need to input into the np.cov, because by performing squared_distances we are actually doing covariances but in rbf space. \n",
    "\n",
    "(Notice we are not doing x - x' in normal covariance way, but (x - x')^2)\n",
    "\n",
    "4).  Use eig to calculate the eigenvectors and values\n",
    "\n",
    "5). Get the projection and plot it. \n",
    "\n",
    "When you project your data, you do rbf @ eigenvectors, NOT X @ eigenvectors since your eigenvectors are in rbf space\n",
    "\n",
    "yes, that's it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
